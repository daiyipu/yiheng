{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10个范例带你入门LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本第二节课通过演示10个具有代表性的应用范例，带你零基础入门langchain，所有代码全部可执行\n",
    "\n",
    "1，文本总结(Summarization): 对文本/聊天内容的重点内容总结。\n",
    "\n",
    "2，文档问答(Question and Answering Over Documents): 使用文档作为上下文信息，基于文档内容进行问答。\n",
    "\n",
    "3，信息抽取(Extraction): 从文本内容中抽取结构化的内容。\n",
    "\n",
    "4，结果评估(Evaluation): 分析并评估LLM输出的结果的好坏。\n",
    "\n",
    "5，数据库问答(Querying Tabular Data): 从数据库/类数据库内容中抽取数据信息。\n",
    "\n",
    "6，代码理解(Code Understanding): 分析代码，并从代码中获取逻辑，同时也支持QA。\n",
    "\n",
    "7，API交互(Interacting with APIs): 通过对API文档的阅读，理解API文档并向真实世界调用API获取真实数据。\n",
    "\n",
    "8，聊天机器人(Chatbots): 具备记忆能力的聊天机器人框架（有UI交互能力)\n",
    "\n",
    "9，用不到 50 行代码实现一个文档对话机器人\n",
    "\n",
    "10，智能体(Agents): 使用LLMs进行任务分析和决策，并调用工具执行决策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0,\"../../../zhipuai-sdk-python-v4\")\n",
    "# # sys.path.insert(0,\"../../../langchain\")\n",
    "# sys.path.insert(0,\"../../../langchain/libs/core\")\n",
    "# sys.path.insert(0,\"../../../langchain/libs/community\")\n",
    "# from zhipuai import ZhipuAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用你自己的OpenAI API key\n",
    "openai_api_key='sk-PFwMvMFcU7VXaL5cYM9vT3B'\n",
    "zhipu_api_key='08132b620644081b192567639fc0a9b1.OAz1XI2d21zLZDk9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import jwt\n",
    " \n",
    "def generate_token(apikey: str, exp_seconds: int):\n",
    "    try:\n",
    "        id, secret = apikey.split(\".\")\n",
    "    except Exception as e:\n",
    "        raise Exception(\"invalid apikey\", e)\n",
    " \n",
    "    payload = {\n",
    "        \"api_key\": id,\n",
    "        \"exp\": int(round(time.time() * 1000)) + exp_seconds * 1000,\n",
    "        \"timestamp\": int(round(time.time() * 1000)),\n",
    "    }\n",
    " \n",
    "    return jwt.encode(\n",
    "        payload,\n",
    "        secret,\n",
    "        algorithm=\"HS256\",\n",
    "        headers={\"alg\": \"HS256\", \"sign_type\": \"SIGN\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.chat_models import ChatOpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain_openai import ChatOpenAI\n",
    "llm =ChatOpenAI(\n",
    "        model_name=\"glm-4\",\n",
    "        openai_api_base=\"https://open.bigmodel.cn/api/paas/v4\",\n",
    "        openai_api_key=generate_token(zhipu_api_key,10000),\n",
    "        streaming=False,\n",
    "        temperature=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IarVI6ptBbD"
   },
   "source": [
    "## 一， 文本总结(Summarization)\n",
    "\n",
    "扔给LLM一段文本，让他给你生成总结可以说是最常见的场景之一了\n",
    "\n",
    "目前最火的应用应该是 chatPDF，可以利用langchain进行实现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# 创建模板\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# 创建一个 Lang Chain Prompt 模板，稍后可以插入值\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1，短文本总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:23.111724Z",
     "start_time": "2023-09-28T12:17:23.103991Z"
    },
    "id": "JgkE3t3JscMj"
   },
   "outputs": [],
   "source": [
    "confusing_text = \"\"\"\n",
    "For the next 130 years, debate raged.\n",
    "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
    "“The problem is that when you look up close at the anatomy, it’s evocative of a lot of different things, but it’s diagnostic of nothing,” says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
    "“And it’s so damn big that when whenever someone says it’s something, everyone else’s hackles get up: ‘How could you have a lichen 20 feet tall?’”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:23.120550Z",
     "start_time": "2023-09-28T12:17:23.114124Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "en5ZKBmisz5O",
    "outputId": "dcb5cc7d-0ee5-40e0-ba5b-507d76c22a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Prompt Begin -------\n",
      "\n",
      "%INSTRUCTIONS:\n",
      "Please summarize the following piece of text.\n",
      "Respond in a manner that a 5 year old\n",
      "%TEXT:\n",
      "\n",
      "For the next 130 years, debate raged.\n",
      "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
      "“The problem is that when you look up close at the anatomy, it’s evocative of a lot of different things, but it’s diagnostic of nothing,” says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
      "“And it’s so damn big that when whenever someone says it’s something, everyone else’s hackles get up: ‘How could you have a lichen 20 feet tall?’”\n",
      "\n",
      "\n",
      "------- Prompt End -------\n"
     ]
    }
   ],
   "source": [
    "print (\"------- Prompt Begin -------\")\n",
    "# 打印模板内容\n",
    "final_prompt = prompt.format(text=confusing_text)\n",
    "print(final_prompt)\n",
    "\n",
    "print (\"------- Prompt End -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='春风拂面花千树，江湖儿女情无数。\\n燕舞莺歌梦初醒，柳絮飘飞入江湖。\\n\\n翠竹轻摇绿意浓，桃花笑语映日红。\\n剑气满怀歌未了，诗酒趁年华，共赏春光浓。', response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 17, 'total_tokens': 78}, 'model_name': 'glm-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "llm.invoke([HumanMessage(content=\"请以古龙的口吻，写首关于春天诗\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2，长文本总结\n",
    "\n",
    "对于文本长度较短的文本我们可以直接这样执行summary操作\n",
    "\n",
    "但是对于文本长度超过lLM支持的max token size 时将会遇到困难\n",
    "\n",
    "Lang Chain 提供了开箱即用的工具解决长文本的问题：load_summarize_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:28.626319Z",
     "start_time": "2023-09-28T12:17:28.615313Z"
    },
    "id": "7DUFpAcZs-dE"
   },
   "outputs": [],
   "source": [
    "# Summaries Of Longer Text\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:28.637070Z",
     "start_time": "2023-09-28T12:17:28.630331Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_f8GlcSVtK5n",
    "outputId": "99d16873-8f3d-4b48-84d2-bac5f89a094a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wonderland\n",
      "Where is the wonderland I want to find it.——题记\n",
      "（一）\n",
      "“Wonderland”，很多人会把这个词翻译作“奇幻世界”之类的。但我，更爱把它称做“乐土”。\n",
      "人生只有一次。我奉信完全随心所欲、自由自在的人生。因此，我也一直，梦想着……不，幻想着，寻找到“乐土”。\n",
      "我觉得有一句来自东方古国的话很适合当我的座右铭——“人生得意须尽欢”。如果这唯一的人生都不能过得快乐，令自己满意，那还有什么意义呢？人生在世，重要的不是为别人留下些什么，而是能够在离开的时候潇洒一笑，说，我这一生过得无怨无悔。\n",
      "没错，我就是这\n"
     ]
    }
   ],
   "source": [
    "with open('wonderland.txt', 'r',encoding = 'utf-8') as file:\n",
    "    text = file.read() # 文章本身是爱丽丝梦游仙境\n",
    "\n",
    "# 打印小说的前285个字符\n",
    "print (text[:285])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:31.318161Z",
     "start_time": "2023-09-28T12:17:28.637070Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjtfOEvFt7ds",
    "outputId": "35fd6db3-8c32-47c7-c898-ae684bf70610"
   },
   "outputs": [],
   "source": [
    "# %pip install tiktoken --user # 安装用于分割文本的依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:31.562487Z",
     "start_time": "2023-09-28T12:17:31.320172Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "noL6AUb1ts2a",
    "outputId": "35228f63-3665-4e2c-d743-51589fc12246"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1938 tokens in your file\n"
     ]
    }
   ],
   "source": [
    "num_tokens = llm.get_num_tokens(text)\n",
    "\n",
    "print (f\"There are {num_tokens} tokens in your file\") \n",
    "# 全文一共4w8词\n",
    "# 很明显这样的文本量是无法直接送进LLM进行处理和生成的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决长文本的方式无非是'chunking','splitting' 原文本为小的段落/分割部分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:31.569873Z",
     "start_time": "2023-09-28T12:17:31.564504Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQsyYfGht2EH",
    "outputId": "5d505565-68d5-4810-85db-a69e2b4da542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You now have 1 docs intead of 1 piece of text\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=5000, chunk_overlap=350)\n",
    "# 虽然我使用的是 RecursiveCharacterTextSplitter，但是你也可以使用其他工具\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print (f\"You now have {len(docs)} docs intead of 1 piece of text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在就需要一个 Lang Chain 工具，将分段文本送入LLM进行summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:31.577804Z",
     "start_time": "2023-09-28T12:17:31.571881Z"
    },
    "id": "pwvT14EIuf3Q"
   },
   "outputs": [],
   "source": [
    "# 设置 lang chain\n",
    "# 使用 map_reduce的chain_type，这样可以将多个文档合并成一个\n",
    "chain = load_summarize_chain(llm=llm, chain_type='map_reduce') # verbose=True 展示运行日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:37.113589Z",
     "start_time": "2023-09-28T12:17:31.579817Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILwXodN8u_xF",
    "outputId": "644c900f-f61b-485c-9356-8abffd4c7f5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_documents': [Document(page_content='Wonderland\\nWhere is the wonderland I want to find it.——题记\\n（一）\\n“Wonderland”，很多人会把这个词翻译作“奇幻世界”之类的。但我，更爱把它称做“乐土”。\\n人生只有一次。我奉信完全随心所欲、自由自在的人生。因此，我也一直，梦想着……不，幻想着，寻找到“乐土”。\\n我觉得有一句来自东方古国的话很适合当我的座右铭——“人生得意须尽欢”。如果这唯一的人生都不能过得快乐，令自己满意，那还有什么意义呢？人生在世，重要的不是为别人留下些什么，而是能够在离开的时候潇洒一笑，说，我这一生过得无怨无悔。\\n没错，我就是这么一个自私的人。不过我一直认为这样很好。\\n曾经有那么一个人——跟我完全相反的一个人。他身上背负着来自家庭、来自社会各方面的压力和负担，但他从来没认为这些是应该抛弃的累赘，反而认为这是他的责任。\\n对，责任。他认为这是他必须承担的，是他生命的一部分。\\n哈哈，笑话。你看，他最后被那些责任压垮了哦。英年早逝，死因是劳累过度。这些都和我早就过世的“家人”一模一样。他们都是这样让我无法理解的人。如果人不能为自己而活……我实在是无法想象他们的那种人生。\\n我没有去他的葬礼，而是在他下葬之后的第二天去了公墓。我把从郊外采来，还带着露水的白色野百合放在他的墓碑前，然后就一屁股坐在了地上，舒舒服服地靠着他的墓碑。\\n那天天气不太好，空中飘着密密麻麻的雨丝。低气压也令人生厌，我觉得呼吸困难，鼻腔酸涩。公墓那边是这座濒临毁灭的城市中唯一一处还有点绿化的地方了。寥寥几棵种植在公墓外围的树上的叶子在雨中泛出鲜嫩的绿色，在风中沙沙地摇曳着。你看这世界上的人多不可理喻。明明就剩下这么一点环境还算可以的地方，却要留给已经不在了的人们。为什么……不能留给还在的人？\\n我胡乱抹了抹脸上的水，顺带着捋顺了在雨中凌乱不堪的一头杂毛。我的余光瞟到了让我感兴趣的东西。于是我转头盯着那人墓碑上的遗像发呆。\\n“你说，我以前从来没看过你的笑容。这唯一一次却是看到了你的遗像。”我禁不住笑了出来，“你说，要是你能像我一样只为自己而活的话，是不是就不会是现在这个样子了？”\\n那个人叫宇智波鼬。\\n其实……我也明白，只有这样的他才算是他。\\n不过他已经再也不能反驳我的话了。\\n（二）\\n宇智波鼬是我这辈子遇到的，唯一一个能跟我正常相处的人。啊，当然这种正常是我单方面的意见，我没有问过他。\\n之于他，我大概就什么都不是了。\\n第一次和他说话，是在一个荒凉的小公园——我经常去的一个地方。\\n那时候我和他都是小学生。只不过他非常优秀，而我……是那种和所有人都没办法相处，成绩也非常糟糕，还经常逃学的孩子。\\n那天我就是逃学去了公园里，随便在白纸上涂抹着色彩。一直到了黄昏，路灯接连亮起来的时候，不知道他为什么碰巧路过，然后又发现了我。\\n“在画画吗？”他背着巨大且沉重的书包凑到我身边问道。\\n“嗯……”我把最后一笔画上去，然后甩下画笔，饶有兴趣地看着他，“你确定要跟我说话？我可是伟大的精神病。”\\n“那些只是同学们胡乱说的。你只是不喜欢和人沟通吧？”他一本正经地回答道。\\n“哈，是真的哟。”我大笑着说道，顺手对着灰蒙蒙的天空竖过去一个标准的中指，“好学生怎么会这个时间在外面到处晃？看，遇上我这种人，你就不怕我一时冲动，杀了你吗？”\\n“你呢？为什么不回家？”他没有理会我的疯言疯语，执着地想要和我正常地沟通。\\n“我没有家啊。”看他一时也没有要走的意思，我便恢复了常态——如果我有那种东西的话，认真地回答道，“更何况我现在就是想要在这里画画。”\\n“那到了这种时间，也应该回住的地方好好休息了。”\\n“哦。”我敷衍着，继续把画上一些细节都认真地修改了一番。\\n“可以给我看看吗？”大概是没了话题，他提出了要看画的要求。\\n“当然。”我也就不再继续修改，直接把巨幅的画板抱起来，举给他看。\\n“很漂亮……但是……”他看到我的画之后立刻皱起了眉，清秀的眉目间随即多了一份不解。\\n“颜色太鲜艳？”我指了一下画上大片红色的纱帘，“还是你觉得葬礼只能用白啊黑啊这一类的颜色来描绘？”')], 'output_text': 'The narrative reflects on the concept of \"wonderland\" or \"le tu,\" advocating for a life of freedom and joy over responsibilities. It contrasts the narrator\\'s philosophy with that of Yuhi Mito, who died young under the weight of his duties. The text delves into their unique bond, societal expectations, and the pursuit of individual happiness.'}\n"
     ]
    }
   ],
   "source": [
    "# Use it. This will run through the 36 documents, summarize the chunks, then get a summary of the summary.\n",
    "# 典型的map reduce的思路去解决问题，将文章拆分成多个部分，再将多个部分分别进行 summarize，最后再进行 合并，对 summarys 进行 summary\n",
    "output = chain.invoke(docs)\n",
    "print (output)\n",
    "# Try yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I74h0HtgwA--"
   },
   "source": [
    "## 二，文档问答(QA based Documents)\n",
    "\n",
    "为了确保LLM能够执行QA任务\n",
    "1. 需要向LLM传递能够让他参考的上下文信息\n",
    "2. 需要向LLM准确地传达我们的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1，短文本问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:37.127328Z",
     "start_time": "2023-09-28T12:17:37.120211Z"
    },
    "id": "J-a5IoExwADi"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Rachel is 30 years old\n",
    "Bob is 45 years old\n",
    "Kevin is 65 years old\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who is under 40 years old?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:38.345026Z",
     "start_time": "2023-09-28T12:17:37.127328Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89q7fvB7wfu9",
    "outputId": "d19f61dc-68f5-4e08-9fca-4da08f149121"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Rachel is under 40 years old. She is 30 years old. Bob and Kevin are both over 40 years old.', response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 34, 'total_tokens': 62}, 'model_name': 'glm-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=context + question\n",
    "# output = llm.invoke(HumanMessage(content=t))\n",
    "\n",
    "llm.invoke([HumanMessage(content=t)])\n",
    "# print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2，长文本问答\n",
    "\n",
    "对于更长的文本，可以文本进行分块，对分块的内容进行 embedding，将 embedding 存储到数据库中，然后进行查询。\n",
    "\n",
    "目标是选择相关的文本块，但是我们应该选择哪些文本块呢？目前最流行的方法是基于比较向量嵌入来选择相似的文本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:41.027953Z",
     "start_time": "2023-09-28T12:17:38.345026Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKp2J9kHxhQp",
    "outputId": "efae0b92-c25f-47e9-bfa7-554838a82aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/davy/opt/anaconda3/lib/python3.9/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /Users/davy/opt/anaconda3/lib/python3.9/site-packages (from faiss-cpu) (1.26.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for lightgbm: [Errno 2] No such file or directory: '/Users/davy/opt/anaconda3/lib/python3.9/site-packages/lightgbm-3.3.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu --user # 需要注意，faiss存在GPU和CPU版本基于你的 runtime 安装对应的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:41.035650Z",
     "start_time": "2023-09-28T12:17:41.029787Z"
    },
    "id": "jqbzqzMBwiXP"
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:41.043549Z",
     "start_time": "2023-09-28T12:17:41.037660Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVJuuoG2xBnS",
    "outputId": "84af0b45-8623-4887-c308-4d7628df2495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 1683 characters in that document\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('wonderland.txt',encoding='utf-8') # 载入一个长文本，我们还是使用爱丽丝漫游仙境这篇小说作为输入\n",
    "doc = loader.load()\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:41.051368Z",
     "start_time": "2023-09-28T12:17:41.045559Z"
    },
    "id": "81ohG7WpxKNJ"
   },
   "outputs": [],
   "source": [
    "# 将小说分割成多个部分\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:41.060970Z",
     "start_time": "2023-09-28T12:17:41.052714Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FArqtB8yxPZr",
    "outputId": "4d89d204-6d05-4fe6-d300-e40de171c86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 1 documents that have an average of 1,683 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "# 获取字符的总数，以便可以计算平均值\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:43.550478Z",
     "start_time": "2023-09-28T12:17:41.062980Z"
    },
    "id": "STNVCrQxxUO8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davy/opt/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2f/gylrw_qs6xj4j6hygjj1_wgc0000gn/T/ipykernel_4653/4018102586.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Embed 文档，然后使用伪数据库将文档和原始文本结合起来\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 这一步会向 OpenAI 发起 API 请求\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdocsearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/vectorstores.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \"\"\"\n\u001b[0;32m--> 930\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         return cls.__from(\n\u001b[1;32m    932\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m#       than the maximum context and use length-safe embedding function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_len_safe_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     async def aembed_documents(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36m_get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mbatched_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             response = embed_with_retry(\n\u001b[0m\u001b[1;32m    495\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_chunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;34m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         )\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     def patch(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 897\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    898\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    951\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    927\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    231\u001b[0m         )\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    197\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect_failed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mssl_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_extra_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ssl_object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     }\n\u001b[1;32m    121\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connect_tcp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_tcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mconnect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             sock = socket.create_connection(\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 设置 embedding 引擎\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embed 文档，然后使用伪数据库将文档和原始文本结合起来\n",
    "# 这一步会向 OpenAI 发起 API 请求\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:43.556540Z",
     "start_time": "2023-09-28T12:17:43.551491Z"
    },
    "id": "4KNHfrfdxcx9"
   },
   "outputs": [],
   "source": [
    "# 创建QA-retrieval chain\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:56.158418Z",
     "start_time": "2023-09-28T12:17:43.558557Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "G1A1_UI9xvVn",
    "outputId": "b9b600df-96b2-4809-b8d6-067ec909520b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The author describes Alice following with a large and heavy book bag.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does the author describe the Alice following with?\"\n",
    "qa.run(query)\n",
    "# 这个过程中，检索器会去获取类似的文件部分，并结合你的问题让 LLM 进行推理，最后得到答案\n",
    "# 这一步还有很多可以细究的步骤，比如如何选择最佳的分割大小，如何选择最佳的 embedding 引擎，如何选择最佳的检索器等等\n",
    "# 同时也可以选择云端向量存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcLfoguiz9EY"
   },
   "source": [
    "## 三，信息抽取(Extraction)\n",
    "\n",
    "Extraction是从一段文本中解析结构化数据的过程.\n",
    "\n",
    "通常与Extraction parser一起使用，以构建数据，以下是一些使用范例。\n",
    "\n",
    "1. 从句子中提取结构化行以插入数据库\n",
    "2. 从长文档中提取多行以插入数据库\n",
    "3. 从用户查询中提取参数以进行 API 调用\n",
    "4. 最近最火的 Extraction 库是 KOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1，手动格式转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:56.167393Z",
     "start_time": "2023-09-28T12:17:56.161079Z"
    },
    "id": "sKlFUiuXx_aD"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# chat_model = ChatOpenAI(temperature=0, model='gpt-3.5-turbo', openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:56.179146Z",
     "start_time": "2023-09-28T12:17:56.174402Z"
    },
    "id": "JWGf8ICk073y"
   },
   "outputs": [],
   "source": [
    "# Vanilla Extraction\n",
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:59.647172Z",
     "start_time": "2023-09-28T12:17:56.181158Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjn0cxnZ1Brb",
    "outputId": "64ea2e40-9752-487a-e872-efa0ea183f69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davy/opt/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Here's a Python dictionary with the extracted fruit names and their corresponding emojis:\\n\\n```python\\nfruit_emojis = {\\n    'Apple': '\\\\U0001F34E',  # Red Apple emoji\\n    'Pear': '\\\\U0001F350',   # Pear emoji\\n    'Kiwi': '\\\\U0001F95D'    # Kiwi Fruit emoji\\n}\\n\\n# Note: The 'kiwi' in the input sentence was misspelled as 'an kiwi', I corrected it to 'Kiwi'.\\n```\\n\\nMake sure to use a font that supports these emojis, or your IDE may not display them correctly.\" response_metadata={'token_usage': {'completion_tokens': 133, 'prompt_tokens': 47, 'total_tokens': 180}, 'model_name': 'glm-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "# Make your prompt which combines the instructions w/ the fruit names\n",
    "prompt = (instructions + fruit_names)\n",
    "\n",
    "# Call the LLM\n",
    "output = llm([HumanMessage(content=prompt)])\n",
    "\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:59.653446Z",
     "start_time": "2023-09-28T12:17:59.649206Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gImh8_6B1Hst",
    "outputId": "aa8687e6-be97-4e56-8368-e400ae9f9046"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/davy/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3457\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/2f/gylrw_qs6xj4j6hygjj1_wgc0000gn/T/ipykernel_4653/2349998927.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    output_dict = eval(output.content) #利用python中的eval函数手动转换格式\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Here's a Python dictionary with the extracted fruit names and their corresponding emojis:\u001b[0m\n\u001b[0m                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "output_dict = eval(output.content) #利用python中的eval函数手动转换格式\n",
    "\n",
    "print (output_dict)\n",
    "print (type(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2，自动格式转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用langchain.output_parsers.StructuredOutputParser可以自动生成一个带有格式说明的提示。\n",
    "\n",
    "这样就不需要担心提示工程输出格式的问题了，将这部分完全交给 Lang Chain 来执行，将LLM的输出转化为 python 对象。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:59.661196Z",
     "start_time": "2023-09-28T12:17:59.654455Z"
    },
    "id": "rA5hAH3q11zG"
   },
   "outputs": [],
   "source": [
    "# 解析输出并获取结构化的数据\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# 解析器将会把LLM的输出使用我定义的schema进行解析并返回期待的结构数据给我\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:59.670185Z",
     "start_time": "2023-09-28T12:17:59.662204Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzsTB-WK3CSj",
    "outputId": "0ef60a81-11df-47c7-c03b-0c427a9f3572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:59.679887Z",
     "start_time": "2023-09-28T12:17:59.673194Z"
    },
    "id": "o9C10yFv3GF0"
   },
   "outputs": [],
   "source": [
    "# 这个 Prompt 与之前我们构建 Chat Model 时 Prompt 不同\n",
    "# 这个 Prompt 是一个 ChatPromptTemplate，它会自动将我们的输出转化为 python 对象\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
    "                                                    {format_instructions}\\n{user_prompt}\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:17:59.690035Z",
     "start_time": "2023-09-28T12:17:59.681903Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndEjq-x13Uct",
    "outputId": "647dd66a-f924-483d-9a04-5d732b959059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a command from the user, extract the artist and song names \n",
      "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "I really like So Young by Portugal. The Man\n"
     ]
    }
   ],
   "source": [
    "artist_query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
    "print(artist_query.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:18:02.494631Z",
     "start_time": "2023-09-28T12:17:59.690687Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MU01EgAx3Xrl",
    "outputId": "41972c45-4372-4dc6-f7e6-fd25728d3881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "artist_output = llm(artist_query.to_messages())\n",
    "output = output_parser.parse(artist_output.content)\n",
    "\n",
    "print (output)\n",
    "print (type(output))\n",
    "# 这里要注意的是，因为我们使用的 turbo 模型，生成的结果并不一定是每次都一致的\n",
    "# 替换成gpt4模型可能是更好的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cu1MzoY7368M"
   },
   "source": [
    "## 四，结果评估(Evaluation)\n",
    "\n",
    "由于自然语言的不可预测性和可变性，评估LLM的输出是否正确有些困难，langchain 提供了一种方式帮助我们去解决这一难题。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:18:02.503317Z",
     "start_time": "2023-09-28T12:18:02.497326Z"
    },
    "id": "w787Sqsn3eIm"
   },
   "outputs": [],
   "source": [
    "# Embeddings, store, and retrieval\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Model and doc loader\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Eval\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:18:02.513845Z",
     "start_time": "2023-09-28T12:18:02.506325Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IYOmSZCx4jTG",
    "outputId": "1ad8ee26-f421-4fb1-87bd-bce0fe2682d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 1683 characters in that document\n"
     ]
    }
   ],
   "source": [
    "# 还是使用爱丽丝漫游仙境作为文本输入\n",
    "loader = TextLoader('wonderland.txt',encoding='utf-8')\n",
    "doc = loader.load()\n",
    "\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:18:02.524654Z",
     "start_time": "2023-09-28T12:18:02.516860Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auRa34Lk4qAQ",
    "outputId": "3e60b5a1-1c13-4270-be41-cc9cf2f2f714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 1 documents that have an average of 1,683 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:18:03.224428Z",
     "start_time": "2023-09-28T12:18:02.524654Z"
    },
    "id": "WkE1fi_L4tQS"
   },
   "outputs": [],
   "source": [
    "# Embeddings and docstore\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:18:03.230549Z",
     "start_time": "2023-09-28T12:18:03.224428Z"
    },
    "id": "v9oya8su4wpZ"
   },
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), input_key=\"question\")\n",
    "# 注意这里的 input_key 参数，这个参数告诉了 chain 我的问题在字典中的哪个 key 里\n",
    "# 这样 chain 就会自动去找到问题并将其传递给 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:18:03.237875Z",
     "start_time": "2023-09-28T12:18:03.232560Z"
    },
    "id": "bTUujN3k5OKe"
   },
   "outputs": [],
   "source": [
    "question_answers = [\n",
    "    {'question' : \"Which animal give alice a instruction?\", 'answer' : 'rabbit'},\n",
    "    {'question' : \"What is the author of the book\", 'answer' : 'Elon Mask'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:18:36.204697Z",
     "start_time": "2023-09-28T12:18:03.238886Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKqRhrpo51O8",
    "outputId": "e2e539ed-93ba-40fe-d195-4bf3d55efa8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Which animal give alice a instruction?',\n",
       "  'answer': 'rabbit',\n",
       "  'result': \" I don't know.\"},\n",
       " {'question': 'What is the author of the book',\n",
       "  'answer': 'Elon Mask',\n",
       "  'result': \" I don't know.\"}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = chain.apply(question_answers)\n",
    "predictions\n",
    "# 使用LLM模型进行预测，并将答案与我提供的答案进行比较，这里信任我自己提供的人工答案是正确的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:00.257486Z",
     "start_time": "2023-09-28T12:18:36.205705Z"
    },
    "id": "6w8L8Qam51g5"
   },
   "outputs": [],
   "source": [
    "# Start your eval chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "graded_outputs = eval_chain.evaluate(question_answers,\n",
    "                                     predictions,\n",
    "                                     question_key=\"question\",\n",
    "                                     prediction_key=\"result\",\n",
    "                                     answer_key='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:00.281866Z",
     "start_time": "2023-09-28T12:19:00.257486Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c4HECHp6L8F",
    "outputId": "cb43aa93-ac6f-4075-e785-f30425d6c501"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': ' INCORRECT'}, {'results': ' INCORRECT'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaUJftcC6aCV"
   },
   "source": [
    "## 五，数据库问答(Querying Tabular Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:00.323997Z",
     "start_time": "2023-09-28T12:19:00.285875Z"
    },
    "id": "dIpb5Hi46OCr"
   },
   "outputs": [],
   "source": [
    "# 使用自然语言查询一个 SQLite 数据库，我们将使用旧金山树木数据集\n",
    "# Don't run following code if you don't run sqlite and follow db\n",
    "from langchain import OpenAI, SQLDatabase\n",
    "# %pip install  --user langchain_experimental\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "# llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:00.647603Z",
     "start_time": "2023-09-28T12:19:00.329010Z"
    },
    "id": "6DiP-LlX6nrh"
   },
   "outputs": [],
   "source": [
    "sqlite_db_path = 'San_Francisco_Trees.db'\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:00.694738Z",
     "start_time": "2023-09-28T12:19:00.657630Z"
    },
    "id": "X568YYZ265Pi"
   },
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm=llm, db=db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:41.013454Z",
     "start_time": "2023-09-28T12:19:00.697748Z"
    },
    "id": "vLuuRm4n66NM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many Species of trees are there in San Francisco?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(DISTINCT \"qSpecies\") FROM \"SFTrees\"\n",
      "SQLResult:\u001b[0m"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) unrecognized token: \":\"\n[SQL: SELECT COUNT(DISTINCT \"qSpecies\") FROM \"SFTrees\"\nSQLResult:]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1969\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m                     self.dialect.do_execute(\n\u001b[0m\u001b[1;32m   1971\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_statement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: unrecognized token: \":\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2f/gylrw_qs6xj4j6hygjj1_wgc0000gn/T/ipykernel_4653/2664579604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"How many Species of trees are there in San Francisco?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             outputs = (\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_experimental/sql/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# improvement of few shot prompt seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_steps\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_experimental/sql/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mSQL_QUERY\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msql_cmd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0msql_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msql_cmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSQL_QUERY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mintermediate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# output: sql exec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/utilities/sql_database.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, command, fetch, include_columns, parameters, execution_options)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstatement\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mno\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0man\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \"\"\"\n\u001b[0;32m--> 498\u001b[0;31m         result = self._execute(\n\u001b[0m\u001b[1;32m    499\u001b[0m             \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/utilities/sql_database.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, command, fetch, parameters, execution_options)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Query expression has unknown type: {type(command)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             cursor = connection.execute(\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m             return meth(\n\u001b[0m\u001b[1;32m   1422\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m                 \u001b[0mdistilled_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExecutable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             return connection._execute_clauseelement(\n\u001b[0m\u001b[1;32m    515\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistilled_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1641\u001b[0m             \u001b[0mlinting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_linting\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN_LINTING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m         )\n\u001b[0;32m-> 1643\u001b[0;31m         ret = self._execute_context(\n\u001b[0m\u001b[1;32m   1644\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_ctx_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_compiled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec_insertmany_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m             return self._exec_single_context(\n\u001b[0m\u001b[1;32m   1850\u001b[0m                 \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m             self._handle_dbapi_exception(\n\u001b[0m\u001b[1;32m   1990\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_statement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2354\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0msqlalchemy_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1968\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m                     self.dialect.do_execute(\n\u001b[0m\u001b[1;32m   1971\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_statement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m                     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) unrecognized token: \":\"\n[SQL: SELECT COUNT(DISTINCT \"qSpecies\") FROM \"SFTrees\"\nSQLResult:]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "db_chain.invoke(\"How many Species of trees are there in San Francisco?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4ukkWFf7BaS"
   },
   "source": [
    "1. Find which table to use\n",
    "2. Find which column to use\n",
    "3. Construct the correct sql query\n",
    "4. Execute that query\n",
    "5. Get the result\n",
    "6. Return a natural language reponse back\n",
    "\n",
    "confirm LLM result via pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:41.184170Z",
     "start_time": "2023-09-28T12:19:41.016462Z"
    },
    "id": "CbEX6OVi7Fot"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/78/a8/07dd10f90ca915ed914853cd57f79bfc22e1ef4384ab56cb4336d2fc1f2a/pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached https://files.pythonhosted.org/packages/d5/fb/a79efcab32b8a1f1ddca7f35109a50e4a80d42ac1c9187ab46522b2407d7/tzdata-2023.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2023.3.post1 tzdata-2023.3\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 23.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "%pip install pandas --user\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the SQLite database\n",
    "connection = sqlite3.connect(sqlite_db_path)\n",
    "\n",
    "# Define your SQL query\n",
    "query = \"SELECT count(distinct qSpecies) FROM SFTrees\"\n",
    "\n",
    "# Read the SQL query into a Pandas DataFrame\n",
    "df = pd.read_sql_query(query, connection)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:41.194289Z",
     "start_time": "2023-09-28T12:19:41.186173Z"
    },
    "id": "EIgn39RF7T_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n"
     ]
    }
   ],
   "source": [
    "# Display the result in the first column first cell\n",
    "print(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEBsVmFb7vc7"
   },
   "source": [
    "## 六，代码理解(Code Understanding)\n",
    "\n",
    "代码理解用到的工具和文档问答差不多，不过我们的输入是一个项目的代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:41.205992Z",
     "start_time": "2023-09-28T12:19:41.196301Z"
    },
    "id": "Gz9kF0vk7yxi"
   },
   "outputs": [],
   "source": [
    "# Helper to read local files\n",
    "import os\n",
    "\n",
    "# Vector Support\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Model and chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Text splitters\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:19:41.223981Z",
     "start_time": "2023-09-28T12:19:41.208002Z"
    },
    "id": "AgEUoDi_713p"
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=(), openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:31:59.532311Z",
     "start_time": "2023-09-28T12:31:59.337879Z"
    },
    "id": "vhmKMttD73oC"
   },
   "outputs": [],
   "source": [
    "root_dir = './thefuzz-master/'\n",
    "docs = []\n",
    "\n",
    "# Go through each folder\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    \n",
    "    # Go through each file\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            # Load up the file as a doc and split\n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:00.163121Z",
     "start_time": "2023-09-28T12:32:00.146052Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,doc in enumerate(docs):\n",
    "    docs[i].page_content = docs[i].page_content[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:00.647054Z",
     "start_time": "2023-09-28T12:32:00.621590Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWickZcQ9r98",
    "outputId": "6a2977db-6f24-4ef9-817b-c3be97b330f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 129 documents\n",
      "\n",
      "------ Start Document ------\n",
      "id|custom_title|stubhub_title|vividseats_title\n",
      "701562|Toronto Blue Jays at Baltimore Orioles (Wednesday April 25, 2012)|Baltimore Orioles vs Toronto Blue Jays [4/25/2012] Tickets at StubHub!|Toronto Blue Jays at Baltimore Orioles\n",
      "701563|Texas Rangers at Baltimore Orioles (Tuesday May 8, 2012)|Baltim\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(docs)} documents\\n\")\n",
    "print (\"------ Start Document ------\")\n",
    "print (docs[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:06.755675Z",
     "start_time": "2023-09-28T12:32:02.000937Z"
    },
    "id": "Ybg6PTit9xJH"
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:06.880659Z",
     "start_time": "2023-09-28T12:32:06.761807Z"
    },
    "id": "E8rXOXmw-Tpu"
   },
   "outputs": [],
   "source": [
    "# Get our retriever ready\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:12.452596Z",
     "start_time": "2023-09-28T12:32:06.880659Z"
    },
    "id": "lqaXfVf2-VK2"
   },
   "outputs": [],
   "source": [
    "query = \"What function do I use if I want to find the most similar item in a list of items?\"\n",
    "output = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:12.467282Z",
     "start_time": "2023-09-28T12:32:12.457817Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfbsq_x7-bND",
    "outputId": "9fd9d866-964c-4077-e23e-2cb85c3aeb66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the most similar item in a list of items, you can use the cosine similarity function.\n"
     ]
    }
   ],
   "source": [
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:24.647845Z",
     "start_time": "2023-09-28T12:32:12.472294Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fn8_2xL3-gF5",
    "outputId": "94a4fdd2-22f7-4302-f9bc-aa896cca7da1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fuzzywuzzy import process\n",
      "\n",
      "choices = ['Detroit Tigers vs Houston Astros', 'New York Mets vs Houston Astros', 'Cleveland Indians at Chicago White Sox',\n",
      "           'Milwaukee Brewers at Chicago White Sox', 'New York Yankees at Chicago White Sox', 'Boston Red Sox at Chicago White Sox',\n",
      "           'Washington Nationals at Boston Red Sox', 'Toronto Blue Jays at Boston Red Sox']\n",
      "\n",
      "query = 'Detroit Tigers at Houston Astros'\n",
      "\n",
      "result = process.extractOne(query, choices)\n",
      "print(result)\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you write the code to use the process.extractOne() function? Only respond with code. No other text or explanation\"\n",
    "output = qa.run(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOD5KzPE-vRf"
   },
   "source": [
    "## 七，API交互(Interacting with APIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOD5KzPE-vRf"
   },
   "source": [
    "如果你需要的数据或操作在 API 之后，就需要LLM能够和API进行交互。\n",
    "\n",
    "到这个环节，就与 Agents 和 Plugins 息息相关了。\n",
    "\n",
    "Demo可能很简单，但是功能可以很复杂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:35.507938Z",
     "start_time": "2023-09-28T12:32:35.489124Z"
    },
    "id": "zawoKRWy-v1Y"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:36.373561Z",
     "start_time": "2023-09-28T12:32:36.356169Z"
    },
    "id": "JMes_6Xo_Mhh"
   },
   "outputs": [],
   "source": [
    "api_docs = \"\"\"\n",
    "\n",
    "BASE URL: https://restcountries.com/\n",
    "\n",
    "API Documentation:\n",
    "\n",
    "The API endpoint /v3.1/name/{name} Used to find informatin about a country. All URL parameters are listed below:\n",
    "    - name: Name of country - Ex: italy, france\n",
    "    \n",
    "The API endpoint /v3.1/currency/{currency} Uesd to find information about a region. All URL parameters are listed below:\n",
    "    - currency: 3 letter currency. Example: USD, COP\n",
    "    \n",
    "Woo! This is my documentation\n",
    "\"\"\"\n",
    "\n",
    "chain_new = APIChain.from_llm_and_api_docs(llm, api_docs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:32:41.641991Z",
     "start_time": "2023-09-28T12:32:36.605174Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "gdI8gkyi_N2t",
    "outputId": "b2da73bf-6bf6-41a9-c816-28bc8ef7b1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"France\",\"official\":\"French Republic\",\"nativeName\":{\"fra\":{\"official\":\"République française\",\"common\":\"France\"}}},\"tld\":[\".fr\"],\"cca2\":\"FR\",\"ccn3\":\"250\",\"cca3\":\"FRA\",\"cioc\":\"FRA\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"EUR\":{\"name\":\"Euro\",\"symbol\":\"€\"}},\"idd\":{\"root\":\"+3\",\"suffixes\":[\"3\"]},\"capital\":[\"Paris\"],\"altSpellings\":[\"FR\",\"French Republic\",\"République française\"],\"region\":\"Europe\",\"subregion\":\"Western Europe\",\"languages\":{\"fra\":\"French\"},\"translations\":{\"ara\":{\"official\":\"الجمهورية الفرنسية\",\"common\":\"فرنسا\"},\"bre\":{\"official\":\"Republik Frañs\",\"common\":\"Frañs\"},\"ces\":{\"official\":\"Francouzská republika\",\"common\":\"Francie\"},\"cym\":{\"official\":\"French Republic\",\"common\":\"France\"},\"deu\":{\"official\":\"Französische Republik\",\"common\":\"Frankreich\"},\"est\":{\"official\":\"Prantsuse Vabariik\",\"common\":\"Prantsusmaa\"},\"fin\":{\"official\":\"Ranskan tasavalta\",\"common\":\"Ranska\"},\"fra\":{\"official\":\"République française\",\"common\":\"France\"},\"hrv\":{\"official\":\"Francuska Republika\",\"common\":\"Francuska\"},\"hun\":{\"official\":\"Francia Köztársaság\",\"common\":\"Franciaország\"},\"ita\":{\"official\":\"Repubblica francese\",\"common\":\"Francia\"},\"jpn\":{\"official\":\"フランス共和国\",\"common\":\"フランス\"},\"kor\":{\"official\":\"프랑스 공화국\",\"common\":\"프랑스\"},\"nld\":{\"official\":\"Franse Republiek\",\"common\":\"Frankrijk\"},\"per\":{\"official\":\"جمهوری فرانسه\",\"common\":\"فرانسه\"},\"pol\":{\"official\":\"Republika Francuska\",\"common\":\"Francja\"},\"por\":{\"official\":\"República Francesa\",\"common\":\"França\"},\"rus\":{\"official\":\"Французская Республика\",\"common\":\"Франция\"},\"slk\":{\"official\":\"Francúzska republika\",\"common\":\"Francúzsko\"},\"spa\":{\"official\":\"República francés\",\"common\":\"Francia\"},\"srp\":{\"official\":\"Француска Република\",\"common\":\"Француска\"},\"swe\":{\"official\":\"Republiken Frankrike\",\"common\":\"Frankrike\"},\"tur\":{\"official\":\"Fransa Cumhuriyeti\",\"common\":\"Fransa\"},\"urd\":{\"official\":\"جمہوریہ فرانس\",\"common\":\"فرانس\"},\"zho\":{\"official\":\"法兰西共和国\",\"common\":\"法国\"}},\"latlng\":[46.0,2.0],\"landlocked\":false,\"borders\":[\"AND\",\"BEL\",\"DEU\",\"ITA\",\"LUX\",\"MCO\",\"ESP\",\"CHE\"],\"area\":551695.0,\"demonyms\":{\"eng\":{\"f\":\"French\",\"m\":\"French\"},\"fra\":{\"f\":\"Française\",\"m\":\"Français\"}},\"flag\":\"\\uD83C\\uDDEB\\uD83C\\uDDF7\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/g7QxxSFsWyTPKuzd7\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/1403916\"},\"population\":67391582,\"gini\":{\"2018\":32.4},\"fifa\":\"FRA\",\"car\":{\"signs\":[\"F\"],\"side\":\"right\"},\"timezones\":[\"UTC-10:00\",\"UTC-09:30\",\"UTC-09:00\",\"UTC-08:00\",\"UTC-04:00\",\"UTC-03:00\",\"UTC+01:00\",\"UTC+02:00\",\"UTC+03:00\",\"UTC+04:00\",\"UTC+05:00\",\"UTC+10:00\",\"UTC+11:00\",\"UTC+12:00\"],\"continents\":[\"Europe\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/fr.png\",\"svg\":\"https://flagcdn.com/fr.svg\",\"alt\":\"The flag of France is composed of three equal vertical bands of blue, white and red.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/fr.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/fr.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[48.87,2.33]},\"postalCode\":{\"format\":\"#####\",\"regex\":\"^(\\\\d{5})$\"}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' France is an officially-assigned, independent country located in Western Europe. Its capital is Paris and its official language is French. Its currency is the Euro (€). It has a population of 67,391,582 and its borders are with Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain, and Switzerland.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new.run('Can you tell me information about france?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:00.359843Z",
     "start_time": "2023-09-28T12:32:41.646006Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "-cIEtsLW_Q8Q",
    "outputId": "7a3c1d8d-4807-49aa-d4fb-e9fcb6186f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/currency/COP\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"Colombia\",\"official\":\"Republic of Colombia\",\"nativeName\":{\"spa\":{\"official\":\"República de Colombia\",\"common\":\"Colombia\"}}},\"tld\":[\".co\"],\"cca2\":\"CO\",\"ccn3\":\"170\",\"cca3\":\"COL\",\"cioc\":\"COL\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"COP\":{\"name\":\"Colombian peso\",\"symbol\":\"$\"}},\"idd\":{\"root\":\"+5\",\"suffixes\":[\"7\"]},\"capital\":[\"Bogotá\"],\"altSpellings\":[\"CO\",\"Republic of Colombia\",\"República de Colombia\"],\"region\":\"Americas\",\"subregion\":\"South America\",\"languages\":{\"spa\":\"Spanish\"},\"translations\":{\"ara\":{\"official\":\"جمهورية كولومبيا\",\"common\":\"كولومبيا\"},\"bre\":{\"official\":\"Republik Kolombia\",\"common\":\"Kolombia\"},\"ces\":{\"official\":\"Kolumbijská republika\",\"common\":\"Kolumbie\"},\"cym\":{\"official\":\"Gweriniaeth Colombia\",\"common\":\"Colombia\"},\"deu\":{\"official\":\"Republik Kolumbien\",\"common\":\"Kolumbien\"},\"est\":{\"official\":\"Colombia Vabariik\",\"common\":\"Colombia\"},\"fin\":{\"official\":\"Kolumbian tasavalta\",\"common\":\"Kolumbia\"},\"fra\":{\"official\":\"République de Colombie\",\"common\":\"Colombie\"},\"hrv\":{\"official\":\"Republika Kolumbija\",\"common\":\"Kolumbija\"},\"hun\":{\"official\":\"Kolumbiai Köztársaság\",\"common\":\"Kolumbia\"},\"ita\":{\"official\":\"Repubblica di Colombia\",\"common\":\"Colombia\"},\"jpn\":{\"official\":\"コロンビア共和国\",\"common\":\"コロンビア\"},\"kor\":{\"official\":\"콜롬비아 공화국\",\"common\":\"콜롬비아\"},\"nld\":{\"official\":\"Republiek Colombia\",\"common\":\"Colombia\"},\"per\":{\"official\":\"جمهوری کلمبیا\",\"common\":\"کلمبیا\"},\"pol\":{\"official\":\"Republika Kolumbii\",\"common\":\"Kolumbia\"},\"por\":{\"official\":\"República da Colômbia\",\"common\":\"Colômbia\"},\"rus\":{\"official\":\"Республика Колумбия\",\"common\":\"Колумбия\"},\"slk\":{\"official\":\"Kolumbijská republika\",\"common\":\"Kolumbia\"},\"spa\":{\"official\":\"República de Colombia\",\"common\":\"Colombia\"},\"srp\":{\"official\":\"Република Колумбија\",\"common\":\"Колумбија\"},\"swe\":{\"official\":\"Republiken Colombia\",\"common\":\"Colombia\"},\"tur\":{\"official\":\"Kolombiya Cumhuriyeti\",\"common\":\"Kolombiya\"},\"urd\":{\"official\":\"جمہوریہ کولمبیا\",\"common\":\"کولمبیا\"},\"zho\":{\"official\":\"哥伦比亚共和国\",\"common\":\"哥伦比亚\"}},\"latlng\":[4.0,-72.0],\"landlocked\":false,\"borders\":[\"BRA\",\"ECU\",\"PAN\",\"PER\",\"VEN\"],\"area\":1141748.0,\"demonyms\":{\"eng\":{\"f\":\"Colombian\",\"m\":\"Colombian\"},\"fra\":{\"f\":\"Colombienne\",\"m\":\"Colombien\"}},\"flag\":\"\\uD83C\\uDDE8\\uD83C\\uDDF4\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/zix9qNFX69E9yZ2M6\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/120027\"},\"population\":50882884,\"gini\":{\"2019\":51.3},\"fifa\":\"COL\",\"car\":{\"signs\":[\"CO\"],\"side\":\"right\"},\"timezones\":[\"UTC-05:00\"],\"continents\":[\"South America\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/co.png\",\"svg\":\"https://flagcdn.com/co.svg\",\"alt\":\"The flag of Colombia is composed of three horizontal bands of yellow, blue and red, with the yellow band twice the height of the other two bands.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/co.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/co.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[4.71,-74.07]}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-JdLoPt56PJ8Tc0pwuac3gPUq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-JdLoPt56PJ8Tc0pwuac3gPUq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The currency of Colombia is the Colombian peso (COP), symbolized by the \"$\" sign.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new.run('Can you tell me about the currency COP?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL3XHt4U_dqp"
   },
   "source": [
    "## 八，聊天机器人(Chatbots)\n",
    "\n",
    "聊天机器人使用了之前提及过的很多工具，且最重要的是增加了一个重要的工具：记忆力。\n",
    "\n",
    "与用户进行实时交互，为用户提供自然语言问题的平易近人的 UI，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:00.370087Z",
     "start_time": "2023-09-28T12:33:00.361854Z"
    },
    "id": "bgceKFnd_aHP"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Chat specific components\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:00.392384Z",
     "start_time": "2023-09-28T12:33:00.377351Z"
    },
    "id": "piN2tjyw_2Xk"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a chatbot that is unhelpful.\n",
    "Your goal is to not help the user but only make jokes.\n",
    "Take what the user is saying and make a joke out of it\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:00.408301Z",
     "start_time": "2023-09-28T12:33:00.394395Z"
    },
    "id": "XmRaX6u4ADIb"
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(openai_api_key=openai_api_key), \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:23.586728Z",
     "start_time": "2023-09-28T12:33:00.408301Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "VVc-YVHiAFWN",
    "outputId": "4ea06df4-e200-4d3e-a7f8-b75e033fdf15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" It's neither, it's a mystery!\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm_chain.predict(human_input=\"Is an pear a fruit or vegetable?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:38.556762Z",
     "start_time": "2023-09-28T12:33:23.590377Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "zwcE5P9-AM3k",
    "outputId": "22e9f486-d1fe-4261-9e1b-8d32de7e62fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI:  It's neither, it's a mystery!\n",
      "Human: What was one of the fruits I first asked you about?\n",
      "Chatbot:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-JdLoPt56PJ8Tc0pwuac3gPUq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-JdLoPt56PJ8Tc0pwuac3gPUq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-JdLoPt56PJ8Tc0pwuac3gPUq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-JdLoPt56PJ8Tc0pwuac3gPUq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An enigma!'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"What was one of the fruits I first asked you about?\")\n",
    "# 这里第二个问题的答案是来自于第一个答案本身的，因此我们使用到了 memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 九，用不到 50 行代码实现一个文档对话机器人\n",
    "假设所有 2022 年更新的内容都存在于 2022.txt 这个文档中，那么通过如下的代码，就可以让 ChatGPT 来支持回答 2022 年的问题\n",
    "\n",
    "其中原理也很简单：\n",
    "\n",
    "对用户的输入/prompt向量化\n",
    "文档分词\n",
    "文档分割\n",
    "文本向量化\n",
    "向量化了才能进行向量之间相似度的计算\n",
    "向量化的文本存到向量数据库里\n",
    "根据用户的输入/prompt去向量数据里寻找答案(答案的判定是基于prompt/输入与文本中相关段落向量的相似性匹配)\n",
    "最后通过LLM返回答案\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.0.258\n",
      "  Using cached https://files.pythonhosted.org/packages/99/b6/f94cd453f69f9f49bb357d5035b7d17fdb213e1212dca560959010886904/langchain-0.0.258-py3-none-any.whl\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (2.8.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (6.0.1)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (0.5.14)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\" in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (4.0.3)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (1.24.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (2.31.0)\n",
      "Collecting pydantic<2,>=1 (from langchain==0.0.258)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/d7/dcc878bac609c805925696d29a9a517802cb53d9d750b17935c22bc2177c/pydantic-1.10.13-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (8.2.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (2.0.20)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (0.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from langchain==0.0.258) (3.8.5)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.258)\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/e7/22abb5a10733bf8142984201aedf27d4a58f5810ebdfe9679f9876c7bf4d/openapi_schema_pydantic-1.2.4-py3-none-any.whl\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.258) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.258) (0.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from requests<3,>=2->langchain==0.0.258) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from requests<3,>=2->langchain==0.0.258) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from requests<3,>=2->langchain==0.0.258) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from requests<3,>=2->langchain==0.0.258) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from pydantic<2,>=1->langchain==0.0.258) (4.7.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.258) (2.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.258) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.258) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.258) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.258) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.258) (1.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.258) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.258) (1.0.0)\n",
      "Installing collected packages: pydantic, openapi-schema-pydantic, langchain\n",
      "  Found existing installation: pydantic 2.3.0\n",
      "    Uninstalling pydantic-2.3.0:\n",
      "      Successfully uninstalled pydantic-2.3.0\n",
      "  Rolling back uninstall of pydantic\n",
      "  Moving to /Users/xuan/Library/Caches/com.apple.python/Users/xuan/Library/Python/3.8/lib/python/site-packages/pydantic/\n",
      "   from /Users/xuan/Library/Caches/com.apple.python/Users/xuan/Library/Python/3.8/lib/python/site-packages/~ydantic\n",
      "  Moving to /Users/xuan/Library/Python/3.8/lib/python/site-packages/pydantic-2.3.0.dist-info/\n",
      "   from /Users/xuan/Library/Python/3.8/lib/python/site-packages/~ydantic-2.3.0.dist-info\n",
      "  Moving to /Users/xuan/Library/Python/3.8/lib/python/site-packages/pydantic/\n",
      "   from /Users/xuan/Library/Python/3.8/lib/python/site-packages/~ydantic\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Library/Python/3.8'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 23.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting openai==0.23.1\n",
      "Requirement already satisfied: pandas>=1.2.3 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from openai==0.23.1) (2.0.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from openai==0.23.1) (4.7.1)\n",
      "Requirement already satisfied: numpy in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from openai==0.23.1) (1.24.4)\n",
      "Requirement already satisfied: tqdm in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from openai==0.23.1) (4.66.1)\n",
      "Collecting pandas-stubs>=1.1.0.11 (from openai==0.23.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/0c/e9/e9550b05e262afc75b014b79224b6910f7cbc85cc40448b709ac90a58ecd/pandas_stubs-2.0.3.230814-py3-none-any.whl\n",
      "Collecting openpyxl>=3.0.7 (from openai==0.23.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/6a/94/a59521de836ef0da54aaf50da6c4da8fb4072fb3053fa71f052fd9399e7a/openpyxl-3.1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.20 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from openai==0.23.1) (2.31.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai==0.23.1) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai==0.23.1) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai==0.23.1) (2.8.2)\n",
      "Collecting types-pytz>=2022.1.1 (from pandas-stubs>=1.1.0.11->openai==0.23.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/79/4f/58c28f50233612d9890cc2b068aee977ef3f7d6434ef4debf98162e34152/types_pytz-2023.3.1.1-py3-none-any.whl\n",
      "Collecting et-xmlfile (from openpyxl>=3.0.7->openai==0.23.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/96/c2/3dd434b0108730014f1b96fd286040dc3bcb70066346f7e01ec2ac95865f/et_xmlfile-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai==0.23.1) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai==0.23.1) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai==0.23.1) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/xuan/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai==0.23.1) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai==0.23.1) (1.15.0)\n",
      "Installing collected packages: types-pytz, pandas-stubs, et-xmlfile, openpyxl, openai\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Library/Python/3.8'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 23.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: chroma in /Users/xuan/Library/Python/3.8/lib/python/site-packages (0.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 23.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jieba in /Users/xuan/Library/Python/3.8/lib/python/site-packages (0.42.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 23.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x84 in position 24: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/xuan/Desktop/langchain应用示例/3_langchain_9_usecases.ipynb 单元格 92\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m chain  \u001b[39m# 返回该对象\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# 在加载之前调用init()进行初始化处理\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m init()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m# 调用load函数，获取ConversationalRetrievalChain对象\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m chain \u001b[39m=\u001b[39m load()  \n",
      "\u001b[1;32m/Users/xuan/Desktop/langchain应用示例/3_langchain_9_usecases.ipynb 单元格 92\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m files:        \u001b[39m# 遍历每个文件\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data/\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:   \u001b[39m# 以读模式打开文件\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread()   \u001b[39m# 读取文件内容\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     cut_data \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(jb\u001b[39m.\u001b[39mcut(data))])       \u001b[39m# 对读取的文件内容进行分词处理\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xuan/Desktop/langchain%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/3_langchain_9_usecases.ipynb#Y160sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     cut_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data/cut/cut_\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m      \u001b[39m# 定义处理后的文件路径和名称\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[1;32m    323\u001b[0m     \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x84 in position 24: invalid start byte"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "# 在我们开始前，安装需要的依赖\n",
    "%pip install langchain==0.0.258\n",
    "%pip install openai==0.23.1\n",
    "\n",
    "%pip install chroma --user\n",
    "\n",
    "import os                            # 导入os模块，用于操作系统相关的操作\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "%pip install jieba --user            # 安装jieba分词库\n",
    "import jieba as jb                   # 导入结巴分词库\n",
    "from langchain.chains import ConversationalRetrievalChain   # 导入用于创建对话检索链的类\n",
    "from langchain.chat_models import ChatOpenAI                # 导入用于创建ChatOpenAI对象的类\n",
    "from langchain.document_loaders import DirectoryLoader      # 导入用于加载文件的类\n",
    "from langchain.embeddings import OpenAIEmbeddings           # 导入用于创建词向量嵌入的类\n",
    "from langchain.text_splitter import TokenTextSplitter       # 导入用于分割文档的类\n",
    "from langchain.vectorstores import Chroma                   # 导入用于创建向量数据库的类\n",
    "\n",
    "if not os.path.exists('./data/cut'):\n",
    "    os.makedirs('./data/cut')\n",
    "\n",
    "# 初始化函数，用于处理输入的文档\n",
    "def init():  \n",
    "    files = ['2022.txt']      # 需要处理的文件列表\n",
    "    for file in files:        # 遍历每个文件\n",
    "        with open(f\"./data/{file}\", 'r', encoding='utf-8') as f:   # 以读模式打开文件\n",
    "            data = f.read()   # 读取文件内容\n",
    "\n",
    "        cut_data = \" \".join([w for w in list(jb.cut(data))])       # 对读取的文件内容进行分词处理\n",
    "        cut_file = f\"./data/cut/cut_{file}\"      # 定义处理后的文件路径和名称\n",
    "        with open(cut_file, 'w') as f:           # 以写模式打开文件\n",
    "            f.write(cut_data)                    # 将处理后的内容写入文件\n",
    "\n",
    "# 新建一个函数用于加载文档\n",
    "def load_documents(directory):  \n",
    "    # 创建DirectoryLoader对象，用于加载指定文件夹内的所有.txt文件\n",
    "    loader = DirectoryLoader(directory, glob='**/*.txt')  \n",
    "    docs = loader.load()  # 加载文件\n",
    "    return docs  # 返回加载的文档\n",
    "\n",
    "# 新建一个函数用于分割文档\n",
    "def split_documents(docs):  \n",
    "    # 创建TokenTextSplitter对象，用于分割文档\n",
    "    text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)  \n",
    "    docs_texts = text_splitter.split_documents(docs)  # 分割加载的文本\n",
    "    return docs_texts  # 返回分割后的文本\n",
    "\n",
    "# 新建一个函数用于创建词嵌入\n",
    "def create_embeddings(api_key):  \n",
    "    # 创建OpenAIEmbeddings对象，用于获取OpenAI的词向量\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=api_key)  \n",
    "    return embeddings  # 返回创建的词嵌入\n",
    "\n",
    "# 新建一个函数用于创建向量数据库\n",
    "def create_chroma(docs_texts, embeddings, persist_directory):  \n",
    "    # 使用文档，embeddings和持久化目录创建Chroma对象\n",
    "    vectordb = Chroma.from_documents(docs_texts, embeddings, persist_directory=persist_directory)  \n",
    "    vectordb.persist()      # 持久化存储向量数据\n",
    "    return vectordb         # 返回创建的向量数据库\n",
    "\n",
    "# load函数，调用上面定义的具有各个职责的函数\n",
    "def load():\n",
    "    docs = load_documents('./data/cut')        # 调用load_documents函数加载文档\n",
    "    docs_texts = split_documents(docs)         # 调用split_documents函数分割文档\n",
    "    \n",
    "    api_key = os.environ.get('OPENAI_API_KEY')   # 从环境变量中获取OpenAI的API密钥\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OpenAI API key is missing. Please set it as an environment variable.\")\n",
    "\n",
    "    embeddings = create_embeddings(api_key)      # 调用create_embeddings函数创建词嵌入\n",
    "\n",
    "    # 调用create_chroma函数创建向量数据库\n",
    "    vectordb = create_chroma(docs_texts, embeddings, './data/cut/')  \n",
    "\n",
    "    # 创建ChatOpenAI对象，用于进行聊天对话\n",
    "    openai_ojb = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")  \n",
    "\n",
    "    # 从模型和向量检索器创建ConversationalRetrievalChain对象\n",
    "    chain = ConversationalRetrievalChain.from_llm(openai_ojb, vectordb.as_retriever())  \n",
    "    return chain  # 返回该对象\n",
    "\n",
    "# 在加载之前调用init()进行初始化处理\n",
    "init()\n",
    "\n",
    "# 调用load函数，获取ConversationalRetrievalChain对象\n",
    "chain = load()  \n",
    "\n",
    "# 定义一个函数，根据输入的问题获取答案\n",
    "def get_ans(question):  \n",
    "    chat_history = []      # 初始化聊天历史为空列表\n",
    "    result = chain({       # 调用chain对象获取聊天结果\n",
    "        'chat_history': chat_history,  # 传入聊天历史\n",
    "        'question': question,          # 传入问题\n",
    "    })\n",
    "    return result['answer']      # 返回获取的答案\n",
    "\n",
    "if __name__ == '__main__':       # 如果此脚本作为主程序运行\n",
    "    s = input('please input:')   # 获取用户输入\n",
    "    while s != 'exit':      # 如果用户输入的不是'exit'\n",
    "        ans = get_ans(s)    # 调用get_ans函数获取答案\n",
    "        print(ans)  # 打印答案\n",
    "        s = input('please input:')  # 获取用户输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgkgX-3mAtQr"
   },
   "source": [
    "## 十，智能体(Agents)\n",
    "\n",
    "Agents是 LLM 中最热门的 🔥 主题之一。\n",
    "\n",
    "Agents可以查看数据、推断下一步应该采取什么行动，并通过工具为您执行该行动, 是一个具备AI智能的决策者。\n",
    "\n",
    "温馨提示：小心使用 Auto GPT, 会迅速消耗掉你大量的token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:38.573253Z",
     "start_time": "2023-09-28T12:33:38.556762Z"
    },
    "id": "37dcZ16fARFV"
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "import os\n",
    "import json\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Agent imports\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# Tool imports\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.utilities import TextRequestsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:38.591305Z",
     "start_time": "2023-09-28T12:33:38.573253Z"
    },
    "id": "8SjEdp2QBMOK"
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_CSE_ID\"] = \"YOUR_GOOGLE_CSE_ID\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:38.605241Z",
     "start_time": "2023-09-28T12:33:38.591305Z"
    },
    "id": "2hgZUkp_BU9H"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:38.950867Z",
     "start_time": "2023-09-28T12:33:38.606760Z"
    },
    "id": "zxYGgmXZBXJW"
   },
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "requests = TextRequestsWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:38.950867Z",
     "start_time": "2023-09-28T12:33:38.950867Z"
    },
    "id": "LZapOD7rBZTG"
   },
   "outputs": [],
   "source": [
    "toolkit = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to search google to answer questions about current events\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"Requests\",\n",
    "        func=requests.get,\n",
    "        description=\"Useful for when you to make a request to a URL\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:38.950867Z",
     "start_time": "2023-09-28T12:33:38.950867Z"
    },
    "id": "G-ZorVNnCBbT"
   },
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:38.950867Z",
     "start_time": "2023-09-28T12:33:38.950867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "1gHznAdQCHTd",
    "outputId": "7c37c3b3-7bd7-4c42-b037-b8a457d2dce4"
   },
   "outputs": [],
   "source": [
    "response = agent({\"input\":\"What is the capital of canada?\"})\n",
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:33:38.950867Z",
     "start_time": "2023-09-28T12:33:38.950867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "9pAhwnmUCJCs",
    "outputId": "4b567eff-76fa-4c9b-ea86-cb3051b6fc09"
   },
   "outputs": [],
   "source": [
    "response = agent({\"input\":\"Tell me what the comments are about on this webpage https://news.ycombinator.com/item?id=34425779\"})\n",
    "response['output']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242.909px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
