{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRCxoaCstcha"
   },
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install tiktoken\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXBtRDTjtgtx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxBdb3TStlW3"
   },
   "source": [
    "#1.Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRpP4GTktovJ",
    "outputId": "6b61d345-c7da-4440-9033-791c1ecb7a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "组合后的用户请求: \n",
      "你作为工作5-10年的程序员回答如下问题。一句话概括。\n",
      "我想学习Python编程，需要哪几个步骤?\n",
      "\n",
      "\n",
      "大语言模型的回应: 需要先学习Python语法和基础知识，然后熟悉各种Python库和框架，最后完成实践项目，以加深对Python编程的理解。\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "#下面的{Coding}作为替换符\n",
    "#加入了Instruction，让LLM知道提问的背景。让它知道调用旅游方面的能力。\n",
    "template = \"\"\"\n",
    "你作为工作5-10年的程序员回答如下问题。一句话概括。\n",
    "我想学习{Coding}编程，需要哪几个步骤?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    #接受用户输入\n",
    "    input_variables=[\"Coding\"],\n",
    "    #定义Prompt tempalte\n",
    "    template=template,\n",
    ")\n",
    "#这里是真正的用户输入 \"Python\"\n",
    "final_prompt = prompt.format(Coding='Python')\n",
    "\n",
    "print (f\"组合后的用户请求: {final_prompt}\")\n",
    "\n",
    "print (f\"大语言模型的回应: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MK7QsPquqhj"
   },
   "source": [
    "#2.ChatMessagePromptTemplate 通过Role实现精准回应\n",
    "LangChain提供了不同类型的MessagePromptTemplate。\n",
    "\n",
    "HumanMessagePromptTemplate：人类消息\n",
    "\n",
    "SystemMessagePromptTemplate：系统消息（Instruction，Conexti）\n",
    "\n",
    "AIMessagePromptTemplate：AI消息\n",
    "\n",
    "可以使用ChatMessagePromptTemplate，它允许用户指定角色名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k339v8Ey2kWS"
   },
   "source": [
    "ChatMessagePromptTemplate 通过role + 模版 的方式生成prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XJttCPUvNCH",
    "outputId": "3d6352b5-01ab-4bac-8e0f-3cadb7b66757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\n回答：\\n\\n如果你想描写一个小学生，可以先着重描述他们的外表，比如他们的脸型、发型、肤色、服装等。你也可以描述他们的动作、语言、情绪和态度，以及他们的语调和表情。此外，你也可以描述小学生的性格和他们的内心世界，比如他们有什么想法和梦想。', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 38, 'completion_tokens': 243, 'total_tokens': 281}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('d0a98e52-0c75-4a99-8fa4-349a0841a43f'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "prompt = \"我想知道如何描写{subject}\"\n",
    "\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"小学生\", template=prompt)\n",
    "formatted_prompt =chat_message_prompt.format(subject=\"人物\")\n",
    "prompts = [str(formatted_prompt)]\n",
    "# 将格式化的提示传递给LLM模型并获取回应\n",
    "response = llm.generate(prompts)\n",
    "\n",
    "# 打印LLM模型的回应\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mXFRjFIYZV6"
   },
   "source": [
    "#3.通过MessagesPlaceholder 实现多类型Message合作\n",
    "LangChain中的\"MessagesPlaceholder\"功能，的应用场景：\n",
    "\n",
    "例如，一个聊天系统，用户和AI进行了多轮的对话，对很多输入的内容进行了分析。\n",
    "\n",
    "在这种情况下，需要对上面的对话进行总结。就需要使用\"MessagesPlaceholder\"讲上面人类输入和AI相应的信息作为模版放入到其中，并且限制总结的字数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvNN2yGL6vou",
    "outputId": "be995678-3dc7-44d1-f08e-2e773dd685c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\n总结：阅读、练习、编辑校对.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 609, 'total_tokens': 641}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('ca52c063-7de2-44bd-ad8b-6ced86bacda9'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.schema.messages import HumanMessage,AIMessage\n",
    "\n",
    "#创建语言模型\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "#人类消息模版\n",
    "human_prompt = \"讲我们上面的对话通过{word_count}个字总结一下.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "#创建聊天模版\n",
    "#定义变量conversation，这个是用来存放多个消息模版的\n",
    "chat_prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=\"conversation\"), human_message_template])\n",
    "\n",
    "#人类输入的信息\n",
    "human_message = HumanMessage(content=\"如何写好小学作文?\")\n",
    "\n",
    "#AI消息回应\n",
    "#使用硬编码，在真实的场景中可以是语言模型返回的结果，或者使保存前几次模型返回的信息。\n",
    "ai_message = AIMessage(content=\"\"\"\\\n",
    "1. 阅读广泛：阅读是提升写作技巧的关键。阅读各种文学作品、新闻报道和其他优秀的写作作品，可以扩展你的词汇量和写作风格，并为你提供灵感和观点.\n",
    "2. 练习写作：写作就像是一项技能，需要不断的练习。坚持写作，并接受反馈和指导，以不断改进你的表达和结构。多样化你的写作练习，包括叙事、说明和议论等不同类型的作文.\n",
    "3. 重视编辑和校对：写作并不仅仅是把思绪表达出来，而是经过多次修改和润色才能达到最佳效果。花时间编辑和校对你的作文，确保语法准确、句子流畅，同时注意段落结构和逻辑连贯\\\n",
    "\"\"\")\n",
    "\n",
    "#调用MessagesPlaceholder 分别插入human_message（人类的输入）, ai_message（模型的历史回应），最后得到总结\n",
    "formatted_prompt = chat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=\"10\").to_messages()\n",
    "\n",
    "prompts = [str(formatted_prompt)]\n",
    "# 将格式化的提示传递给LLM模型并获取回应\n",
    "response = llm.generate(prompts)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq2utmvOrsAp"
   },
   "source": [
    "#4 Partial prompt templates 模版的部分加载\n",
    "\n",
    "LangChain支持两种方式进行部分填充：\n",
    "\n",
    "字符串值进行部分格式化。\n",
    "返回字符串值的函数进行部分格式化。\n",
    "\n",
    "\n",
    "日期的函数就是函数部分格式化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qw_uQlxZwBWV"
   },
   "source": [
    "##4.1 字符串方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VvCLD02rtsb",
    "outputId": "39512a46-59f4-4905-b0e0-b08695d30823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\n小明一边跳起来一边把球往篮筐里扔，小心翼翼地调整身体的姿势，把球尽可能地投进篮筐。他又站起来，移动到篮下，用双手把球接住，又用身体从不同的角度把球投进篮筐，运用起跳和抛投的技巧，努力把球投进篮筐。', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 233, 'prompt_tokens': 15, 'total_tokens': 248}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('299de547-f23d-4849-bf49-63789b797fe0'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 初始化一个OpenAI模型\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# 创建一个PromptTemplate，它需要两个输入变量：\"{姓名}\"和\"{动作}\"\n",
    "prompt = PromptTemplate(template=\"{姓名}正在{动作}\", input_variables=[\"姓名\", \"动作\"])\n",
    "\n",
    "# 使用partial方法，预先填充\"姓名\"变量为\"小明\"\n",
    "partial_prompt = prompt.partial(姓名=\"小明\")\n",
    "\n",
    "# 使用format方法，填充剩余的\"动作\"变量为\"打篮球\"，并将结果转化为字符串\n",
    "formatted_prompt = str(partial_prompt.format(动作=\"打篮球\"))\n",
    "\n",
    "# 将格式化的提示传递给LLM模型并获取回应\n",
    "response = llm.generate([formatted_prompt])\n",
    "\n",
    "# 打印LLM模型的回应\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9VgQzSfv_PJ"
   },
   "source": [
    "##4.2 函数方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D58GpNj1v-Jz",
    "outputId": "a4750667-8553-4b56-eda6-92dbe303ac21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\n2023年07月17日, 07点30分32秒，一个科学家在做一次实验，他让时间停止，结果实验失败了，他发现自己只能再次停留在07点30分32秒，科学家大声喊：“我又回到这里了！”', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 171, 'prompt_tokens': 48, 'total_tokens': 219}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('8c4d08c5-8572-4ee9-933c-3db02b5b7f6e'))]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 初始化一个OpenAI模型\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# 定义一个函数，返回当前的日期和时间\n",
    "def _get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y年%m月%d日, %H点%M分%S秒\")\n",
    "\n",
    "# 创建一个PromptTemplate，它需要两个输入变量：\"{adjective}\"和\"{date}\"\n",
    "prompt = PromptTemplate(\n",
    "    template=\"给我讲一个关于{date}的{adjective}笑话\",\n",
    "    input_variables=[\"adjective\", \"date\"]\n",
    ")\n",
    "\n",
    "# 使用partial方法，预先填充\"date\"变量为当前的日期和时间\n",
    "partial_prompt = prompt.partial(date=_get_datetime)\n",
    "\n",
    "# 使用format方法，填充剩余的\"adjective\"变量为\"有趣\"，并将结果转化为字符串\n",
    "formatted_prompt = str(partial_prompt.format(adjective=\"有趣\"))\n",
    "\n",
    "# 将格式化的提示传递给LLM模型并获取回应\n",
    "response = llm.generate([formatted_prompt])\n",
    "\n",
    "# 打印LLM模型的回应\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTdwNoaTFCMY"
   },
   "source": [
    "#5.组合Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2lWULfszwAp",
    "outputId": "2bfddc14-cada-4b3a-f669-4e4c2fe899eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text=' 我最喜欢的社交媒体网站是推特。', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 148, 'total_tokens': 182}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('887e466a-755c-4b80-8938-485088d6c81e'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 创建LLM实例\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# 1 定义最终的提示模板\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "# 定义\"introduction\"部分的提示模板\n",
    "introduction_template = \"\"\"你正在模仿{person}。\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "# 定义\"example\"部分的提示模板\n",
    "example_template = \"\"\"这是一个互动的例子:\n",
    "\n",
    "问: {example_q}\n",
    "答: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "# 定义\"start\"部分的提示模板\n",
    "start_template = \"\"\"现在，开始真正的互动！\n",
    "\n",
    "问: {input}\n",
    "答:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)\n",
    "\n",
    "# 将所有部分的提示模板组合到一起\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n",
    "\n",
    "# 格式化管道提示并生成LLM的回应\n",
    "formatted_prompt = pipeline_prompt.format(\n",
    "    person=\"小马哥\",\n",
    "    example_q=\"你最喜欢的车是什么？\",\n",
    "    example_a=\"特斯拉\",\n",
    "    input=\"你最喜欢的社交媒体网站是什么？\"\n",
    ")\n",
    "response = llm.generate([str(formatted_prompt)])\n",
    "\n",
    "# 打印LLM模型的回应\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usarbNyEFI9W"
   },
   "source": [
    "#6.Prompt template 序列化\n",
    "\n",
    "支持JSON和YAML两种格式。\n",
    "\n",
    "支持在一个文件中指定所有内容，或者将不同的组件（模板、示例等）存储在不同的文件中并进行引用。\n",
    "\n",
    "提供了一个加载提示的单一入口点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bw2n8qo0FOAv",
    "outputId": "b0d87997-8a71-4b30-eade-545c0ea203b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text=' 无聊', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 84, 'total_tokens': 90}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('90ab7118-ff83-4629-9903-3e2553b6ed92'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "# 加载提示模板\n",
    "prompt = load_prompt('few_shot_prompt_examples_in.json')\n",
    "\n",
    "# 使用LLM生成回应\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "response = llm.generate([str(prompt.format(adjective=\"有趣\"))])\n",
    "\n",
    "# 打印LLM模型的回应\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiLP7HguTJqx"
   },
   "source": [
    "#7.Example Selectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lelx-sxMU59J"
   },
   "source": [
    "##7.1Select by length（根据Prompt 控制示例长度）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuOhPWVaU_gq",
    "outputId": "880cf761-c2a4-467b-e936-1811195df92f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给出每个输入的反义词\n",
      "\n",
      "输入: 开心\n",
      "输出: 难过\n",
      "\n",
      "输入: 高\n",
      "输出: 矮\n",
      "\n",
      "输入: 精力充沛\n",
      "输出: 无精打采\n",
      "\n",
      "输入: 阳光明媚\n",
      "输出: 阴沉\n",
      "\n",
      "输入: 风大\n",
      "输出: 风平浪静\n",
      "\n",
      "输入: 热情\n",
      "输出:\n",
      "测试例子\n",
      "generations=[[Generation(text=' 冷漠', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 185, 'completion_tokens': 6, 'prompt_tokens': 179}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('47f3b47b-d3ff-45a8-8259-97d8b6a30a66'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 使用LLM生成回应\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# 这些是一些假设任务的例子，创建反义词。\n",
    "examples = [\n",
    "    {\"input\": \"开心\", \"output\": \"难过\"},\n",
    "    {\"input\": \"高\", \"output\": \"矮\"},\n",
    "    {\"input\": \"精力充沛\", \"output\": \"无精打采\"},\n",
    "    {\"input\": \"阳光明媚\", \"output\": \"阴沉\"},\n",
    "    {\"input\": \"风大\", \"output\": \"风平浪静\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"输入: {input}\\n输出: {output}\",\n",
    ")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    # 这些是它可以选择的例子。\n",
    "    examples=examples,\n",
    "    # 这是用来格式化例子的PromptTemplate。\n",
    "    example_prompt=example_prompt,\n",
    "    # 这是格式化的例子应该有的最大长度。\n",
    "    # 长度是通过下面的get_text_length函数来测量的。\n",
    "    max_length=30,\n",
    "\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # 我们提供一个ExampleSelector，而不是例子。\n",
    "    #限定了长度的示例选择器\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"给出每个输入的反义词\",\n",
    "    suffix=\"输入: {adjective}\\n输出:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# 一个有小输入的例子，所以它选择了所有的例子。\n",
    "print(dynamic_prompt.format(adjective=\"热情\"))\n",
    "\n",
    "# 你可以使用你的语言模型来生成输出\n",
    "print(\"测试例子\")\n",
    "output = llm.generate([dynamic_prompt.format(adjective=\"热情\")])\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGTvQnGAUtjg"
   },
   "source": [
    "##7.2 Select by similarity（选择与Prompt 类似的示例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7b-f1m3TNj4",
    "outputId": "6e1c9de1-e134-42c0-9d9d-3c5c93ad8350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给出一个事物通常所在的位置\n",
      "\n",
      "示例输入: 鸟\n",
      "示例输出: 鸟巢\n",
      "\n",
      "示例输入: 树\n",
      "示例输出: 土地\n",
      "\n",
      "输入: 花朵\n",
      "输出:\n",
      "generations=[[Generation(text=' 花园', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 119, 'completion_tokens': 4, 'prompt_tokens': 115}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('cf9fb91a-1f51-4df1-a36a-13ecd45d29f3'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 创建一个LLM模型实例\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# 创建一个用于格式化示例的PromptTemplate实例\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"示例输入: {input}\\n示例输出: {output}\",\n",
    ")\n",
    "\n",
    "# 创建一个示例列表\n",
    "examples = [\n",
    "    {\"input\": \"老师\", \"output\": \"教室\"},\n",
    "    {\"input\": \"医生\", \"output\": \"医院\"},\n",
    "    {\"input\": \"司机\", \"output\": \"汽车\"},\n",
    "    {\"input\": \"树\", \"output\": \"土地\"},\n",
    "    {\"input\": \"鸟\", \"output\": \"鸟巢\"},\n",
    "]\n",
    "\n",
    "# 创建一个SemanticSimilarityExampleSelector实例，用于选择与输入语义相似的示例\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,  # 可供选择的示例列表\n",
    "    OpenAIEmbeddings(),  # 用于生成嵌入向量的嵌入类\n",
    "    FAISS,  # 向量数据库用来存放示例，为比较做准备\n",
    "    k=2  # 要选择的示例数量，返回相似的示例数字为2\n",
    ")\n",
    "\n",
    "# 创建一个FewShotPromptTemplate实例\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # 用于选择示例的对象\n",
    "    example_prompt=example_prompt,  # 用于格式化示例的PromptTemplate实例\n",
    "    prefix=\"给出一个事物通常所在的位置\",  # 提示的前缀 instruction\n",
    "    suffix=\"输入: {noun}\\n输出:\",  # 提示的后缀\n",
    "    input_variables=[\"noun\"],  # 提示将接收的输入变量\n",
    ")\n",
    "\n",
    "# 选择一个名词\n",
    "my_noun = \"花朵\"\n",
    "\n",
    "# 格式化提示并打印\n",
    "print(similar_prompt.format(noun=my_noun))\n",
    "\n",
    "# 使用LLM模型生成回应\n",
    "response = llm.generate([str(similar_prompt.format(noun=my_noun))])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaUmIAQiliUf"
   },
   "source": [
    "#8.LLMS\n",
    "LLMs是LangChain的核心组件，LangChain并不提供自己的LLMs，而是提供了一个与多种LLMs交互的标准接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHIkumIillSt",
    "outputId": "10c2b096-7be7-48eb-db46-148a3f923c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "两个熊在森林里玩耍，一只熊说：“我累了，我要休息一会儿。”另一只熊问：“那你怎么休息？”第一只熊说：“我要睡觉！”另一只熊说：“你怎么睡觉？你没有床！”第一只熊说：“没关系，我可以睡在树上！”\n",
      "10\n",
      "[Generation(text='\\n\\n一个人去买帽子，卖家问：“要几只？”顾客答：“不要，我是来看样的。”', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n",
      "[Generation(text='\\n\\n若梦里不见你\\n不曾知你离去\\n心痛如刀割\\n无奈泪滂沱\\n\\n情意犹未抹\\n多少年未曾言\\n心思不曾离\\n等待着你归来', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 创建一个LLM模型实例\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# 使用__call__方法生成文本\n",
    "joke = llm(\"给我讲个笑话\")\n",
    "print(joke)\n",
    "\n",
    "# 使用generate方法生成文本\n",
    "#请求数组中有两个prompt ，后面通过* 5 告诉 LLM，我要对LLM发起10次请求，因此会得到10个响应。\n",
    "#由于这里是例子，实际想表达的是LLM可以接受批量的prompt 请求\n",
    "#批量发起prompt 请求\n",
    "responses = llm.generate([\"给我讲个笑话\", \"给我写首诗\"]*5)\n",
    "\n",
    "# 请求的次数\n",
    "print(len(responses.generations))\n",
    "\n",
    "# 输出第一个生成的文本\n",
    "print(responses.generations[0])\n",
    "\n",
    "# 输出最后一个生成的文本\n",
    "print(responses.generations[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m12kpuam5kIw"
   },
   "source": [
    "#9.Caching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL1WFua6_UT_"
   },
   "source": [
    "##9.1 内存缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2gg_RB_9Q1s",
    "outputId": "8340a30c-51be-4746-86a2-8f20c26a0bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!\n",
      "预测花费了 0.7900259494781494 seconds.\n",
      "\n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!\n",
      "预测花费了 0.0003459453582763672 seconds.\n"
     ]
    }
   ],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "import time\n",
    "\n",
    "\n",
    "# 设置内存缓存\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# 初始化一个大型语言模型\n",
    "llm = OpenAI()\n",
    "\n",
    "# 第一次预测，结果未被缓存，所以需要较长时间\n",
    "\n",
    "start_time = time.time()  # 获取当前时间\n",
    "result1 = llm.predict(\"给我说个笑话\")\n",
    "print(result1)\n",
    "end_time = time.time()  # 获取当前时间\n",
    "elapsed_time = end_time - start_time  # 计算经过的时间\n",
    "print(f\"预测花费了 {elapsed_time} seconds.\")\n",
    "\n",
    "\n",
    "# 第二次预测相同的输入，由于结果已经被缓存，所以预测速度会更快\n",
    "\n",
    "start_time = time.time()  # 获取当前时间\n",
    "result2 = llm.predict(\"给我说个笑话\")\n",
    "print(result2)\n",
    "end_time = time.time()  # 获取当前时间\n",
    "elapsed_time = end_time - start_time  # 计算经过的时间\n",
    "print(f\"预测花费了 {elapsed_time} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKzEBpou_X7b"
   },
   "source": [
    "##9.2数据库缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4s87u8yb_4Va",
    "outputId": "c7a14cb9-1251-4452-84f7-ad86fb419a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "一位老太太去买鱼，店主问她：“你要买活鱼还是死鱼？”\n",
      "老太太说：“还活着的！”店主说：“那可不行，你得先把它们杀死！”老太太恼怒地说：“这么说，你是在告诉我，我老了！”\n",
      "预测花费了 12.39306902885437 seconds.\n",
      "\n",
      "\n",
      "一位老太太去买鱼，店主问她：“你要买活鱼还是死鱼？”\n",
      "老太太说：“还活着的！”店主说：“那可不行，你得先把它们杀死！”老太太恼怒地说：“这么说，你是在告诉我，我老了！”\n",
      "预测花费了 0.0021202564239501953 seconds.\n"
     ]
    }
   ],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "import time\n",
    "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n",
    "\n",
    "start_time = time.time()  # 获取当前时间\n",
    "# 第一次，它还不在缓存中，所以应该需要更长的时间\n",
    "result1= llm.predict(\"给我讲个笑话\")\n",
    "print(result1)\n",
    "end_time = time.time()  # 获取当前时间\n",
    "elapsed_time = end_time - start_time  # 计算经过的时间\n",
    "print(f\"预测花费了 {elapsed_time} seconds.\")\n",
    "\n",
    "start_time = time.time()  # 获取当前时间\n",
    "# 第二次，它在缓存中，时间会短一些\n",
    "result2= llm.predict(\"给我讲个笑话\")\n",
    "print(result2)\n",
    "end_time = time.time()  # 获取当前时间\n",
    "elapsed_time = end_time - start_time  # 计算经过的时间\n",
    "print(f\"预测花费了 {elapsed_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI7Y5_iIhFn3"
   },
   "source": [
    "#10.FakeListLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "dlXUMdg0hKo4",
    "outputId": "edc463d0-e609-432c-b56d-664975019b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: print(2 + 2)\u001b[0m\n",
      "Observation: Python REPL is not a valid tool, try another one.\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入所需的库和模块\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "# 加载名为\"python_repl\"的工具\n",
    "tools = load_tools([\"python_repl\"])\n",
    "\n",
    "# 定义一个模拟的LLM响应列表\n",
    "responses = [\"Action: Python REPL\\nAction Input: print(2 + 2)\", \"Final Answer: 4\"]\n",
    "\n",
    "# 创建一个FakeListLLM实例，它会按照预定义的响应列表来响应提示\n",
    "llm = FakeListLLM(responses=responses)\n",
    "\n",
    "# 使用这些工具和LLM初始化一个代理\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "# 运行代理以回答问题\"2 + 2是多少?\"\n",
    "agent.run(\"whats 2 + 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5dM3ATj1v_Z"
   },
   "source": [
    "#11.异步调用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB41e7Hv1zdf"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "# 定义一个串行生成的函数\n",
    "def generate_serially():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    for _ in range(5):\n",
    "        resp = llm.generate([\"我是串行函数的信息\"])\n",
    "        print(resp.generations[0][0].text)\n",
    "\n",
    "# 定义一个异步生成的函数\n",
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate([\"我是异步函数的信息\"])\n",
    "    print(resp.generations[0][0].text)\n",
    "\n",
    "# 定义一个并发生成的函数\n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    tasks = [async_generate(llm) for _ in range(5)]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "# 计算并发执行的时间\n",
    "s = time.perf_counter()\n",
    "await generate_concurrently()\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"并发执行在 {elapsed:0.2f} 秒内完成.\" + \"\\033[0m\")\n",
    "\n",
    "# 计算串行执行的时间\n",
    "s = time.perf_counter()\n",
    "generate_serially()\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"串行执行在 {elapsed:0.2f} 秒内完成.\" + \"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg86A32n8Pe1"
   },
   "source": [
    "#12.流式响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGBaL9xg8UAU",
    "outputId": "4493e70c-bda1-4c16-f6d3-62abe23eae1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "好的，我给你唱一首《Let It Go》：\n",
      "\n",
      "The snow glows white on the mountain tonight\n",
      "Not a footprint to be seen\n",
      "A kingdom of isolation, and it looks like I'm the queen\n",
      "The wind is howling like this swirling storm inside\n",
      "Couldn't keep it in, Heaven knows I tried\n",
      "\n",
      "Don't let them in, don't let them see\n",
      "Be the good girl you always have to be\n",
      "Conceal, don't feel, don't let them know\n",
      "Well, now they know\n",
      "\n",
      "Let it go, let it go\n",
      "Can't hold it back anymore\n",
      "Let it go, let it go\n",
      "Turn away and slam the door\n",
      "I don't care what they're going to say\n",
      "Let the storm rage on\n",
      "The cold never bothered me anyway\n",
      "\n",
      "It's funny how some distance makes everything seem small\n",
      "And the fears that once controlled me can't get to me at all\n",
      "It's time to see what I can do\n",
      "To test the limits and break through\n",
      "No right, no wrong, no rules for me, I'm free\n",
      "\n",
      "Let it go, let it go\n",
      "I"
     ]
    }
   ],
   "source": [
    "# 导入所需的库\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 创建一个使用流式响应和回调处理器的 OpenAI LLM 实例\n",
    "llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "\n",
    "# 使用 LLM 生成一首关于气泡水的歌曲\n",
    "resp = llm(\"给我唱首歌。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sFmv1n_OARO"
   },
   "source": [
    "#13.Output parser\n",
    "\n",
    "\"setup\"和\"punchline\"是为了解析一个笑话的结构而设置的。\"setup\"是笑话的开头部分，通常是一个问题，而\"punchline\"是笑话的结尾部分，通常是对问题的回答或者是笑话的高潮部分。\n",
    "\n",
    "如果你想解析其他类型的文本，你可以定义其他的字段。例如，如果你想解析一个新闻报道，你可能会定义\"headline\"（标题）、\"author\"（作者）、\"date\"（日期）和\"content\"（内容）等字段。这完全取决于你想要解析的文本的结构和你的需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "By4CcQInODpR",
    "outputId": "3b87666c-2cf2-4fd5-b755-754241f2ef35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "model = OpenAI(model_name='text-davinci-003', temperature=0.0)\n",
    "\n",
    "# 定义所需的数据结构。\n",
    "class Joke(BaseModel):\n",
    "    #定义字段\n",
    "    #这里假设笑话是一问一答的方式进行的。\n",
    "    setup: str = Field(description=\"提出一个问题\")\n",
    "    punchline: str = Field(description=\"回答，产生一个笑话\")\n",
    "\n",
    "\n",
    "# 设置解析器并将指令注入到提示模板中。\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    #template 中定义了两个变量：\n",
    "    #一个是控制输出的format_instructions：定义两个字段和一个验证方法\n",
    "    #一个是控制输入的query\n",
    "    template=\"回应用户的查询。\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# 构建一个查询，旨在提示语言模型填充数据结构。\n",
    "joke_query = \"说一个笑话\"\n",
    "input = prompt.format_prompt(query=joke_query)\n",
    "output = model(input.to_string())\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUOlv1WBX5ck"
   },
   "source": [
    "#14.Parser List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRzbUmiQX8hr",
    "outputId": "2ba135bc-4bd1-4647-8bad-445e997d536f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['草莓', '抹茶', '巧克力', '香草', '杏仁']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入需要的库\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 创建一个逗号分隔的列表输出解析器\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# 获取格式化指示\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# 创建一个提示模板\n",
    "prompt = PromptTemplate(\n",
    "    template=\"列出五种{subject}。\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# 创建一个OpenAI模型实例\n",
    "model = OpenAI(temperature=0)\n",
    "\n",
    "# 使用提示模板和主题输入变量来格式化输入\n",
    "_input = prompt.format(subject=\"冰淇淋口味\")\n",
    "\n",
    "# 使用模型实例来生成输出\n",
    "output = model(_input)\n",
    "\n",
    "# 使用输出解析器来解析输出\n",
    "output_parser.parse(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2pNYwFJYf_p"
   },
   "source": [
    "#15.Date time parser 日期解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srsslE2AYnEu",
    "outputId": "02365167-166f-427c-c425-23bb4b2eae69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 12, 25, 0, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入需要的库\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 创建一个日期时间输出解析器\n",
    "output_parser = DatetimeOutputParser()\n",
    "\n",
    "# 定义一个字符串模板\n",
    "template = \"\"\"回答用户的问题:\n",
    "\n",
    "{question}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "\n",
    "# 创建一个提示模板\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# 创建一个LLMChain实例\n",
    "chain = LLMChain(prompt=prompt, llm=OpenAI())\n",
    "\n",
    "# 使用LLMChain实例来运行一个问题\n",
    "output = chain.run(\"2020 年的圣诞节是什么时候？\")\n",
    "\n",
    "# 使用输出解析器来解析输出\n",
    "output_parser.parse(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L75vCtrbZwgf"
   },
   "source": [
    "#16.Auto-fixing parser （自动修复解析器）\n",
    "`OutputFixingParser`是一个特殊的输出解析器，它的工作方式是：如果原始的解析器（在这里是`parser`）无法解析输入的字符串（在这里是`misformatted`），那么`OutputFixingParser`会使用一个语言模型（在这里是`ChatOpenAI`）来尝试“修复”输入的字符串，使其能够被原始的解析器解析。\n",
    "\n",
    "具体来说，`OutputFixingParser`会将无法解析的字符串和解析器期望的格式指示一起作为提示传递给语言模型，然后语言模型会生成一个新的字符串，这个新的字符串应该能够被原始的解析器解析。这就是为什么`new_parser.parse(misformatted)`能够成功解析字符串的原因。\n",
    "\n",
    "需要注意的是，这种方法并不总是能够成功。语言模型可能无法生成一个能够被原始解析器解析的字符串，或者生成的字符串可能并不符合我们的期望。因此，使用`OutputFixingParser`时需要谨慎，并且可能需要对其输出进行额外的检查或处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "aSVa4yB1Z1Tp",
    "outputId": "1be84cc8-c505-4378-99b2-f25a5b665724"
   },
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-54efd6ea9ca9>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 使用解析器来解析这个错误格式化的字符串\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisformatted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Failed to parse {name} from completion {text}. Got: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Actor from completion {'name': '孙悟空', 'film_names': ['大闹天宫']}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "# 导入需要的库\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# 定义一个名为\"Actor\"的数据结构，包含\"name\"（名字）和\"film_names\"（电影名字列表）两个字段\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"演员的名字\")\n",
    "    film_names: List[str] = Field(description=\"他们主演的电影的名字列表\")\n",
    "\n",
    "# 创建一个PydanticOutputParser实例，用于解析\"Actor\"数据结构\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "# 定义一个错误格式化的字符串\n",
    "misformatted = \"{'name': '孙悟空', 'film_names': ['大闹天宫']}\"\n",
    "#misformatted = '{\"name\": \"孙悟空\", \"film_names\": [\"大闹天宫\"]}'\n",
    "\n",
    "\n",
    "# 使用解析器来解析这个错误格式化的字符串\n",
    "parser.parse(misformatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sj-jkduQbQT-",
    "outputId": "a8d1ad7c-7828-47c5-adc6-30913a8d7e48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(name='孙悟空', film_names=['大闹天宫'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())\n",
    "new_parser.parse(misformatted)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0MK7QsPquqhj"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
