{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "VqgEAYP0MiqM",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "03e7c4ae-1792-4cdf-d561-44fb15ae4e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langchain in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.1.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.5.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.0.28)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.31 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.1.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.1.36)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.5.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: openai in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (0.23.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (0.16.3)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rfc3986[idna2008]<2,>=1.3->httpx<1,>=0.23.0->openai) (1.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpcore<0.17.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tiktoken in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting Chromadb\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b6/87/68c070b4919087f08e6fa528044db1348d99ea4a13c78aca6366c3545f12/chromadb-0.5.7-py3-none-any.whl (599 kB)\n",
      "     ---------------------------------------- 0.0/599.2 kB ? eta -:--:--\n",
      "     --- ----------------------------------- 61.4/599.2 kB 1.7 MB/s eta 0:00:01\n",
      "     --------- ---------------------------- 153.6/599.2 kB 1.8 MB/s eta 0:00:01\n",
      "     ------------------- ------------------ 307.2/599.2 kB 2.4 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 471.0/599.2 kB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  593.9/599.2 kB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 599.2/599.2 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting build>=1.0.3 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/91/fd/e4bda6228637ecae5732162b5ac2a5a822e2ba8e546eb4997cde51b231a3/build-1.2.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (2.8.2)\n",
      "Collecting chroma-hnswlib==0.7.6 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/63/ee3e8b7a8f931918755faacf783093b61f32f59042769d9db615999c3de0/chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl (150 kB)\n",
      "     ---------------------------------------- 0.0/151.0 kB ? eta -:--:--\n",
      "     ------------------------------------ - 143.4/151.0 kB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 151.0/151.0 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (0.112.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->Chromadb) (0.30.6)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (1.23.5)\n",
      "Collecting posthog>=2.4.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c4/3e/53cf7e3ead6c0a5f4064fa4e697a2985bace0aae0ed428983774213a3485/posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 0.0/54.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 54.3/54.3 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (4.9.0)\n",
      "Collecting onnxruntime>=1.14.1 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3c/d8/68b63dc86b502169d017a86fe8bc718f4b0055ef1f6895bfaddd04f2eead/onnxruntime-1.19.2-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "     ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/11.1 MB 4.2 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.3/11.1 MB 3.8 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.3/11.1 MB 3.8 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.5/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.6/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.8/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.9/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/11.1 MB 3.0 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.2/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.2/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.4/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.5/11.1 MB 2.6 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.6/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 1.7/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 1.9/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.0/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.2/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.3/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.4/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 2.6/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 2.6/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 2.8/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.0/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.2/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.3/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.5/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.6/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 3.7/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 3.9/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.0/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.1/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 4.3/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 4.5/11.1 MB 3.0 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 4.6/11.1 MB 3.0 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 4.6/11.1 MB 3.0 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 4.8/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 4.9/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 4.9/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.1/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.2/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 5.3/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 5.4/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 5.5/11.1 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 5.6/11.1 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 5.7/11.1 MB 2.8 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 5.8/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 5.9/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.0/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 6.1/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 6.2/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 6.3/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.4/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.5/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.6/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 6.7/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 6.8/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 6.9/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 7.0/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 7.1/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 7.2/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 7.2/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 7.4/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 7.4/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 7.5/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 7.6/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 7.7/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.8/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.9/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.9/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 8.0/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 8.1/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 8.2/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 8.3/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 8.4/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 8.4/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 8.5/11.1 MB 2.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.6/11.1 MB 2.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.7/11.1 MB 2.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.8/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.9/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.9/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 9.0/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 9.1/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.2/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.3/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.3/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.4/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.5/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.6/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.7/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.8/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.9/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 10.0/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 10.1/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 10.1/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 10.2/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.4/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.4/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.5/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.6/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.7/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.8/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.9/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.0/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.1/11.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 11.1/11.1 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-api>=1.2.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fb/1f/737dcdbc9fea2fa96c1b392ae47275165a7c641663fbb08a8d252968eed2/opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 0.0/64.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 64.0/64.0 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8d/80/32217460c2c64c0568cea38410124ff680a9b65f6732867bbf857c4d8626/opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ee/50/745ab075a3041b7a5f29a579d2c28eaad54f64b4589d8f9fd364c62cf0f3/opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c1/bd/a6602e71e315055d63b2ff07172bd2d012b4cba2d4e00735d74ba42fc4d6/opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "     ---------------------------------------- 0.0/110.5 kB ? eta -:--:--\n",
      "     -------------------------------- ------ 92.2/110.5 kB 5.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 92.2/110.5 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ 110.5/110.5 kB 911.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (0.15.2)\n",
      "Collecting pypika>=0.48.9 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c7/2c/94ed7b91db81d61d7096ac8f2d325ec562fc75e35f3baea8749c85b28784/PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB 3.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (4.66.1)\n",
      "Collecting overrides>=7.3.1 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2c/ab/fc8290c6a4c722e5514d80f62b2dc4c4df1a68a41d1364e625c35990fcf3/overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (6.4.0)\n",
      "Collecting grpcio>=1.58.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f3/72/6046088fa273d2c4fe72009d2411d5ccd053017014b1197c4881ead3ee70/grpcio-1.66.1-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/4.3 MB 3.6 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.2/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.3/4.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.4/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.5/4.3 MB 2.2 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.6/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.7/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.8/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.8/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.0/4.3 MB 2.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.0/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 1.1/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 1.3/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.4/4.3 MB 2.2 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.4/4.3 MB 2.0 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.6/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 1.7/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 1.8/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 1.9/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 1.9/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 2.1/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 2.2/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 2.2/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 2.3/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 2.4/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 2.5/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 2.6/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 2.7/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 2.8/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.9/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.9/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 3.0/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 3.1/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 3.1/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 3.1/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 3.2/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 3.3/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 3.4/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 3.4/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 3.5/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 3.6/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 3.6/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 3.7/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 3.8/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.9/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 4.0/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 4.0/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 4.1/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.2/4.3 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.2/4.3 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.3/4.3 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting bcrypt>=4.0.1 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1c/2a/c74052e54162ec639266d91539cca7cbf3d1d3b8b36afbfeaee0ea6a1702/bcrypt-4.2.0-cp39-abi3-win_amd64.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 143.4/151.7 kB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 151.7/151.7 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (0.12.5)\n",
      "Collecting kubernetes>=28.1.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/62/a1/2027ddede72d33be2effc087580aeba07e733a7360780ae87226f1f91bd8/kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/1.7 MB 3.6 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.2/1.7 MB 2.8 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.3/1.7 MB 2.5 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.4/1.7 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.5/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.5/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.6/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.7/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 0.8/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.9/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.9/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.9/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.1/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.2/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.3/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.4/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.5/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.5/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.6/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8e/50/4e629064bc48c17a970b6897bb799b2fd1774cc136231ab34ab9471c0e2d/mmh3-5.0.0-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (3.9.15)\n",
      "Collecting httpx>=0.27.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/95/9377bcb415797e44274b51d46e3249eba641711cf3348050f76ee7b15ffc/httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "     ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 71.7/76.4 kB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 76.4/76.4 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (13.0.1)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from build>=1.0.3->Chromadb) (23.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ae/f3/431b9d5fe7d14af7a32340792ef43b8a714e7726f1d7b69cc4e8e7a3f1d7/pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from build>=1.0.3->Chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from build>=1.0.3->Chromadb) (2.0.1)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from fastapi>=0.95.2->Chromadb) (0.38.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->Chromadb) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->Chromadb) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/d4/e5d7e4f2174f8a4d63c8897d79eb8fe2503f7ecc03282fee1fa2719c2704/httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 0.0/77.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.9/77.9 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->Chromadb) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->Chromadb) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->Chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (2.8.2)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bb/fb/9af9e3f2996677bdda72734482934fe85a3abde174e5f0783ac2f817ba98/google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\n",
      "     ---------------------------------------- 0.0/200.9 kB ? eta -:--:--\n",
      "     ----------------------- -------------- 122.9/200.9 kB 3.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 194.6/200.9 kB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 200.9/200.9 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (0.58.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ------------------------------ ------- 122.9/151.7 kB 3.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 143.4/151.7 kB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 151.7/151.7 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (1.26.14)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a7/06/3d6badcf13db419e25b07041d9c7b4a2c331d3f4e7134445ec5df57714cd/coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\admin\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->Chromadb) (4.24.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->Chromadb) (1.11.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->Chromadb) (1.2.14)\n",
      "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c0/14/362d31bf1076b21e1bcdcb0dc61944822ff263937b804a79231df2774d28/importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/08/49bfe7cf737952cc1a9c43e80cc258ed45dad7f183c5b8276fc94cb3862d/googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "     ---------------------------------------- 0.0/220.9 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 41.0/220.9 kB ? eta -:--:--\n",
      "     --------------------------------- ---- 194.6/220.9 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 220.9/220.9 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/41/27/4610ab3d9bb3cde4309b6505f98b3aabca04a26aa480aa18cede23149837/opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/94/56/3d2d826834209b19a5141eed717f7922150224d1a982385d19a9444cbf8d/opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 0.0/52.5 kB ? eta -:--:--\n",
      "     ---------------------------------------  51.2/52.5 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 52.5/52.5 kB 541.3 kB/s eta 0:00:00\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/74/a0e0d38622856597dd8e630f2bd793760485eb165708e11b8be1696bbb5a/opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0a/7f/405c41d4f359121376c9d5117dcf68149b8122d3f6c718996d037bd4d800/opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b7/7a/4f0063dbb0b6c971568291a8bc19a4ca70d3c185db2d956230dd67429dfc/opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "     ---------------------------------------- 0.0/149.7 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/149.7 kB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 92.2/149.7 kB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ------ 122.9/149.7 kB 901.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 149.7/149.7 kB 893.9 kB/s eta 0:00:00\n",
      "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ad/2e/36097c0a4d0115b8c7e377c90bab7783ac183bc5cb4071308f8959454311/opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb) (65.6.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/39/e3/893e8757be2612e6c266d9bb58ad2e3651524b5b40cf56761e985a28b13e/asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->Chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->Chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic>=1.9->Chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic>=1.9->Chromadb) (2.20.1)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich>=10.11.0->Chromadb) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich>=10.11.0->Chromadb) (2.11.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->Chromadb) (0.20.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer>=0.9.0->Chromadb) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer>=0.9.0->Chromadb) (1.5.4)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1e/fc/8a26c2adcd3f141e4729897633f03832b71ebea6f4c31cce67a92ded1961/httptools-0.6.1-cp310-cp310-win_amd64.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.2 kB ? eta -:--:--\n",
      "     ----------------------------------- ---- 51.2/58.2 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 58.2/58.2 kB 773.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->Chromadb) (1.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fe/1b/5148898ba55fc9c111a2a4a5fb67ad3fa7eb2b3d7f0618241ed88749313d/watchfiles-0.24.0-cp310-none-win_amd64.whl (277 kB)\n",
      "     ---------------------------------------- 0.0/277.5 kB ? eta -:--:--\n",
      "     -------- ------------------------------ 61.4/277.5 kB 1.7 MB/s eta 0:00:01\n",
      "     ---------------- --------------------- 122.9/277.5 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 225.3/277.5 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 277.5/277.5 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7b/f9/83bc78788d6ce5492fa44133708584a885080aa7c790be2532f326948115/websockets-13.0.1-cp310-cp310-win_amd64.whl (152 kB)\n",
      "     ---------------------------------------- 0.0/152.2 kB ? eta -:--:--\n",
      "     ----------------------- --------------- 92.2/152.2 kB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 152.2/152.2 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->Chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->Chromadb) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->Chromadb) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->Chromadb) (2023.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->Chromadb) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->Chromadb) (2.0.4)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f0/0f/310fb31e39e2d734ccaa2c0fb981ee41f7bd5056ce9bc29b2248bd569169/humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 81.9/86.8 kB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 86.8/86.8 kB 981.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->Chromadb) (1.2.1)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ee/2d/a308abc94a1ee028ef99cb5385fb756d61022b7adbb5361683cd380cf575/pyreadline3-3.5.3-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 0.0/79.9 kB ? eta -:--:--\n",
      "     ----------------------------------- ---- 71.7/79.9 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 79.9/79.9 kB 893.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->Chromadb) (0.4.8)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53834 sha256=1ddfc57ae53f8fbfd2e969887cbc6b098ff5f1d922214fded6d11ae0452835b9\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\06\\08\\01\\e1ce9a2b2e9e51d8540e77461c4cc395c902b9020c296aae50\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, flatbuffers, websockets, rsa, pyreadline3, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, oauthlib, mmh3, importlib-metadata, httptools, httpcore, grpcio, googleapis-common-protos, chroma-hnswlib, bcrypt, asgiref, watchfiles, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, humanfriendly, httpx, google-auth, build, opentelemetry-semantic-conventions, opentelemetry-instrumentation, kubernetes, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation-asgi, onnxruntime, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, Chromadb\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.16.3\n",
      "    Uninstalling httpcore-0.16.3:\n",
      "      Successfully uninstalled httpcore-0.16.3\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.23.3\n",
      "    Uninstalling httpx-0.23.3:\n",
      "      Successfully uninstalled httpx-0.23.3\n",
      "Successfully installed Chromadb-0.5.7 asgiref-3.8.1 bcrypt-4.2.0 build-1.2.2 chroma-hnswlib-0.7.6 coloredlogs-15.0.1 flatbuffers-24.3.25 google-auth-2.34.0 googleapis-common-protos-1.65.0 grpcio-1.66.1 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.2 humanfriendly-10.0 importlib-metadata-8.4.0 kubernetes-30.1.0 mmh3-5.0.0 oauthlib-3.2.2 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 overrides-7.7.0 posthog-3.6.6 pypika-0.48.9 pyproject_hooks-1.1.0 pyreadline3-3.5.3 requests-oauthlib-2.0.0 rsa-4.9 watchfiles-0.24.0 websockets-13.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.29.0 requires importlib-metadata<7,>=1.4, but you have importlib-metadata 8.4.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install tiktoken\n",
    "!pip install faiss-cpu\n",
    "!pip install Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AEZ1MOtANzWC"
   },
   "outputs": [],
   "source": [
    "#  \n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk8soY5IOzR6"
   },
   "source": [
    "#1.Conversation buffer memory\n",
    "ConversationBufferMemoryverbose=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMO5LT0oUG0M"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojNEF8JON_jy",
    "outputId": "6b7bd05e-2de9-46eb-d248-00b1fae73f5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'), AIMessage(content='whats up')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "#\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyzDjxcAUNRx"
   },
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "SDbjpm1PPDym",
    "outputId": "151ea883-0523-4f4f-b422-9157e8b1e14c"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 193] %1  Win32 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationChain\n\u001b[1;32m----> 5\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m conversation \u001b[38;5;241m=\u001b[39m ConversationChain(\n\u001b[0;32m      7\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m      8\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     memory\u001b[38;5;241m=\u001b[39mConversationBufferMemory()\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m conversation\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\langchain_core\\load\\serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\pydantic\\v1\\main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\langchain_community\\llms\\openai.py:300\u001b[0m, in \u001b[0;36mBaseOpenAI.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    294\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_organization\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    295\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_organization\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_ORG_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_ORGANIZATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    298\u001b[0m )\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import openai python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install openai`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_os\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoneType, Transport, ProxiesTypes\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_from_path\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\types\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m Image\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model \u001b[38;5;28;01mas\u001b[39;00m Model\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionDefinition \u001b[38;5;28;01mas\u001b[39;00m FunctionDefinition, FunctionParameters \u001b[38;5;28;01mas\u001b[39;00m FunctionParameters\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\types\\image.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImage\u001b[39;00m(BaseModel):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\_models.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerics\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FieldInfo\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Body,\n\u001b[0;32m     24\u001b[0m     IncEx,\n\u001b[0;32m     25\u001b[0m     Query,\n\u001b[0;32m     26\u001b[0m     ModelT,\n\u001b[0;32m     27\u001b[0m     Headers,\n\u001b[0;32m     28\u001b[0m     Timeout,\n\u001b[0;32m     29\u001b[0m     NotGiven,\n\u001b[0;32m     30\u001b[0m     AnyMapping,\n\u001b[0;32m     31\u001b[0m     HttpxRequestFiles,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_list, is_given, is_mapping, parse_date, parse_datetime, strip_not_given\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     PYDANTIC_V2,\n\u001b[0;32m     36\u001b[0m     ConfigDict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     field_get_default,\n\u001b[0;32m     46\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\_types.py:21\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     IO,\n\u001b[0;32m      6\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     Sequence,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, Protocol, TypeAlias, TypedDict, override, runtime_checkable\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m URL, Proxy, Timeout, Response, BaseTransport, AsyncBaseTransport\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__version__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __description__, __title__, __version__\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_auth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\_api.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_TIMEOUT_CONFIG\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Response\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\_client.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ASGITransport\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncBaseTransport, BaseTransport\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncHTTPTransport, HTTPTransport\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwsgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WSGITransport\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     AsyncByteStream,\n\u001b[0;32m     36\u001b[0m     AuthTypes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     VerifyTypes,\n\u001b[0;32m     50\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\_transports\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwsgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\_transports\\default.py:33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TracebackType\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpcore\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_LIMITS, Limits, Proxy, create_ssl_context\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     ConnectError,\n\u001b[0;32m     38\u001b[0m     ConnectTimeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     WriteTimeout,\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m request, stream\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_async\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     AsyncConnectionInterface,\n\u001b[0;32m      4\u001b[0m     AsyncConnectionPool,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     AsyncSOCKSProxy,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     SOCKET_OPTION,\n\u001b[0;32m     13\u001b[0m     AsyncNetworkBackend,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     NetworkStream,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\_api.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterator, Optional, Union\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m URL, Extensions, HeaderTypes, Response\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sync\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionPool\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m      9\u001b[0m     method: Union[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m     10\u001b[0m     url: Union[URL, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     extensions: Optional[Extensions] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Sends an HTTP request, returning the response.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m        An instance of `httpcore.Response`.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPConnection\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionPool\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp11\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTP11Connection\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Origin, Request, Response\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ssl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_ssl_context\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_synchronization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lock\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trace\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp11\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTP11Connection\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\_synchronization.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Our async synchronization primatives use either 'anyio' or 'trio' depending\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# on if they're running under asyncio or trio.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtrio\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     trio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\__init__.py:22\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# General layout:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# trio/_core/... is the self-contained core library. It does various\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# pyright explicitly does not care about `__version__`\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# see https://github.com/microsoft/pyright/blob/main/docs/typed-libraries.md#type-completeness\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     TrioInternalError \u001b[38;5;28;01mas\u001b[39;00m TrioInternalError,\n\u001b[0;32m     24\u001b[0m     RunFinishedError \u001b[38;5;28;01mas\u001b[39;00m RunFinishedError,\n\u001b[0;32m     25\u001b[0m     WouldBlock \u001b[38;5;28;01mas\u001b[39;00m WouldBlock,\n\u001b[0;32m     26\u001b[0m     Cancelled \u001b[38;5;28;01mas\u001b[39;00m Cancelled,\n\u001b[0;32m     27\u001b[0m     BusyResourceError \u001b[38;5;28;01mas\u001b[39;00m BusyResourceError,\n\u001b[0;32m     28\u001b[0m     ClosedResourceError \u001b[38;5;28;01mas\u001b[39;00m ClosedResourceError,\n\u001b[0;32m     29\u001b[0m     run \u001b[38;5;28;01mas\u001b[39;00m run,\n\u001b[0;32m     30\u001b[0m     open_nursery \u001b[38;5;28;01mas\u001b[39;00m open_nursery,\n\u001b[0;32m     31\u001b[0m     CancelScope \u001b[38;5;28;01mas\u001b[39;00m CancelScope,\n\u001b[0;32m     32\u001b[0m     current_effective_deadline \u001b[38;5;28;01mas\u001b[39;00m current_effective_deadline,\n\u001b[0;32m     33\u001b[0m     TASK_STATUS_IGNORED \u001b[38;5;28;01mas\u001b[39;00m TASK_STATUS_IGNORED,\n\u001b[0;32m     34\u001b[0m     current_time \u001b[38;5;28;01mas\u001b[39;00m current_time,\n\u001b[0;32m     35\u001b[0m     BrokenResourceError \u001b[38;5;28;01mas\u001b[39;00m BrokenResourceError,\n\u001b[0;32m     36\u001b[0m     EndOfChannel \u001b[38;5;28;01mas\u001b[39;00m EndOfChannel,\n\u001b[0;32m     37\u001b[0m     Nursery \u001b[38;5;28;01mas\u001b[39;00m Nursery,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_timeouts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     move_on_at \u001b[38;5;28;01mas\u001b[39;00m move_on_at,\n\u001b[0;32m     42\u001b[0m     move_on_after \u001b[38;5;28;01mas\u001b[39;00m move_on_after,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     TooSlowError \u001b[38;5;28;01mas\u001b[39;00m TooSlowError,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sync\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     52\u001b[0m     Event \u001b[38;5;28;01mas\u001b[39;00m Event,\n\u001b[0;32m     53\u001b[0m     CapacityLimiter \u001b[38;5;28;01mas\u001b[39;00m CapacityLimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     Condition \u001b[38;5;28;01mas\u001b[39;00m Condition,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\_core\\__init__.py:27\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ki\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     enable_ki_protection,\n\u001b[0;32m     22\u001b[0m     disable_ki_protection,\n\u001b[0;32m     23\u001b[0m     currently_ki_protected,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Imports that always exist\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_run\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     Task,\n\u001b[0;32m     29\u001b[0m     CancelScope,\n\u001b[0;32m     30\u001b[0m     run,\n\u001b[0;32m     31\u001b[0m     open_nursery,\n\u001b[0;32m     32\u001b[0m     checkpoint,\n\u001b[0;32m     33\u001b[0m     current_task,\n\u001b[0;32m     34\u001b[0m     current_effective_deadline,\n\u001b[0;32m     35\u001b[0m     checkpoint_if_cancelled,\n\u001b[0;32m     36\u001b[0m     TASK_STATUS_IGNORED,\n\u001b[0;32m     37\u001b[0m     current_statistics,\n\u001b[0;32m     38\u001b[0m     current_trio_token,\n\u001b[0;32m     39\u001b[0m     reschedule,\n\u001b[0;32m     40\u001b[0m     remove_instrument,\n\u001b[0;32m     41\u001b[0m     add_instrument,\n\u001b[0;32m     42\u001b[0m     current_clock,\n\u001b[0;32m     43\u001b[0m     current_root_task,\n\u001b[0;32m     44\u001b[0m     spawn_system_task,\n\u001b[0;32m     45\u001b[0m     current_time,\n\u001b[0;32m     46\u001b[0m     wait_all_tasks_blocked,\n\u001b[0;32m     47\u001b[0m     wait_readable,\n\u001b[0;32m     48\u001b[0m     wait_writable,\n\u001b[0;32m     49\u001b[0m     notify_closing,\n\u001b[0;32m     50\u001b[0m     Nursery,\n\u001b[0;32m     51\u001b[0m     start_guest_run,\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Has to come after _run to resolve a circular import\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     56\u001b[0m     cancel_shielded_checkpoint,\n\u001b[0;32m     57\u001b[0m     Abort,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     reattach_detached_coroutine_object,\n\u001b[0;32m     63\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\_core\\_run.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ki\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOCALS_KEY_KI_PROTECTION_ENABLED, KIManager, enable_ki_protection\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multierror\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiError, concat_tb\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_thread_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m start_thread_soon\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     36\u001b[0m     Abort,\n\u001b[0;32m     37\u001b[0m     CancelShieldedCheckpoint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     wait_task_rescheduled,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\_core\\_thread_cache.py:66\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m partial(namefunc, pthread_setname_np)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# construct os thread name method\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m set_os_thread_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_os_thread_name_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# The \"thread cache\" is a simple unbounded thread pool, i.e., it automatically\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# spawns as many threads as needed to handle all the requests its given. Its\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# only purpose is to cache worker threads so that they don't have to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# How long a thread will idle waiting for new work before gives up and exits.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# This value is pretty arbitrary; I don't think it matters too much.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m IDLE_TIMEOUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# seconds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\_core\\_thread_cache.py:42\u001b[0m, in \u001b[0;36mget_os_thread_name_func\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m libpthread_path:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m libpthread \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibpthread_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# get the setname method from it\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# afaik this should never fail\u001b[39;00m\n\u001b[0;32m     46\u001b[0m pthread_setname_np \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(libpthread, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpthread_setname_np\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 193] %1  Win32 "
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "9qT2CZPVTnZc",
    "outputId": "6b3acd08-08bb-426e-c2a5-8bce4c500700"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mconversation\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AI.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conversation' is not defined"
     ]
    }
   ],
   "source": [
    "conversation.predict(input=\" AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "dkwpnGtwTx6n",
    "outputId": "67d81ac0-465b-40c0-9e82-1fc266c6d269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: !\n",
      "AI:  ! AI\n",
      "Human:  AI.\n",
      "AI:  \n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' AI'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "BuMtLywVPYxS",
    "outputId": "9d087e1f-6d95-4655-f96b-de459a51168d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: !\n",
      "AI:  ! AI\n",
      "Human:  AI.\n",
      "AI:  \n",
      "Human: \n",
      "AI:  AI\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6PjM7SRUbH4"
   },
   "source": [
    "#2.Conversation buffer window memory\n",
    "memory  ConversationBufferWindowMemory k \n",
    "\n",
    "\n",
    "ConversationBufferWindowMemoryK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ef4FfvIOU2Ix",
    "outputId": "5a7ef9f6-3b6b-4329-d605-813ea41ff560"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: not much you\\nAI: not much\\nHuman: \\nAI: '}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "#K\n",
    "memory = ConversationBufferWindowMemory( k=2)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "Q5OUy69vVTBi",
    "outputId": "9252ba7e-fa3a-4162-fc4f-cb359010f164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' AI'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    #k=2 \n",
    "    memory=ConversationBufferWindowMemory(k=1),\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "HWtOEisdVmAL",
    "outputId": "d384513f-9329-4656-b228-7576b38cdf6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: \n",
      "AI:  AI\n",
      "Human: ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "_kBGXA9aVsJk",
    "outputId": "176c28de-01f7-406a-be48-592c3c89431e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ?\n",
      "AI:  \n",
      "Human: wifi?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"wifi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "67mC58iPVvMR",
    "outputId": "062221fa-74ce-4299-be10-5a97313c1598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ?\n",
      "AI:  \n",
      "Human: wifi?\n",
      "AI:  wifi\n",
      "Human: ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation_with_summary.predict(input=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "4C5YRD09WhsG",
    "outputId": "9bc98f46-2395-4944-d505-ebd07bb413db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: wifi?\n",
      "AI:  wifi\n",
      "Human: ?\n",
      "AI:  \n",
      "Human: ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'  '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If3JRWAWX3r4"
   },
   "source": [
    "#3.Conversation summary memory\n",
    "ConversationSummaryMemory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7tYEORZHX6rK",
    "outputId": "16e33dcc-db4a-4e0c-b7f0-dabc425907ac"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nThe AI asked if the human had something they wanted to discuss.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(temperature=0), return_messages=True)\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "messages = memory.chat_memory.messages\n",
    "#\n",
    "previous_summary = \"\"\n",
    "#(messages)(previous_summary)\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWBQXyAyYpKe"
   },
   "source": [
    "ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "gUgOpMvNYiqh",
    "outputId": "d57c22a8-f314-4c13-c49a-e40038e84df8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nThe human greets the AI, to which the AI responds with a friendly greeting, addressing the human by name. The human then states that they like to eat apples, to which the AI responds that they also like to eat them.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"hi\")\n",
    "history.add_ai_message(\"hi there!\")\n",
    "history.add_user_message(\"\")\n",
    "history.add_ai_message(\"!\")\n",
    "history.add_user_message(\"\")\n",
    "history.add_ai_message(\"\")\n",
    "\n",
    "memory = ConversationSummaryMemory.from_messages(llm=OpenAI(temperature=0), chat_memory=history, return_messages=True)\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "5raAQ5Z9bNGw",
    "outputId": "a1d24ec6-af17-4ecd-9a09-7de5bd9da309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' AI'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryMemory(llm=OpenAI()),\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "GBD8cB-xbVBj",
    "outputId": "708eaf9f-2b1a-4ce4-8ece-75a203f9af9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human greeted the AI, to which the AI replied happily and asked if the human had any questions or if they wanted to talk.\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "AUd2_L80bXlF",
    "outputId": "e50390aa-2cca-4da1-c910-e12af6754460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "The human greeted the AI, to which the AI replied happily and asked if the human had any questions or if they wanted to talk. The human mentioned that they were working on a project, and the AI was impressed and asked what kind of project it was and what the goals were.\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4B8j80FvcbU"
   },
   "source": [
    "#4.Conversation Summary Buffer Memory\n",
    "max_token_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "pvB7-b70viit",
    "outputId": "cdf79bcb-d876-4e1f-8d1a-4c93c34b280a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nThe human greets the AI and asks if there is anything they can help with. The AI responds that there is nothing, and the human continues the conversation by asking what fruit and sports the AI likes. The AI responds that it likes apples and basketball, and its favorite team is the All-Stars. Finally, the AI states that its favorite subject is math.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, max_token_limit=300, return_messages=True\n",
    ")\n",
    "#\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "\n",
    "messages = memory.chat_memory.messages\n",
    "previous_summary = \"\"\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "XY2VDPfUxWo_",
    "outputId": "4f27332d-7f3b-44b7-af4b-b546154ee85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'  AI-Robo'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=50),\n",
    "    verbose=True,\n",
    ")\n",
    "#\n",
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "6skLrUT5xeMb",
    "outputId": "f85b2eee-85ab-4ae4-f052-2faa3ac62780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds, introducing itself as AI-Robo and stating its purpose is to help the human solve problems. The AI then asks if the human has any questions.\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "ch4Fax89xgAX",
    "outputId": "949ac50c-3346-4f97-c68b-30900218aa90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI introduces itself as AI-Robo and states its purpose is to help the human solve problems. The AI then asks if the human has any questions and the human responds that they are working on a project. The AI understands and asks if there is any specific problem they can help the human solve.\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "pmXlayc1U0M0",
    "outputId": "b020a750-899a-42ce-8f2e-efc32c22ac7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI introduces itself as AI-Robo and states its purpose is to help the human solve problems. The AI then asks if the human has any questions and the human responds that they are working on a project. The AI understands and asks if there is any specific problem they can help the human solve. The AI then acknowledges that the human likes to eat apples and mentions there are various types of apples, such as red apples, green apples, and yellow apples, and asks the human which type they like best.\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "OPCRqgbWWau-",
    "outputId": "717600a6-62b3-4f07-af24-c320fc1917f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "\n",
      "The human greets the AI and the AI introduces itself as AI-Robo and states its purpose is to help the human solve problems. The AI then asks if the human has any questions and the human responds that they are working on a project. The AI understands and asks if there is any specific problem they can help the human solve. The AI then acknowledges that the human likes to eat apples and mentions there are various types of apples, such as red apples, green apples, and yellow apples, and asks the human which type they like best. The human then states they have encountered a problem and the AI responds that it can try to help the human solve it and asks what the problem is.\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "KSR-7WFfWkAL",
    "outputId": "9b15c523-d9ce-4de8-94b6-07618daf7198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI introduces itself as AI-Robo and states its purpose is to help the human solve problems. The AI then asks if the human has any questions and the human responds that they are working on a project. The AI understands and mentions they like basketball, while AI-Robo does not, but enjoys data analysis and helping to solve problems. The AI then acknowledges that the human likes to eat apples and mentions there are various types of apples, before asking the human if they have any problems they need help solving.\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6CPrEA12fcY"
   },
   "source": [
    "#5.Vector store-backed memory\n",
    "VectorStoreRetrieverMemoryVectorDBK\n",
    "\n",
    "memory\n",
    "\n",
    "memoryretrievermemory\n",
    "\n",
    "memorymemory.save_context\n",
    "\n",
    "memory\n",
    "\n",
    "memorymemory.load_memory_variablesmemory\n",
    "\n",
    "memorymemorymemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMI4m5E-2wDU"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import faiss\n",
    "\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = OpenAIEmbeddings().embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BINprgjg3E4X",
    "outputId": "5dde74be-b175-4f50-e0dd-17068ecdbda3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "output: \n"
     ]
    }
   ],
   "source": [
    "# vectorstoreretriever\n",
    "# k  k=1\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\n",
    "#\n",
    "#\n",
    "#\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# \n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"...\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "\n",
    "print(memory.load_memory_variables({\"prompt\": \"?\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "kFF-52mF3TPE",
    "outputId": "2c7f02c5-a210-4a06-b3b9-37be3c3d3966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAIAIAI\n",
      "\n",
      "\n",
      "input: \n",
      "output: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Perry?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' PerryAI'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0) # LLM\n",
    "_DEFAULT_TEMPLATE = \"\"\"AIAIAI\n",
    "\n",
    "\n",
    "{history}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "{input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    "    # max_token_limit\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Perry?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "Bnza1vgH8K3a",
    "outputId": "ad29b952-04b7-451e-8c19-8869816ae744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAIAIAI\n",
      "\n",
      "\n",
      "input: \n",
      "output: ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "conversation_with_summary.predict(input=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "fcx5bFDp8ZBp",
    "outputId": "33e230db-eab3-4ddb-c527-e89012609475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAIAIAI\n",
      "\n",
      "\n",
      "input: \n",
      "output: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "XxNChDEG8iuj",
    "outputId": "b92e03ed-4e4d-407b-cfa2-603bc1fa82a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAIAIAI\n",
      "\n",
      "\n",
      "input: Perry?\n",
      "response:  PerryAI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Perry'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory\n",
    "conversation_with_summary.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY4fnn_6KMbl"
   },
   "source": [
    "#6.How to customize conversational memory\n",
    "\"AI Prefix\"\"Human Prefix\"AI\"AI:\"\"Human:\"\n",
    "\n",
    "\"AI Prefix\"\"AI\"\"Human Prefix\"\"Human\"\"AI Prefix\"\"AI Assistant\"\"Human Prefix\"\"Friend\"AI\"AI Assistant:\"\"Friend:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CF7sI4z2KQt2"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "# Now we can override it and set it to \"AI Assistant\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "AIAIAI\n",
    "\n",
    "\n",
    "{history}\n",
    "human{input}\n",
    "AI\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"AI \",human_prefix=\"\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "4SgD_dJBKiMI",
    "outputId": "2a2749fd-c7c9-4601-df15-e0d0b5d3db81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AIAIAI\n",
      "\n",
      "\n",
      ": \n",
      "AI : 20-25\n",
      "human\n",
      "AI\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'25-30'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dup-b2IMLOy-"
   },
   "source": [
    "#7.How to add memory to a Multi-Input Chain\n",
    "\n",
    "\n",
    "Chain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrTVC3QaLc8d",
    "outputId": "c79b14c0-5168-4976-e7f8-0a93f3a1dc8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open(\"story.txt\") as f:\n",
    "    file_story = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(file_story)\n",
    "\n",
    "#\n",
    "embeddings = OpenAIEmbeddings()\n",
    "#\n",
    "docsearch = Chroma.from_texts(\n",
    "    texts, embeddings, metadatas=[{\"source\": i} for i in range(len(texts))]\n",
    ")\n",
    "\n",
    "query = \"\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scyD7clXPeUl"
   },
   "source": [
    "input_documentshuman_input\n",
    "\n",
    "input_documents\n",
    "\n",
    "human_inputinput_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIP_sRuuMzgs",
    "outputId": "c8270881-b597-40f8-8672-714e4140e45b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' '}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template =  \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    ": {human_input}\n",
    ":\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\", \"context\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "#chain type stuffprompt\n",
    "chain = load_qa_chain(\n",
    "    OpenAI(temperature=0), chain_type=\"stuff\", memory=memory, prompt=prompt\n",
    ")\n",
    "\n",
    "query = \"\"\n",
    "\n",
    "chain({\"input_documents\": docs, \"human_input\": query}, return_only_outputs=True)\n",
    "#print(chain.memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzeU7oskUMlJ",
    "outputId": "29b86d66-827d-4250-c89e-d0ddebd1c285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' '}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\n",
    "\n",
    "chain({\"input_documents\": docs, \"human_input\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bMoIf3iORs8",
    "outputId": "ee2dec8f-ddfd-4d00-f510-c9e304993a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "AI:  \n",
      "Human: \n",
      "AI:  \n",
      "Human: \n",
      "AI:  \n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HypRyESg18Q"
   },
   "source": [
    "#8.Entity memory\n",
    "\n",
    "Entity Memory\n",
    "\n",
    "AI\n",
    "\n",
    "AIMaxMaxAIMaxAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KodHxDMFpZM"
   },
   "source": [
    "##8.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HT3duAelg71F",
    "outputId": "5e01339e-fb31-4d6c-97f7-134b2617affb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human:  \\nAI:   ',\n",
       " 'entities': {'': '',\n",
       "  '': '',\n",
       "  '': '',\n",
       "  '': '',\n",
       "  '': ''}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "memory = ConversationEntityMemory(llm=llm)\n",
    "_input = {\"input\": \" \"}\n",
    "\n",
    "#entity memory entity\n",
    "memory.load_memory_variables(_input)\n",
    "\n",
    "memory.save_context(\n",
    "    _input,\n",
    "    {\"output\": \"  \"}\n",
    ")\n",
    "\n",
    "\n",
    "memory.load_memory_variables({\"input\": ''})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ez-GubxJFwOV"
   },
   "source": [
    "##8.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gC8yCtoNhIsL",
    "outputId": "5fe13251-6ae8-439d-c0d1-623d779e8908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'': '', '': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: \n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'': '', '': ''}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    #entity\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    #entity\n",
    "    memory=ConversationEntityMemory(llm=llm)\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"\")\n",
    "\n",
    "conversation.memory.entity_store.store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3TqjvBjDmdr",
    "outputId": "881359cd-80f8-430c-ea28-109bf2b81812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'': '', '': '', '': '', '': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: \n",
      "AI:  \n",
      "Human: \n",
      "AI:  \n",
      "Last line:\n",
      "Human: \n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'': '',\n",
       " '': '',\n",
       " '': '',\n",
       " '': ''}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"\")\n",
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U-blolKHc5y"
   },
   "source": [
    "#9.Conversation Knowledge Graph Memory\n",
    "\"(Conversation Knowledge Graph Memory)\"\"Sam\"\"Sam\"\n",
    "\n",
    "\"Sam\"\"Sam\"\"\"\"Sam\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VT-vvDd7F001",
    "outputId": "ba8a5463-4b69-4bd2-8f1f-4732f638df30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On :   .  is a neighbor.   .', additional_kwargs={}),\n",
       "  SystemMessage(content='On :   .   .   .  is a neighbor.  is a classmate.   .', additional_kwargs={}),\n",
       "  SystemMessage(content='On :  is a classmate.   .   .', additional_kwargs={})]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True)\n",
    "memory.save_context({\"input\": \" \"}, {\"output\": \"\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"okay\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"really?\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"really?\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "\n",
    "memory.load_memory_variables({\"input\": \"\"})\n",
    "#memory.get_current_entities({\"input\": \"what does Xiaoming like?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "I1OCo_7YZb_s"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "template =  \"\"\"AIAI\n",
    "AIAI\n",
    "\n",
    "\n",
    "\n",
    "{history}\n",
    "\n",
    "\n",
    ": {input}\n",
    "AI\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, verbose=True, prompt=prompt, memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "mDoRTreBJy3u",
    "outputId": "aa5a68e8-f559-47c7-cd2b-f5407482c899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAIAI\n",
      "AIAI\n",
      "\n",
      "\n",
      "\n",
      "On :   .\n",
      "\n",
      "\n",
      ": \n",
      "AI\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input=\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
