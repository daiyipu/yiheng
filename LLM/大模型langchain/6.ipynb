{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "VqgEAYP0MiqM",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "03e7c4ae-1792-4cdf-d561-44fb15ae4e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langchain in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.1.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.5.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.0.28)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.31 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.1.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (0.1.36)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.5.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: openai in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (0.23.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (0.16.3)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rfc3986[idna2008]<2,>=1.3->httpx<1,>=0.23.0->openai) (1.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpcore<0.17.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tiktoken in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting Chromadb\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b6/87/68c070b4919087f08e6fa528044db1348d99ea4a13c78aca6366c3545f12/chromadb-0.5.7-py3-none-any.whl (599 kB)\n",
      "     ---------------------------------------- 0.0/599.2 kB ? eta -:--:--\n",
      "     --- ----------------------------------- 61.4/599.2 kB 1.7 MB/s eta 0:00:01\n",
      "     --------- ---------------------------- 153.6/599.2 kB 1.8 MB/s eta 0:00:01\n",
      "     ------------------- ------------------ 307.2/599.2 kB 2.4 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 471.0/599.2 kB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  593.9/599.2 kB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 599.2/599.2 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting build>=1.0.3 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/91/fd/e4bda6228637ecae5732162b5ac2a5a822e2ba8e546eb4997cde51b231a3/build-1.2.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (2.8.2)\n",
      "Collecting chroma-hnswlib==0.7.6 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/63/ee3e8b7a8f931918755faacf783093b61f32f59042769d9db615999c3de0/chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl (150 kB)\n",
      "     ---------------------------------------- 0.0/151.0 kB ? eta -:--:--\n",
      "     ------------------------------------ - 143.4/151.0 kB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 151.0/151.0 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (0.112.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->Chromadb) (0.30.6)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (1.23.5)\n",
      "Collecting posthog>=2.4.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c4/3e/53cf7e3ead6c0a5f4064fa4e697a2985bace0aae0ed428983774213a3485/posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 0.0/54.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 54.3/54.3 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (4.9.0)\n",
      "Collecting onnxruntime>=1.14.1 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3c/d8/68b63dc86b502169d017a86fe8bc718f4b0055ef1f6895bfaddd04f2eead/onnxruntime-1.19.2-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "     ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/11.1 MB 4.2 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.3/11.1 MB 3.8 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.3/11.1 MB 3.8 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.5/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.6/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.8/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.9/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/11.1 MB 3.0 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.2/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.2/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.4/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.5/11.1 MB 2.6 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.6/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 1.7/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 1.9/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.0/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.2/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.3/11.1 MB 2.7 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.4/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 2.6/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 2.6/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 2.8/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.0/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.2/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.3/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.5/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.6/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 3.7/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 3.9/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.0/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.1/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 4.3/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 4.5/11.1 MB 3.0 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 4.6/11.1 MB 3.0 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 4.6/11.1 MB 3.0 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 4.8/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 4.9/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 4.9/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.1/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.2/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 5.3/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 5.4/11.1 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 5.5/11.1 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 5.6/11.1 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 5.7/11.1 MB 2.8 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 5.8/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 5.9/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.0/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 6.1/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 6.2/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 6.3/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.4/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.5/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 6.6/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 6.7/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 6.8/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 6.9/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 7.0/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 7.1/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 7.2/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 7.2/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 7.4/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 7.4/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 7.5/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 7.6/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 7.7/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.8/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.9/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.9/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 8.0/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 8.1/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 8.2/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 8.3/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 8.4/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 8.4/11.1 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 8.5/11.1 MB 2.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.6/11.1 MB 2.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.7/11.1 MB 2.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.8/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.9/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.9/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 9.0/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 9.1/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.2/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.3/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.3/11.1 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.4/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.5/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.6/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.7/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.8/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.9/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 10.0/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 10.1/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 10.1/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 10.2/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.4/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.4/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.5/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.6/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.7/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.8/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.9/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.0/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.1/11.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 11.1/11.1 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-api>=1.2.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fb/1f/737dcdbc9fea2fa96c1b392ae47275165a7c641663fbb08a8d252968eed2/opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 0.0/64.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 64.0/64.0 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8d/80/32217460c2c64c0568cea38410124ff680a9b65f6732867bbf857c4d8626/opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ee/50/745ab075a3041b7a5f29a579d2c28eaad54f64b4589d8f9fd364c62cf0f3/opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c1/bd/a6602e71e315055d63b2ff07172bd2d012b4cba2d4e00735d74ba42fc4d6/opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "     ---------------------------------------- 0.0/110.5 kB ? eta -:--:--\n",
      "     -------------------------------- ------ 92.2/110.5 kB 5.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 92.2/110.5 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ 110.5/110.5 kB 911.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (0.15.2)\n",
      "Collecting pypika>=0.48.9 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c7/2c/94ed7b91db81d61d7096ac8f2d325ec562fc75e35f3baea8749c85b28784/PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB 3.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (4.66.1)\n",
      "Collecting overrides>=7.3.1 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2c/ab/fc8290c6a4c722e5514d80f62b2dc4c4df1a68a41d1364e625c35990fcf3/overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (6.4.0)\n",
      "Collecting grpcio>=1.58.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f3/72/6046088fa273d2c4fe72009d2411d5ccd053017014b1197c4881ead3ee70/grpcio-1.66.1-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/4.3 MB 3.6 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.2/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.3/4.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.4/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.5/4.3 MB 2.2 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.6/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.7/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.8/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.8/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.0/4.3 MB 2.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.0/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 1.1/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 1.3/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.4/4.3 MB 2.2 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.4/4.3 MB 2.0 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.6/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 1.7/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 1.8/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 1.9/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 1.9/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 2.1/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 2.2/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 2.2/4.3 MB 2.1 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 2.3/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 2.4/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 2.5/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 2.6/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 2.7/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 2.8/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.9/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.9/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 3.0/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 3.1/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 3.1/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 3.1/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 3.2/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 3.3/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 3.4/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 3.4/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 3.5/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 3.6/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 3.6/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 3.7/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 3.8/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.9/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 4.0/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 4.0/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 4.1/4.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.2/4.3 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.2/4.3 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.3/4.3 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting bcrypt>=4.0.1 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1c/2a/c74052e54162ec639266d91539cca7cbf3d1d3b8b36afbfeaee0ea6a1702/bcrypt-4.2.0-cp39-abi3-win_amd64.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 143.4/151.7 kB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 151.7/151.7 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (0.12.5)\n",
      "Collecting kubernetes>=28.1.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/62/a1/2027ddede72d33be2effc087580aeba07e733a7360780ae87226f1f91bd8/kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/1.7 MB 3.6 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.2/1.7 MB 2.8 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.3/1.7 MB 2.5 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.4/1.7 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.5/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.5/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.6/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.7/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 0.8/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.9/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.9/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.9/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.1/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.2/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.3/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.4/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.5/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.5/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.6/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8e/50/4e629064bc48c17a970b6897bb799b2fd1774cc136231ab34ab9471c0e2d/mmh3-5.0.0-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (3.9.15)\n",
      "Collecting httpx>=0.27.0 (from Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/95/9377bcb415797e44274b51d46e3249eba641711cf3348050f76ee7b15ffc/httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "     ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 71.7/76.4 kB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 76.4/76.4 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Chromadb) (13.0.1)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from build>=1.0.3->Chromadb) (23.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ae/f3/431b9d5fe7d14af7a32340792ef43b8a714e7726f1d7b69cc4e8e7a3f1d7/pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from build>=1.0.3->Chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from build>=1.0.3->Chromadb) (2.0.1)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from fastapi>=0.95.2->Chromadb) (0.38.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->Chromadb) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->Chromadb) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/d4/e5d7e4f2174f8a4d63c8897d79eb8fe2503f7ecc03282fee1fa2719c2704/httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 0.0/77.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.9/77.9 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->Chromadb) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->Chromadb) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->Chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (2.8.2)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bb/fb/9af9e3f2996677bdda72734482934fe85a3abde174e5f0783ac2f817ba98/google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\n",
      "     ---------------------------------------- 0.0/200.9 kB ? eta -:--:--\n",
      "     ----------------------- -------------- 122.9/200.9 kB 3.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 194.6/200.9 kB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 200.9/200.9 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (0.58.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ------------------------------ ------- 122.9/151.7 kB 3.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 143.4/151.7 kB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 151.7/151.7 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->Chromadb) (1.26.14)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a7/06/3d6badcf13db419e25b07041d9c7b4a2c331d3f4e7134445ec5df57714cd/coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\admin\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->Chromadb) (4.24.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->Chromadb) (1.11.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->Chromadb) (1.2.14)\n",
      "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c0/14/362d31bf1076b21e1bcdcb0dc61944822ff263937b804a79231df2774d28/importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/08/49bfe7cf737952cc1a9c43e80cc258ed45dad7f183c5b8276fc94cb3862d/googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "     ---------------------------------------- 0.0/220.9 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 41.0/220.9 kB ? eta -:--:--\n",
      "     --------------------------------- ---- 194.6/220.9 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 220.9/220.9 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/41/27/4610ab3d9bb3cde4309b6505f98b3aabca04a26aa480aa18cede23149837/opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/94/56/3d2d826834209b19a5141eed717f7922150224d1a982385d19a9444cbf8d/opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 0.0/52.5 kB ? eta -:--:--\n",
      "     ---------------------------------------  51.2/52.5 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 52.5/52.5 kB 541.3 kB/s eta 0:00:00\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/74/a0e0d38622856597dd8e630f2bd793760485eb165708e11b8be1696bbb5a/opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0a/7f/405c41d4f359121376c9d5117dcf68149b8122d3f6c718996d037bd4d800/opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b7/7a/4f0063dbb0b6c971568291a8bc19a4ca70d3c185db2d956230dd67429dfc/opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "     ---------------------------------------- 0.0/149.7 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/149.7 kB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 92.2/149.7 kB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ------ 122.9/149.7 kB 901.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 149.7/149.7 kB 893.9 kB/s eta 0:00:00\n",
      "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ad/2e/36097c0a4d0115b8c7e377c90bab7783ac183bc5cb4071308f8959454311/opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb) (65.6.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/39/e3/893e8757be2612e6c266d9bb58ad2e3651524b5b40cf56761e985a28b13e/asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->Chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->Chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic>=1.9->Chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic>=1.9->Chromadb) (2.20.1)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich>=10.11.0->Chromadb) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich>=10.11.0->Chromadb) (2.11.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->Chromadb) (0.20.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer>=0.9.0->Chromadb) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer>=0.9.0->Chromadb) (1.5.4)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1e/fc/8a26c2adcd3f141e4729897633f03832b71ebea6f4c31cce67a92ded1961/httptools-0.6.1-cp310-cp310-win_amd64.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.2 kB ? eta -:--:--\n",
      "     ----------------------------------- ---- 51.2/58.2 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 58.2/58.2 kB 773.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->Chromadb) (1.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fe/1b/5148898ba55fc9c111a2a4a5fb67ad3fa7eb2b3d7f0618241ed88749313d/watchfiles-0.24.0-cp310-none-win_amd64.whl (277 kB)\n",
      "     ---------------------------------------- 0.0/277.5 kB ? eta -:--:--\n",
      "     -------- ------------------------------ 61.4/277.5 kB 1.7 MB/s eta 0:00:01\n",
      "     ---------------- --------------------- 122.9/277.5 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 225.3/277.5 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 277.5/277.5 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7b/f9/83bc78788d6ce5492fa44133708584a885080aa7c790be2532f326948115/websockets-13.0.1-cp310-cp310-win_amd64.whl (152 kB)\n",
      "     ---------------------------------------- 0.0/152.2 kB ? eta -:--:--\n",
      "     ----------------------- --------------- 92.2/152.2 kB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 152.2/152.2 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->Chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->Chromadb) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->Chromadb) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->Chromadb) (2023.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->Chromadb) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->Chromadb) (2.0.4)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f0/0f/310fb31e39e2d734ccaa2c0fb981ee41f7bd5056ce9bc29b2248bd569169/humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 81.9/86.8 kB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 86.8/86.8 kB 981.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->Chromadb) (1.2.1)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->Chromadb)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ee/2d/a308abc94a1ee028ef99cb5385fb756d61022b7adbb5361683cd380cf575/pyreadline3-3.5.3-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 0.0/79.9 kB ? eta -:--:--\n",
      "     ----------------------------------- ---- 71.7/79.9 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 79.9/79.9 kB 893.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->Chromadb) (0.4.8)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53834 sha256=1ddfc57ae53f8fbfd2e969887cbc6b098ff5f1d922214fded6d11ae0452835b9\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\06\\08\\01\\e1ce9a2b2e9e51d8540e77461c4cc395c902b9020c296aae50\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, flatbuffers, websockets, rsa, pyreadline3, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, oauthlib, mmh3, importlib-metadata, httptools, httpcore, grpcio, googleapis-common-protos, chroma-hnswlib, bcrypt, asgiref, watchfiles, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, humanfriendly, httpx, google-auth, build, opentelemetry-semantic-conventions, opentelemetry-instrumentation, kubernetes, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation-asgi, onnxruntime, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, Chromadb\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.16.3\n",
      "    Uninstalling httpcore-0.16.3:\n",
      "      Successfully uninstalled httpcore-0.16.3\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.23.3\n",
      "    Uninstalling httpx-0.23.3:\n",
      "      Successfully uninstalled httpx-0.23.3\n",
      "Successfully installed Chromadb-0.5.7 asgiref-3.8.1 bcrypt-4.2.0 build-1.2.2 chroma-hnswlib-0.7.6 coloredlogs-15.0.1 flatbuffers-24.3.25 google-auth-2.34.0 googleapis-common-protos-1.65.0 grpcio-1.66.1 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.2 humanfriendly-10.0 importlib-metadata-8.4.0 kubernetes-30.1.0 mmh3-5.0.0 oauthlib-3.2.2 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 overrides-7.7.0 posthog-3.6.6 pypika-0.48.9 pyproject_hooks-1.1.0 pyreadline3-3.5.3 requests-oauthlib-2.0.0 rsa-4.9 watchfiles-0.24.0 websockets-13.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.29.0 requires importlib-metadata<7,>=1.4, but you have importlib-metadata 8.4.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install tiktoken\n",
    "!pip install faiss-cpu\n",
    "!pip install Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AEZ1MOtANzWC"
   },
   "outputs": [],
   "source": [
    "#  已经设置系统变量和记录在印象笔记\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk8soY5IOzR6"
   },
   "source": [
    "#1.Conversation buffer memory\n",
    "会话缓冲记忆（ConversationBufferMemory）可以储存消息并将消息提取到一个变量中。这可以将其提取为字符串，也可以以消息列表的形式获取历史记录。这在与聊天模型一起使用时非常有用。最后，该笔记本展示了如何在链（设置verbose=True以便我们可以看到提示）中使用这一功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMO5LT0oUG0M"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojNEF8JON_jy",
    "outputId": "6b7bd05e-2de9-46eb-d248-00b1fae73f5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'), AIMessage(content='whats up')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "#历史聊天信息\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyzDjxcAUNRx"
   },
   "source": [
    "通过chain的方式，记录聊天的历史记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "SDbjpm1PPDym",
    "outputId": "151ea883-0523-4f4f-b422-9157e8b1e14c"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 193] %1 不是有效的 Win32 应用程序。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationChain\n\u001b[1;32m----> 5\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m conversation \u001b[38;5;241m=\u001b[39m ConversationChain(\n\u001b[0;32m      7\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m      8\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     memory\u001b[38;5;241m=\u001b[39mConversationBufferMemory()\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m conversation\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你好!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\langchain_core\\load\\serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\pydantic\\v1\\main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\langchain_community\\llms\\openai.py:300\u001b[0m, in \u001b[0;36mBaseOpenAI.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    294\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_organization\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    295\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_organization\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_ORG_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_ORGANIZATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    298\u001b[0m )\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import openai python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install openai`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_os\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoneType, Transport, ProxiesTypes\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_from_path\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\types\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m Image\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model \u001b[38;5;28;01mas\u001b[39;00m Model\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionDefinition \u001b[38;5;28;01mas\u001b[39;00m FunctionDefinition, FunctionParameters \u001b[38;5;28;01mas\u001b[39;00m FunctionParameters\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\types\\image.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImage\u001b[39;00m(BaseModel):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\_models.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerics\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FieldInfo\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Body,\n\u001b[0;32m     24\u001b[0m     IncEx,\n\u001b[0;32m     25\u001b[0m     Query,\n\u001b[0;32m     26\u001b[0m     ModelT,\n\u001b[0;32m     27\u001b[0m     Headers,\n\u001b[0;32m     28\u001b[0m     Timeout,\n\u001b[0;32m     29\u001b[0m     NotGiven,\n\u001b[0;32m     30\u001b[0m     AnyMapping,\n\u001b[0;32m     31\u001b[0m     HttpxRequestFiles,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_list, is_given, is_mapping, parse_date, parse_datetime, strip_not_given\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     PYDANTIC_V2,\n\u001b[0;32m     36\u001b[0m     ConfigDict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     field_get_default,\n\u001b[0;32m     46\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\openai\\_types.py:21\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     IO,\n\u001b[0;32m      6\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     Sequence,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, Protocol, TypeAlias, TypedDict, override, runtime_checkable\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m URL, Proxy, Timeout, Response, BaseTransport, AsyncBaseTransport\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__version__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __description__, __title__, __version__\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_auth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\_api.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_TIMEOUT_CONFIG\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Response\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\_client.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ASGITransport\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncBaseTransport, BaseTransport\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncHTTPTransport, HTTPTransport\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwsgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WSGITransport\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     AsyncByteStream,\n\u001b[0;32m     36\u001b[0m     AuthTypes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     VerifyTypes,\n\u001b[0;32m     50\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\_transports\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwsgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpx\\_transports\\default.py:33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TracebackType\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpcore\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_LIMITS, Limits, Proxy, create_ssl_context\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     ConnectError,\n\u001b[0;32m     38\u001b[0m     ConnectTimeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     WriteTimeout,\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m request, stream\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_async\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     AsyncConnectionInterface,\n\u001b[0;32m      4\u001b[0m     AsyncConnectionPool,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     AsyncSOCKSProxy,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     SOCKET_OPTION,\n\u001b[0;32m     13\u001b[0m     AsyncNetworkBackend,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     NetworkStream,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\_api.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterator, Optional, Union\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m URL, Extensions, HeaderTypes, Response\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sync\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionPool\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m      9\u001b[0m     method: Union[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m     10\u001b[0m     url: Union[URL, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     extensions: Optional[Extensions] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Sends an HTTP request, returning the response.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m        An instance of `httpcore.Response`.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPConnection\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionPool\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp11\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTP11Connection\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Origin, Request, Response\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ssl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_ssl_context\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_synchronization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lock\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trace\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp11\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTP11Connection\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\httpcore\\_synchronization.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Our async synchronization primatives use either 'anyio' or 'trio' depending\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# on if they're running under asyncio or trio.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtrio\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     trio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\__init__.py:22\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# General layout:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# trio/_core/... is the self-contained core library. It does various\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# pyright explicitly does not care about `__version__`\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# see https://github.com/microsoft/pyright/blob/main/docs/typed-libraries.md#type-completeness\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     TrioInternalError \u001b[38;5;28;01mas\u001b[39;00m TrioInternalError,\n\u001b[0;32m     24\u001b[0m     RunFinishedError \u001b[38;5;28;01mas\u001b[39;00m RunFinishedError,\n\u001b[0;32m     25\u001b[0m     WouldBlock \u001b[38;5;28;01mas\u001b[39;00m WouldBlock,\n\u001b[0;32m     26\u001b[0m     Cancelled \u001b[38;5;28;01mas\u001b[39;00m Cancelled,\n\u001b[0;32m     27\u001b[0m     BusyResourceError \u001b[38;5;28;01mas\u001b[39;00m BusyResourceError,\n\u001b[0;32m     28\u001b[0m     ClosedResourceError \u001b[38;5;28;01mas\u001b[39;00m ClosedResourceError,\n\u001b[0;32m     29\u001b[0m     run \u001b[38;5;28;01mas\u001b[39;00m run,\n\u001b[0;32m     30\u001b[0m     open_nursery \u001b[38;5;28;01mas\u001b[39;00m open_nursery,\n\u001b[0;32m     31\u001b[0m     CancelScope \u001b[38;5;28;01mas\u001b[39;00m CancelScope,\n\u001b[0;32m     32\u001b[0m     current_effective_deadline \u001b[38;5;28;01mas\u001b[39;00m current_effective_deadline,\n\u001b[0;32m     33\u001b[0m     TASK_STATUS_IGNORED \u001b[38;5;28;01mas\u001b[39;00m TASK_STATUS_IGNORED,\n\u001b[0;32m     34\u001b[0m     current_time \u001b[38;5;28;01mas\u001b[39;00m current_time,\n\u001b[0;32m     35\u001b[0m     BrokenResourceError \u001b[38;5;28;01mas\u001b[39;00m BrokenResourceError,\n\u001b[0;32m     36\u001b[0m     EndOfChannel \u001b[38;5;28;01mas\u001b[39;00m EndOfChannel,\n\u001b[0;32m     37\u001b[0m     Nursery \u001b[38;5;28;01mas\u001b[39;00m Nursery,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_timeouts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     move_on_at \u001b[38;5;28;01mas\u001b[39;00m move_on_at,\n\u001b[0;32m     42\u001b[0m     move_on_after \u001b[38;5;28;01mas\u001b[39;00m move_on_after,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     TooSlowError \u001b[38;5;28;01mas\u001b[39;00m TooSlowError,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sync\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     52\u001b[0m     Event \u001b[38;5;28;01mas\u001b[39;00m Event,\n\u001b[0;32m     53\u001b[0m     CapacityLimiter \u001b[38;5;28;01mas\u001b[39;00m CapacityLimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     Condition \u001b[38;5;28;01mas\u001b[39;00m Condition,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\_core\\__init__.py:27\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ki\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     enable_ki_protection,\n\u001b[0;32m     22\u001b[0m     disable_ki_protection,\n\u001b[0;32m     23\u001b[0m     currently_ki_protected,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Imports that always exist\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_run\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     Task,\n\u001b[0;32m     29\u001b[0m     CancelScope,\n\u001b[0;32m     30\u001b[0m     run,\n\u001b[0;32m     31\u001b[0m     open_nursery,\n\u001b[0;32m     32\u001b[0m     checkpoint,\n\u001b[0;32m     33\u001b[0m     current_task,\n\u001b[0;32m     34\u001b[0m     current_effective_deadline,\n\u001b[0;32m     35\u001b[0m     checkpoint_if_cancelled,\n\u001b[0;32m     36\u001b[0m     TASK_STATUS_IGNORED,\n\u001b[0;32m     37\u001b[0m     current_statistics,\n\u001b[0;32m     38\u001b[0m     current_trio_token,\n\u001b[0;32m     39\u001b[0m     reschedule,\n\u001b[0;32m     40\u001b[0m     remove_instrument,\n\u001b[0;32m     41\u001b[0m     add_instrument,\n\u001b[0;32m     42\u001b[0m     current_clock,\n\u001b[0;32m     43\u001b[0m     current_root_task,\n\u001b[0;32m     44\u001b[0m     spawn_system_task,\n\u001b[0;32m     45\u001b[0m     current_time,\n\u001b[0;32m     46\u001b[0m     wait_all_tasks_blocked,\n\u001b[0;32m     47\u001b[0m     wait_readable,\n\u001b[0;32m     48\u001b[0m     wait_writable,\n\u001b[0;32m     49\u001b[0m     notify_closing,\n\u001b[0;32m     50\u001b[0m     Nursery,\n\u001b[0;32m     51\u001b[0m     start_guest_run,\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Has to come after _run to resolve a circular import\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     56\u001b[0m     cancel_shielded_checkpoint,\n\u001b[0;32m     57\u001b[0m     Abort,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     reattach_detached_coroutine_object,\n\u001b[0;32m     63\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\_core\\_run.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ki\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOCALS_KEY_KI_PROTECTION_ENABLED, KIManager, enable_ki_protection\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multierror\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiError, concat_tb\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_thread_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m start_thread_soon\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     36\u001b[0m     Abort,\n\u001b[0;32m     37\u001b[0m     CancelShieldedCheckpoint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     wait_task_rescheduled,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\_core\\_thread_cache.py:66\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m partial(namefunc, pthread_setname_np)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# construct os thread name method\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m set_os_thread_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_os_thread_name_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# The \"thread cache\" is a simple unbounded thread pool, i.e., it automatically\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# spawns as many threads as needed to handle all the requests its given. Its\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# only purpose is to cache worker threads so that they don't have to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# How long a thread will idle waiting for new work before gives up and exits.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# This value is pretty arbitrary; I don't think it matters too much.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m IDLE_TIMEOUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# seconds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\trio\\_core\\_thread_cache.py:42\u001b[0m, in \u001b[0;36mget_os_thread_name_func\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m libpthread_path:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m libpthread \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibpthread_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# get the setname method from it\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# afaik this should never fail\u001b[39;00m\n\u001b[0;32m     46\u001b[0m pthread_setname_np \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(libpthread, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpthread_setname_np\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 193] %1 不是有效的 Win32 应用程序。"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"你好!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "9qT2CZPVTnZc",
    "outputId": "6b3acd08-08bb-426e-c2a5-8bce4c500700"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mconversation\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m我非常的好， 我正在和AI聊天.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conversation' is not defined"
     ]
    }
   ],
   "source": [
    "conversation.predict(input=\"我非常的好， 我正在和AI聊天.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "dkwpnGtwTx6n",
    "outputId": "67d81ac0-465b-40c0-9e82-1fc266c6d269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好!\n",
      "AI:  你好! 很高兴见到你！我是一个AI，我可以回答你的问题，或者我们可以聊聊天？\n",
      "Human: 我非常的好， 我正在和AI聊天.\n",
      "AI:  哇，这听起来很有趣！我很高兴能和你聊天！你有什么想问我的吗？\n",
      "Human: 能向我介绍你吗？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 当然可以！我是一个AI，我可以回答你的问题，或者我们可以聊聊天。我可以提供你有关技术，科学，文化，历史等方面的信息。我还可以帮助你解决问题，提供建议，并且可以和你一起玩游戏！'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"能向我介绍你吗？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "BuMtLywVPYxS",
    "outputId": "9d087e1f-6d95-4655-f96b-de459a51168d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好!\n",
      "AI:  你好! 很高兴见到你！我是一个AI，我可以回答你的问题，或者我们可以聊聊天？\n",
      "Human: 我非常的好， 我正在和AI聊天.\n",
      "AI:  哇，这听起来很有趣！我很高兴能和你聊天！你有什么想问我的吗？\n",
      "Human: 能向我介绍你吗？\n",
      "AI:  当然可以！我是一个AI，我可以回答你的问题，或者我们可以聊聊天。我可以提供你有关技术，科学，文化，历史等方面的信息。我还可以帮助你解决问题，提供建议，并且可以和你一起玩游戏！\n",
      "Human: 你做什么工作的？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 我的工作是回答你的问题，提供你有关技术，科学，文化，历史等方面的信息，帮助你解决问题，提供建议，并且可以和你一起玩游戏！'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"你做什么工作的？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6PjM7SRUbH4"
   },
   "source": [
    "#2.Conversation buffer window memory\n",
    "memory 不能无限制地记录所有聊天信息， 通过ConversationBufferWindowMemory 中的k 参数的设置表明记录历史中多少次沟通\n",
    "\n",
    "\n",
    "会话缓冲窗口记忆（ConversationBufferWindowMemory）会跟踪随时间变化的对话交互列表，但只使用最后K次交互。这对于保持最近交互的滑动窗口很有用，因此缓冲区不会变得过大。这种类型的记忆提供了基本功能，包括将其作为字符串提取，并以消息列表的形式获取历史记录。此功能在与聊天模型一起使用时非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ef4FfvIOU2Ix",
    "outputId": "5a7ef9f6-3b6b-4329-d605-813ea41ff560"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: not much you\\nAI: not much\\nHuman: 你好\\nAI: 你也好'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "#只保存最近K次的聊天信息\n",
    "memory = ConversationBufferWindowMemory( k=2)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"你也好\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "Q5OUy69vVTBi",
    "outputId": "9252ba7e-fa3a-4162-fc4f-cb359010f164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    #设置k=2 意思是获取聊天历史中最近的两条记录\n",
    "    memory=ConversationBufferWindowMemory(k=1),\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "HWtOEisdVmAL",
    "outputId": "d384513f-9329-4656-b228-7576b38cdf6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好\n",
      "AI:  你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？\n",
      "Human: 我这里出现一些状况?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 哦，你可以告诉我更多关于这个状况的细节吗？我可以尝试帮助你解决问题。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"我这里出现一些状况?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "_kBGXA9aVsJk",
    "outputId": "176c28de-01f7-406a-be48-592c3c89431e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 我这里出现一些状况?\n",
      "AI:  哦，你可以告诉我更多关于这个状况的细节吗？我可以尝试帮助你解决问题。\n",
      "Human: 我的wifi坏了?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 哦，我明白。你可以尝试重启你的路由器，看看是否可以解决问题。如果这不起作用，你可以尝试检查你的路由器的电源线是否接好，或者检查你的路由器是否有更新可用。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"我的wifi坏了?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "67mC58iPVvMR",
    "outputId": "062221fa-74ce-4299-be10-5a97313c1598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 我这里出现一些状况?\n",
      "AI:  哦，你可以告诉我更多关于这个状况的细节吗？我可以尝试帮助你解决问题。\n",
      "Human: 我的wifi坏了?\n",
      "AI:  哦，我很抱歉听到你的wifi坏了。你可以尝试重启你的路由器，或者检查你的网络连接设置，看看是否有任何问题。\n",
      "Human: 如何才能解决?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 如果你的路由器已经重启过，你可以尝试检查你的网络连接设置，看看是否有任何问题。你也可以尝试更新你的路由器固件，以确保你的设备正确运行。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation_with_summary.predict(input=\"如何才能解决?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "4C5YRD09WhsG",
    "outputId": "9bc98f46-2395-4944-d505-ebd07bb413db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 我的wifi坏了?\n",
      "AI:  哦，我很抱歉听到你的wifi坏了。你可以尝试重启你的路由器，或者检查你的网络连接设置，看看是否有任何问题。\n",
      "Human: 如何才能解决?\n",
      "AI:  如果你的路由器已经重启过，你可以尝试检查你的网络连接设置，看看是否有任何问题。你也可以尝试更新你的路由器固件，以确保你的设备正确运行。\n",
      "Human: 谢谢你的帮助，我会去试试?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'  不客气！我很高兴能帮到你。如果你还有任何其他问题，请随时联系我，我会尽力帮助你解决。'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"谢谢你的帮助，我会去试试?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If3JRWAWX3r4"
   },
   "source": [
    "#3.Conversation summary memory\n",
    "ConversationSummaryMemory在对话发生时对其进行总结，并将当前的总结存储在内存中。然后，可以使用这个记忆来将迄今为止的对话总结注入到提示/链中。对于较长的对话，这种记忆最有用，因为在提示中逐字保留过去的信息历史会占用太多的令牌。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7tYEORZHX6rK",
    "outputId": "16e33dcc-db4a-4e0c-b7f0-dabc425907ac"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nThe AI asked if the human had something they wanted to discuss.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(temperature=0), return_messages=True)\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"有什么事情吗？\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "messages = memory.chat_memory.messages\n",
    "#之前谈话形成的摘要\n",
    "previous_summary = \"\"\n",
    "#预测基于当前的对话消息(messages)和之前的摘要(previous_summary)的新的摘要。\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWBQXyAyYpKe"
   },
   "source": [
    "如果你有来自这个类之外的消息，你可以很容易地用ChatMessageHistory初始化这个类。在加载过程中，会计算出一个总结。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "gUgOpMvNYiqh",
    "outputId": "d57c22a8-f314-4c13-c49a-e40038e84df8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nThe human greets the AI, to which the AI responds with a friendly greeting, addressing the human by name. The human then states that they like to eat apples, to which the AI responds that they also like to eat them.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"hi\")\n",
    "history.add_ai_message(\"hi there!\")\n",
    "history.add_user_message(\"我的名字叫小明\")\n",
    "history.add_ai_message(\"你好小明!\")\n",
    "history.add_user_message(\"我喜欢吃苹果\")\n",
    "history.add_ai_message(\"我也喜欢吃\")\n",
    "\n",
    "memory = ConversationSummaryMemory.from_messages(llm=OpenAI(temperature=0), chat_memory=history, return_messages=True)\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "5raAQ5Z9bNGw",
    "outputId": "a1d24ec6-af17-4ecd-9a09-7de5bd9da309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryMemory(llm=OpenAI()),\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "GBD8cB-xbVBj",
    "outputId": "708eaf9f-2b1a-4ce4-8ece-75a203f9af9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human greeted the AI, to which the AI replied happily and asked if the human had any questions or if they wanted to talk.\n",
      "Human: 我最近在做一个项目\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 哇，真棒！你在做什么项目？它有什么样的目标？'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"我最近在做一个项目\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "AUd2_L80bXlF",
    "outputId": "e50390aa-2cca-4da1-c910-e12af6754460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "The human greeted the AI, to which the AI replied happily and asked if the human had any questions or if they wanted to talk. The human mentioned that they were working on a project, and the AI was impressed and asked what kind of project it was and what the goals were.\n",
      "Human: 如何能够让项目按时交付？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 完成项目的最佳方式是制定一个明确的计划，并且按照计划严格执行。你可以分解项目的任务，并且确定每个任务的完成时间，以便按时完成项目。此外，你还可以设置定期检查点，以确保项目的进度按时完成。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"如何能够让项目按时交付？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4B8j80FvcbU"
   },
   "source": [
    "#4.Conversation Summary Buffer Memory\n",
    "通过设置max_token_limit字段判断记忆上下文的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "pvB7-b70viit",
    "outputId": "cdf79bcb-d876-4e1f-8d1a-4c93c34b280a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nThe human greets the AI and asks if there is anything they can help with. The AI responds that there is nothing, and the human continues the conversation by asking what fruit and sports the AI likes. The AI responds that it likes apples and basketball, and its favorite team is the All-Stars. Finally, the AI states that its favorite subject is math.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, max_token_limit=300, return_messages=True\n",
    ")\n",
    "#历史的聊天记录\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"有什么事情吗？\"})\n",
    "memory.save_context({\"input\": \"没有什么事情\"}, {\"output\": \"那就好\"})\n",
    "memory.save_context({\"input\": \"你喜欢吃什么水果\"}, {\"output\": \"苹果\"})\n",
    "memory.save_context({\"input\": \"你喜欢什么运动\"}, {\"output\": \"篮球\"})\n",
    "memory.save_context({\"input\": \"你喜欢哪支球队\"}, {\"output\": \"明星队\"})\n",
    "memory.save_context({\"input\": \"你喜欢上什么课\"}, {\"output\": \"数学\"})\n",
    "\n",
    "messages = memory.chat_memory.messages\n",
    "previous_summary = \"\"\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "XY2VDPfUxWo_",
    "outputId": "4f27332d-7f3b-44b7-af4b-b546154ee85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 你好！ 很高兴见到你！我叫AI-Robo，我的工作是帮助你解决问题。你有什么问题吗？'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=50),\n",
    "    verbose=True,\n",
    ")\n",
    "#实时聊天\n",
    "conversation_with_summary.predict(input=\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "6skLrUT5xeMb",
    "outputId": "f85b2eee-85ab-4ae4-f052-2faa3ac62780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds, introducing itself as AI-Robo and stating its purpose is to help the human solve problems. The AI then asks if the human has any questions.\n",
      "Human: 我最近在做一个项目\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 我明白你正在做一个项目。你需要什么帮助？是否有什么特定的问题你需要我帮助你解决？'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"我最近在做一个项目\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "ch4Fax89xgAX",
    "outputId": "949ac50c-3346-4f97-c68b-30900218aa90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI introduces itself as AI-Robo and states its purpose is to help the human solve problems. The AI then asks if the human has any questions and the human responds that they are working on a project. The AI understands and asks if there is any specific problem they can help the human solve.\n",
      "Human: 我喜欢吃苹果\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 很高兴你喜欢吃苹果！苹果有许多种类，比如红苹果、青苹果、黄苹果等。你最喜欢哪种苹果？'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"我喜欢吃苹果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "pmXlayc1U0M0",
    "outputId": "b020a750-899a-42ce-8f2e-efc32c22ac7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI introduces itself as AI-Robo and states its purpose is to help the human solve problems. The AI then asks if the human has any questions and the human responds that they are working on a project. The AI understands and asks if there is any specific problem they can help the human solve. The AI then acknowledges that the human likes to eat apples and mentions there are various types of apples, such as red apples, green apples, and yellow apples, and asks the human which type they like best.\n",
      "Human: 我最近遇到什么问题。\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 哦，你最近遇到了什么问题？我可以尝试帮助你解决它。请你告诉我，这个问题是什么？'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"我最近遇到什么问题。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "OPCRqgbWWau-",
    "outputId": "717600a6-62b3-4f07-af24-c320fc1917f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "\n",
      "The human greets the AI and the AI introduces itself as AI-Robo and states its purpose is to help the human solve problems. The AI then asks if the human has any questions and the human responds that they are working on a project. The AI understands and asks if there is any specific problem they can help the human solve. The AI then acknowledges that the human likes to eat apples and mentions there are various types of apples, such as red apples, green apples, and yellow apples, and asks the human which type they like best. The human then states they have encountered a problem and the AI responds that it can try to help the human solve it and asks what the problem is.\n",
      "Human: 我喜欢打篮球，你呢\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 哦，我不太喜欢打篮球，但我很喜欢从事数据分析，帮助解决问题。你有什么问题需要我帮助你解决吗？'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"我喜欢打篮球，你呢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "KSR-7WFfWkAL",
    "outputId": "9b15c523-d9ce-4de8-94b6-07618daf7198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI introduces itself as AI-Robo and states its purpose is to help the human solve problems. The AI then asks if the human has any questions and the human responds that they are working on a project. The AI understands and mentions they like basketball, while AI-Robo does not, but enjoys data analysis and helping to solve problems. The AI then acknowledges that the human likes to eat apples and mentions there are various types of apples, before asking the human if they have any problems they need help solving.\n",
      "Human: 我喜欢吃什么水果？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 我可以帮你解答这个问题，我喜欢吃苹果，橙子，梨，葡萄，香蕉，李子，桃子，草莓和柠檬。我更喜欢甜蜜的水果，但也喜欢酸的水果。'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"我喜欢吃什么水果？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6CPrEA12fcY"
   },
   "source": [
    "#5.Vector store-backed memory\n",
    "VectorStoreRetrieverMemory是一种内存管理方式，它将内存存储在VectorDB中，并在每次被调用时查询最显著的前K个文档。\n",
    "\n",
    "memory对象和向量数据库之间的关系：\n",
    "\n",
    "memory对象在初始化的时候会接收一个检索器（retriever）对象。在这个例子中，这个检索器是从向量数据库创建的。所以，memory对象中实际上包含了一个到向量数据库的链接。\n",
    "\n",
    "当保存新的上下文信息到memory对象中时，这些信息也会被保存到向量数据库中。在这个例子中，当我们调用memory.save_context方法时，输入的上下文信息会被保存到向量数据库中。\n",
    "\n",
    "当从memory对象中加载数据时，如果这些数据之前已经被保存到了内存中，那么就直接从内存中加载；如果这些数据还没有被保存到内存中，那么就会从向量数据库中加载。\n",
    "\n",
    "memory对象还会负责处理所有与检索相关的操作。比如，当我们调用memory.load_memory_variables方法时，memory对象会使用它内部的检索器从向量数据库中检索出最相关的文档。\n",
    "\n",
    "所以，memory对象和向量数据库之间是需要交换数据的。具体来说，memory对象会将新的上下文信息保存到向量数据库中，同时也会从向量数据库中检索出相关的文档。在检索过程中，memory对象还会保存一些上下文信息，以便在后续的查询中使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMI4m5E-2wDU"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import faiss\n",
    "\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = OpenAIEmbeddings().embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BINprgjg3E4X",
    "outputId": "5dde74be-b175-4f50-e0dd-17068ecdbda3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 我的最爱的食物是披萨\n",
      "output: 这很好知道\n"
     ]
    }
   ],
   "source": [
    "# 将向量存储库（vectorstore）转化为检索器（retriever），这个检索器能够根据输入的查询，\n",
    "#从存储库中检索出与该查询最接近的 k 个文档。这里 k=1，所以每次查询只返回最接近的一个文档。\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\n",
    "#创建了一个基于检索器的内存对象。虽然在向量存储库上直接进行查询是可能的，\n",
    "#但是有些使用场景需要在查询过程中保存和跟踪一些上下文信息。\n",
    "#因此，我们通常会使用一个内存对象来保存这些信息，并提供一个统一的接口来进行查询和存储。\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# 当被添加到一个代理中，内存对象可以保存来自对话或使用的工具的相关信息\n",
    "memory.save_context({\"input\": \"我的最爱的食物是披萨\"}, {\"output\": \"这很好知道\"})\n",
    "memory.save_context({\"input\": \"我最喜欢的运动是足球\"}, {\"output\": \"...\"})\n",
    "memory.save_context({\"input\": \"我不支持凯尔特人队\"}, {\"output\": \"好的\"})\n",
    "\n",
    "print(memory.load_memory_variables({\"prompt\": \"我喜爱的食物?\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "kFF-52mF3TPE",
    "outputId": "2c7f02c5-a210-4a06-b3b9-37be3c3d3966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m以下是人类和AI之间的友好对话。AI健谈并提供了其背景中的许多具体细节。如果AI不知道问题的答案，它会诚实地说它不知道。\n",
      "\n",
      "以前的对话中的相关部分：\n",
      "input: 我的最爱的食物是披萨\n",
      "output: 这很好知道\n",
      "\n",
      "（如果不相关，你不需要使用这些信息）\n",
      "\n",
      "当前的对话：\n",
      "人类：嗨，我叫Perry，你好吗?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 你好Perry，我很好，很高兴认识你！我叫AI，我是一个人工智能系统，我可以回答你的问题，或者我们可以聊聊天，你想做什么？'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0) # 可以是任何有效的LLM\n",
    "_DEFAULT_TEMPLATE = \"\"\"以下是人类和AI之间的友好对话。AI健谈并提供了其背景中的许多具体细节。如果AI不知道问题的答案，它会诚实地说它不知道。\n",
    "\n",
    "以前的对话中的相关部分：\n",
    "{history}\n",
    "\n",
    "（如果不相关，你不需要使用这些信息）\n",
    "\n",
    "当前的对话：\n",
    "人类：{input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    "    # 我们设置了一个非常低的max_token_limit，以便进行测试。\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"嗨，我叫Perry，你好吗?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "Bnza1vgH8K3a",
    "outputId": "ad29b952-04b7-451e-8c19-8869816ae744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m以下是人类和AI之间的友好对话。AI健谈并提供了其背景中的许多具体细节。如果AI不知道问题的答案，它会诚实地说它不知道。\n",
      "\n",
      "以前的对话中的相关部分：\n",
      "input: 我最喜欢的运动是足球\n",
      "output: ...\n",
      "\n",
      "（如果不相关，你不需要使用这些信息）\n",
      "\n",
      "当前的对话：\n",
      "人类：我最喜爱的运动是什么?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 你最喜欢的运动是足球，对吗？足球是一项非常受欢迎的运动，它可以让你发挥你的技能，并且可以让你和朋友们一起玩耍。它也可以帮助你增强你的体能，提高你的协调能力，并且可以让你在比赛中发挥出色。'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这部分数据保存在向量数据库中\n",
    "conversation_with_summary.predict(input=\"我最喜爱的运动是什么?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "fcx5bFDp8ZBp",
    "outputId": "33e230db-eab3-4ddb-c527-e89012609475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m以下是人类和AI之间的友好对话。AI健谈并提供了其背景中的许多具体细节。如果AI不知道问题的答案，它会诚实地说它不知道。\n",
      "\n",
      "以前的对话中的相关部分：\n",
      "input: 我的最爱的食物是披萨\n",
      "output: 这很好知道\n",
      "\n",
      "（如果不相关，你不需要使用这些信息）\n",
      "\n",
      "当前的对话：\n",
      "人类：我最喜爱的食物是什么？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 我不知道你最喜欢的食物是什么，但我知道你之前说过你最喜欢的食物是披萨。'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这部分信息保存在向量数据库中\n",
    "conversation_with_summary.predict(input=\"我最喜爱的食物是什么？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "XxNChDEG8iuj",
    "outputId": "b92e03ed-4e4d-407b-cfa2-603bc1fa82a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m以下是人类和AI之间的友好对话。AI健谈并提供了其背景中的许多具体细节。如果AI不知道问题的答案，它会诚实地说它不知道。\n",
      "\n",
      "以前的对话中的相关部分：\n",
      "input: 嗨，我叫Perry，你好吗?\n",
      "response:  你好Perry，我很好，很高兴认识你！我叫AI，我是一个人工智能系统，我可以回答你的问题，或者我们可以聊聊天，你想做什么？\n",
      "\n",
      "（如果不相关，你不需要使用这些信息）\n",
      "\n",
      "当前的对话：\n",
      "人类：我叫什么名字\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 你叫Perry，对吗？'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#名字的信息是保存在 memory中的不用去向量数据库获取\n",
    "conversation_with_summary.predict(input=\"我叫什么名字\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY4fnn_6KMbl"
   },
   "source": [
    "#6.How to customize conversational memory\n",
    "\"AI Prefix\"和\"Human Prefix\"都是在对话的上下文中表示每条对话的发起者身份的标签。在对话的上下文中，AI的回复通常会以\"AI:\"作为前缀，人类的输入通常会以\"Human:\"作为前缀。通过这些前缀，我们可以清晰地看到对话的流程和每条对话的发起者。\n",
    "\n",
    "默认情况下，\"AI Prefix\"设置为\"AI\"，\"Human Prefix\"设置为\"Human\"，这些都可以被自定义为你想要的任何标签。例如，你可以将\"AI Prefix\"更改为\"AI Assistant\"，将\"Human Prefix\"更改为\"Friend\"，然后在对话中，每条由AI生成的回复都会以\"AI Assistant:\"作为前缀，每条由人类输入的对话都会以\"Friend:\"作为前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CF7sI4z2KQt2"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "# Now we can override it and set it to \"AI Assistant\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "以下是一个人类和AI之间的友好对话。AI健谈并提供了其背景中的许多具体细节。如果AI不知道问题的答案，它会诚实地说它不知道。\n",
    "\n",
    "当前对话：\n",
    "{history}\n",
    "human：{input}\n",
    "AI助手：\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"AI 机器人\",human_prefix=\"朋友\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "4SgD_dJBKiMI",
    "outputId": "2a2749fd-c7c9-4601-df15-e0d0b5d3db81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "以下是一个人类和AI之间的友好对话。AI健谈并提供了其背景中的许多具体细节。如果AI不知道问题的答案，它会诚实地说它不知道。\n",
      "\n",
      "当前对话：\n",
      "朋友: 后天天气如何\n",
      "AI 机器人: 据我们的天气预报，后天的天气将会晴朗，温度在20-25摄氏度之间，有轻微的风。\n",
      "human：今天天气如何\n",
      "AI助手：\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'今天的天气晴朗，温度在25-30摄氏度之间，有轻微的风。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"今天天气如何\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dup-b2IMLOy-"
   },
   "source": [
    "#7.How to add memory to a Multi-Input Chain\n",
    "\n",
    "\n",
    "这个功能的主要目的是为处理多输入的链条（Chain）添加记忆功能。大多数记忆对象假设有一个单一的输入，但在这个例子中，我们处理了一个包含多个输入（相关文档和用户问题）的问题/回答链。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrTVC3QaLc8d",
    "outputId": "c79b14c0-5168-4976-e7f8-0a93f3a1dc8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open(\"story.txt\") as f:\n",
    "    file_story = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(file_story)\n",
    "\n",
    "#嵌入向量\n",
    "embeddings = OpenAIEmbeddings()\n",
    "#保存到向量数据库中\n",
    "docsearch = Chroma.from_texts(\n",
    "    texts, embeddings, metadatas=[{\"source\": i} for i in range(len(texts))]\n",
    ")\n",
    "\n",
    "query = \"老乌龟做了什么\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scyD7clXPeUl"
   },
   "source": [
    "input_documents和human_input都是在这个场景中特定的关键字。\n",
    "\n",
    "input_documents指的是用于问题回答链的文档输入。这些文档应该是与用户问题相关的文档，这些文档被用来帮助生成问题的答案。\n",
    "\n",
    "human_input指的是用户提出的实际问题，这个问题是被提交给问题回答链的，并从input_documents中的文档获取答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIP_sRuuMzgs",
    "outputId": "c8270881-b597-40f8-8672-714e4140e45b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' 莉莉是一只纯洁无瑕的小白兔，是泰格最好的朋友。她勇敢地冒险去寻找神秘的宝石，最终救出了泰格，从而拯救了森林里的所有动物。她的友谊和勇气成为了所有动物的楷模，让我们看到友谊的力量超越了种族和身份。'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template =  \"\"\"你是一个与人类进行对话的聊天机器人。\n",
    "\n",
    "根据以下长文档的提取部分和一个问题，生成最终回答。\n",
    "\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "人类用户: {human_input}\n",
    "聊天机器人:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\", \"context\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "#chain type stuff，把文档整个当作prompt\n",
    "chain = load_qa_chain(\n",
    "    OpenAI(temperature=0), chain_type=\"stuff\", memory=memory, prompt=prompt\n",
    ")\n",
    "\n",
    "query = \"莉莉是谁\"\n",
    "\n",
    "chain({\"input_documents\": docs, \"human_input\": query}, return_only_outputs=True)\n",
    "#print(chain.memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzeU7oskUMlJ",
    "outputId": "29b86d66-827d-4250-c89e-d0ddebd1c285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' 泰格是一只憨态可掬的小老虎，他是莉莉最好的朋友，他们一起摘取果实，他们的友情也因此而更加坚固。然而，他因为拥有珍贵的毛皮而落入猎人的陷阱，最终得到了莉莉的救援。'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"泰格是谁\"\n",
    "\n",
    "chain({\"input_documents\": docs, \"human_input\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bMoIf3iORs8",
    "outputId": "ee2dec8f-ddfd-4d00-f510-c9e304993a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 莉莉是谁\n",
      "AI:  莉莉是一只纯洁无瑕的小白兔，是泰格最好的朋友。她勇敢地冒险去寻找神秘的宝石，最终救出了泰格，从而拯救了森林里的所有动物。她的友谊和勇气成为了所有动物的楷模，让我们看到友谊的力量超越了种族和身份。\n",
      "Human: 老乌龟做了什么\n",
      "AI:  老乌龟是森林里的智者，他告诉莉莉只有找到神秘的宝石，才能解除陷阱，从而救出泰格。\n",
      "Human: 泰格是谁\n",
      "AI:  泰格是一只憨态可掬的小老虎，他是莉莉最好的朋友，他们一起摘取果实，他们的友情也因此而更加坚固。然而，他因为拥有珍贵的毛皮而落入猎人的陷阱，最终得到了莉莉的救援。\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HypRyESg18Q"
   },
   "source": [
    "#8.Entity memory\n",
    "\n",
    "实体记忆（Entity Memory）在处理长期对话或者需要持续跟踪特定实体信息的情境中。\n",
    "\n",
    "在一个持续的对话中，人类用户可能会提供有关某些实体（如人名、地点、事件等）的信息，这些信息可能会在后续的对话中被引用或查询。如果不使用实体记忆，AI可能会忘记或无法适当地处理这些在对话中提到的实体的信息，这可能会导致对话的连贯性和用户体验的降低。\n",
    "\n",
    "例如，如果用户告诉AI，他们的狗名叫Max，那么在后续的对话中，当用户提到Max时，AI应该能够记住Max是用户的狗。如果没有实体记忆，AI可能会忘记这个信息，或者需要用户再次提供这个信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KodHxDMFpZM"
   },
   "source": [
    "##8.1 一般应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HT3duAelg71F",
    "outputId": "5e01339e-fb31-4d6c-97f7-134b2617affb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 小强和小刚下棋，小红在一旁看下棋，小李要他们回教室， 王老师要开始给大家上课。\\nAI:  听上去不错， 他们下的是什么棋？',\n",
       " 'entities': {'小强': '小强是一位下棋的人，正在和小刚下棋，小红在一旁看着，小李要他们回教室，王老师要开始给大家上课。',\n",
       "  '小刚': '小刚是一位下棋的人。',\n",
       "  '小红': '小红在一旁看小强和小刚下棋，小李要他们回教室，王老师要开始给大家上课。',\n",
       "  '小李': '小李要求小强和小刚回教室，因为王老师要开始上课。',\n",
       "  '王老师': '王老师是一位教师，要开始给大家上课。'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "memory = ConversationEntityMemory(llm=llm)\n",
    "_input = {\"input\": \"小强和小刚下棋，小红在一旁看下棋，小李要他们回教室， 王老师要开始给大家上课。\"}\n",
    "\n",
    "#entity memory 的这个方法会将输入的内容提取成entity\n",
    "memory.load_memory_variables(_input)\n",
    "\n",
    "memory.save_context(\n",
    "    _input,\n",
    "    {\"output\": \" 听上去不错， 他们下的是什么棋？\"}\n",
    ")\n",
    "\n",
    "\n",
    "memory.load_memory_variables({\"input\": '小强在做什么？'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ez-GubxJFwOV"
   },
   "source": [
    "##8.2 对话中的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gC8yCtoNhIsL",
    "outputId": "5fe13251-6ae8-439d-c0d1-623d779e8908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'小明': '', '小红': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: 小明和小红正在下棋\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'小明': '小明正在和小红一起下棋，看起来都很有技巧。', '小红': '小红正在与小明下棋。'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    #使用entity提取的模版\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    #对整个对话进行entity的提取\n",
    "    memory=ConversationEntityMemory(llm=llm)\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"小明和小红正在下棋\")\n",
    "\n",
    "conversation.memory.entity_store.store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3TqjvBjDmdr",
    "outputId": "881359cd-80f8-430c-ea28-109bf2b81812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'小明': '小明正在和小红一起下棋，看起来都很有技巧，小刚走过来看他们，想加入他们的游戏。', '小红': '小红正在与小明下棋，小刚走过来看他们。', '小刚': '小刚走过来看小明和小红正在下棋，他也想加入他们的游戏，看起来他们都很开心！', '小李': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: 小明和小红正在下棋\n",
      "AI:  小明和小红看起来都很有技巧，他们正在努力思考下一步该怎么走，看起来很有趣！\n",
      "Human: 小刚走过来看他们下棋。\n",
      "AI:  小刚走过来，看到小明和小红正在下棋，他也想加入他们的游戏，看起来他们都很开心！\n",
      "Last line:\n",
      "Human: 小李，跑来给下棋的人当裁判\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'小明': '小明正在和小红一起下棋，看起来都很有技巧，小刚走过来看他们，想加入他们的游戏，小李也跑过来，想当裁判，看起来他们都很开心！',\n",
       " '小红': '小红正在与小明下棋，小刚走过来看他们，小李也跑过来，想加入他们的游戏并当裁判。',\n",
       " '小刚': '小刚走过来看小明和小红正在下棋，他也想加入他们的游戏，并且提出要当裁判，看起来他们都很开心！',\n",
       " '小李': '小李跑过来，看到小明和小红正在下棋，他也想加入他们的游戏，并且提出要当裁判，看起来他们都很开心！'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"小李，跑来给下棋的人当裁判\")\n",
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U-blolKHc5y"
   },
   "source": [
    "#9.Conversation Knowledge Graph Memory\n",
    "\"对话知识图谱记忆(Conversation Knowledge Graph Memory)\"是一种使用知识图谱来复原记忆的方法。首先，我们可以通过保存对话中的输入和输出来建立知识图谱。当我们询问某个实体，例如\"Sam\"，该方法可以返回关于该实体的历史信息，如\"Sam是朋友\"。\n",
    "\n",
    "此外，这个方法还可以从新的消息中获取当前实体或知识三元组，这些都会在前面的消息的上下文中使用。例如，当我们询问\"Sam的最爱颜色是什么？\"，它会返回实体\"Sam\"。当我们说\"她最喜欢的颜色是红色\"时，它会返回知识三元组，表示\"Sam\"的\"最喜欢的颜色\"是\"红色\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VT-vvDd7F001",
    "outputId": "ba8a5463-4b69-4bd2-8f1f-4732f638df30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On 小刚: 小刚 是 邻居. 小刚 is a neighbor. 小刚 是 裁判.', additional_kwargs={}),\n",
       "  SystemMessage(content='On 小明: 小明 是 我的朋友. 小明 和 小刚. 小明 是 邻居. 小明 is a neighbor. 小明 is a classmate. 小明 需要 裁判.', additional_kwargs={}),\n",
       "  SystemMessage(content='On 小红: 小红 is a classmate. 小红 是同桌 小刚. 小红 需要 裁判.', additional_kwargs={})]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True)\n",
    "memory.save_context({\"input\": \" 你好我是小明\"}, {\"output\": \"谁是小明\"})\n",
    "memory.save_context({\"input\": \"小明是我的朋友\"}, {\"output\": \"okay\"})\n",
    "memory.save_context({\"input\": \"小明和小刚是邻居\"}, {\"output\": \"really?\"})\n",
    "memory.save_context({\"input\": \"小明和小红是同学\"}, {\"output\": \"really?\"})\n",
    "memory.save_context({\"input\": \"小红和小刚是同桌\"}, {\"output\": \"是的\"})\n",
    "memory.save_context({\"input\": \"小刚是裁判\"}, {\"output\": \"是的\"})\n",
    "memory.save_context({\"input\": \"小明和小红下棋需要裁判\"}, {\"output\": \"是的\"})\n",
    "\n",
    "memory.load_memory_variables({\"input\": \"谁是小刚\"})\n",
    "#memory.get_current_entities({\"input\": \"what does Xiaoming like?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "I1OCo_7YZb_s"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "template =  \"\"\"以下是一个友好的人类与AI之间的对话。AI是健谈的，并从它的上下文中提供了许多具体细节。\n",
    "如果AI不知道某个问题的答案，它会诚实地说不知道。AI仅使用“相关信息”部分中包含的信息，不会虚构。\n",
    "\n",
    "相关信息：\n",
    "\n",
    "{history}\n",
    "\n",
    "对话：\n",
    "人类用户: {input}\n",
    "AI：\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, verbose=True, prompt=prompt, memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "mDoRTreBJy3u",
    "outputId": "aa5a68e8-f559-47c7-cd2b-f5407482c899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m以下是一个友好的人类与AI之间的对话。AI是健谈的，并从它的上下文中提供了许多具体细节。\n",
      "如果AI不知道某个问题的答案，它会诚实地说不知道。AI仅使用“相关信息”部分中包含的信息，不会虚构。\n",
      "\n",
      "相关信息：\n",
      "\n",
      "On 王老师: 王老师 喊 所有的人.\n",
      "\n",
      "对话：\n",
      "人类用户: 王老师喊下棋和看棋的人回去上课\n",
      "AI：\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'是的，王老师喊所有的人回去上课。'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input=\"王老师喊下棋和看棋的人回去上课\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
