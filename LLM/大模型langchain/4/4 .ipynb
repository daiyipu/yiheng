{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0xCBrbIxAYjB",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3480cb1b-2adf-4db0-90b9-2b0d36955941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.8\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.238-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.18)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.5.13-py3-none-any.whl (26 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
      "  Downloading langsmith-0.0.12-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n",
      "Successfully installed dataclasses-json-0.5.13 langchain-0.0.238 langsmith-0.0.12 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "Collecting unstructured\n",
      "  Downloading unstructured-0.8.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.0.0)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
      "Collecting msg-parser (from unstructured)\n",
      "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.0.10)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.5.3)\n",
      "Collecting pdf2image (from unstructured)\n",
      "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
      "Collecting pdfminer.six (from unstructured)\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unstructured) (8.4.0)\n",
      "Collecting pypandoc (from unstructured)\n",
      "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n",
      "Collecting python-docx (from unstructured)\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting python-pptx (from unstructured)\n",
      "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.4.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.27.1)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.8.10)\n",
      "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.0.1)\n",
      "Collecting olefile>=0.46 (from msg-parser->unstructured)\n",
      "  Downloading olefile-0.46.zip (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.65.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured) (1.22.4)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured) (2.0.12)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six->unstructured)\n",
      "  Downloading cryptography-41.0.2-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting XlsxWriter>=0.5.7 (from python-pptx->unstructured)\n",
      "  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->unstructured) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured) (2.21)\n",
      "Building wheels for collected packages: python-docx, python-pptx, olefile\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184491 sha256=a9664741c786026df880f0f0e461274e6384b5f26c514115c1e8dcaa10a5bd0e\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
      "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470935 sha256=d3015ebf6bc6e54506e173925563eb9192cdbc9961a75660dd960ce79c6c6d75\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/dd/74/01b3ec7256a0800b99384e9a0f7620e358afc3a51a59bf9b49\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=fc3717092598fa2af4b13fcda216626b6d2917817d73cb0f19f3a7a2f75ff8a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
      "Successfully built python-docx python-pptx olefile\n",
      "Installing collected packages: filetype, XlsxWriter, python-magic, python-docx, pypandoc, pdf2image, olefile, python-pptx, msg-parser, cryptography, pdfminer.six, unstructured\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 3.4.8\n",
      "    Uninstalling cryptography-3.4.8:\n",
      "      Successfully uninstalled cryptography-3.4.8\n",
      "Successfully installed XlsxWriter-3.1.2 cryptography-41.0.2 filetype-1.2.0 msg-parser-1.2.0 olefile-0.46 pdf2image-1.16.3 pdfminer.six-20221105 pypandoc-1.11 python-docx-0.8.11 python-magic-0.4.27 python-pptx-0.6.21 unstructured-0.8.1\n",
      "Collecting jq\n",
      "  Downloading jq-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.1/589.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jq\n",
      "Successfully installed jq-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install tiktoken\n",
    "!pip install faiss-cpu\n",
    "!pip install unstructured\n",
    "!pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4dzFVnIdAbhh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V82qpxdp800c"
   },
   "source": [
    "#1.csv loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPJmHEO8CPSY",
    "outputId": "3bfe93e2-818e-40cc-fc7c-d8612f75c385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='球队: 国民队\\n工资(百万): 81.34\\n胜场数: 98', metadata={'source': 'test.csv', 'row': 0}), Document(page_content='球队: 红人队\\n工资(百万): 82.20\\n胜场数: 97', metadata={'source': 'test.csv', 'row': 1}), Document(page_content='球队: 洋基队\\n工资(百万): 197.96\\n胜场数: 95', metadata={'source': 'test.csv', 'row': 2}), Document(page_content='球队: 巨人队\\n工资(百万): 117.62\\n胜场数: 94', metadata={'source': 'test.csv', 'row': 3}), Document(page_content='球队: 勇士队\\n工资(百万): 83.31\\n胜场数: 94', metadata={'source': 'test.csv', 'row': 4}), Document(page_content='球队: 运动家队\\n工资(百万): 55.37\\n胜场数: 94', metadata={'source': 'test.csv', 'row': 5}), Document(page_content='球队: 游骑兵队\\n工资(百万): 120.51\\n胜场数: 93', metadata={'source': 'test.csv', 'row': 6}), Document(page_content='球队: 金莺队\\n工资(百万): 81.43\\n胜场数: 93', metadata={'source': 'test.csv', 'row': 7}), Document(page_content='球队: 光芒队\\n工资(百万): 64.17\\n胜场数: 90', metadata={'source': 'test.csv', 'row': 8}), Document(page_content='球队: 天使队\\n工资(百万): 154.49\\n胜场数: 89', metadata={'source': 'test.csv', 'row': 9}), Document(page_content='球队: 老虎队\\n工资(百万): 132.30\\n胜场数: 88', metadata={'source': 'test.csv', 'row': 10}), Document(page_content='球队: 红雀队\\n工资(百万): 110.30\\n胜场数: 88', metadata={'source': 'test.csv', 'row': 11}), Document(page_content='球队: 道奇队\\n工资(百万): 95.14\\n胜场数: 86', metadata={'source': 'test.csv', 'row': 12}), Document(page_content='球队: 白袜队\\n工资(百万): 96.92\\n胜场数: 85', metadata={'source': 'test.csv', 'row': 13}), Document(page_content='球队: 啤酒人队\\n工资(百万): 97.65\\n胜场数: 83', metadata={'source': 'test.csv', 'row': 14}), Document(page_content='球队: 费城人队\\n工资(百万): 174.54\\n胜场数: 81', metadata={'source': 'test.csv', 'row': 15}), Document(page_content='球队: 钻石backs队\\n工资(百万): 74.28\\n胜场数: 81', metadata={'source': 'test.csv', 'row': 16}), Document(page_content='球队: 海盗队\\n工资(百万): 63.43\\n胜场数: 79', metadata={'source': 'test.csv', 'row': 17}), Document(page_content='球队: 神父队\\n工资(百万): 55.24\\n胜场数: 76', metadata={'source': 'test.csv', 'row': 18}), Document(page_content='球队: 水手队\\n工资(百万): 81.97\\n胜场数: 75', metadata={'source': 'test.csv', 'row': 19}), Document(page_content='球队: 大都会队\\n工资(百万): 93.35\\n胜场数: 74', metadata={'source': 'test.csv', 'row': 20}), Document(page_content='球队: 蓝鸟队\\n工资(百万): 75.48\\n胜场数: 73', metadata={'source': 'test.csv', 'row': 21}), Document(page_content='球队: 皇家队\\n工资(百万): 60.91\\n胜场数: 72', metadata={'source': 'test.csv', 'row': 22}), Document(page_content='球队: 马林鱼队\\n工资(百万): 118.07\\n胜场数: 69', metadata={'source': 'test.csv', 'row': 23}), Document(page_content='球队: 红袜队\\n工资(百万): 173.18\\n胜场数: 69', metadata={'source': 'test.csv', 'row': 24}), Document(page_content='球队: 印第安人队\\n工资(百万): 78.43\\n胜场数: 68', metadata={'source': 'test.csv', 'row': 25}), Document(page_content='球队: 双城队\\n工资(百万): 94.08\\n胜场数: 66', metadata={'source': 'test.csv', 'row': 26}), Document(page_content='球队: 落矶队\\n工资(百万): 78.06\\n胜场数: 64', metadata={'source': 'test.csv', 'row': 27}), Document(page_content='球队: 小熊队\\n工资(百万): 88.19\\n胜场数: 61', metadata={'source': 'test.csv', 'row': 28}), Document(page_content='球队: 太空人队\\n工资(百万): 60.65\\n胜场数: 55', metadata={'source': 'test.csv', 'row': 29})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# 实例化CSVLoader，指定需要加载的CSV文件路径\n",
    "loader = CSVLoader(file_path='test.csv')\n",
    "# 调用load方法加载数据\n",
    "data = loader.load()\n",
    "# 打印加载的数据\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmVOMv62EHdX"
   },
   "source": [
    "delimiter：这是一个字符，用于分隔CSV文件中的字段。通常，大多数CSV文件使用逗号作为分隔符，但也有可能使用其他字符，如制表符（'\\t'）或分号（';'）。\n",
    "\n",
    "quotechar：这是一个字符，用于封装那些包含特殊字符（如分隔符、换行符或其他引号字符）的字段。常见的引号字符是双引号（'\"'）。如果字段中包含特殊字符，那么这个字段会被quotechar定义的字符封装起来。例如，如果你的分隔符是逗号，而你有一个字段是\"John, Smith\"，那么这个字段会被写成\"\"John, Smith\"\"，以防止逗号被误解为字段的分隔符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xck-FG9aDh9e",
    "outputId": "cc74671b-df2b-4e9c-94ec-51739fd48d4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='球队: 球队\\n工资(百万): 工资(百万)\\n胜场数: 胜场数', metadata={'source': 'test.csv', 'row': 0}), Document(page_content='球队: 国民队\\n工资(百万): 81.34\\n胜场数: 98', metadata={'source': 'test.csv', 'row': 1}), Document(page_content='球队: 红人队\\n工资(百万): 82.20\\n胜场数: 97', metadata={'source': 'test.csv', 'row': 2}), Document(page_content='球队: 洋基队\\n工资(百万): 197.96\\n胜场数: 95', metadata={'source': 'test.csv', 'row': 3}), Document(page_content='球队: 巨人队\\n工资(百万): 117.62\\n胜场数: 94', metadata={'source': 'test.csv', 'row': 4}), Document(page_content='球队: 勇士队\\n工资(百万): 83.31\\n胜场数: 94', metadata={'source': 'test.csv', 'row': 5}), Document(page_content='球队: 运动家队\\n工资(百万): 55.37\\n胜场数: 94', metadata={'source': 'test.csv', 'row': 6}), Document(page_content='球队: 游骑兵队\\n工资(百万): 120.51\\n胜场数: 93', metadata={'source': 'test.csv', 'row': 7}), Document(page_content='球队: 金莺队\\n工资(百万): 81.43\\n胜场数: 93', metadata={'source': 'test.csv', 'row': 8}), Document(page_content='球队: 光芒队\\n工资(百万): 64.17\\n胜场数: 90', metadata={'source': 'test.csv', 'row': 9}), Document(page_content='球队: 天使队\\n工资(百万): 154.49\\n胜场数: 89', metadata={'source': 'test.csv', 'row': 10}), Document(page_content='球队: 老虎队\\n工资(百万): 132.30\\n胜场数: 88', metadata={'source': 'test.csv', 'row': 11}), Document(page_content='球队: 红雀队\\n工资(百万): 110.30\\n胜场数: 88', metadata={'source': 'test.csv', 'row': 12}), Document(page_content='球队: 道奇队\\n工资(百万): 95.14\\n胜场数: 86', metadata={'source': 'test.csv', 'row': 13}), Document(page_content='球队: 白袜队\\n工资(百万): 96.92\\n胜场数: 85', metadata={'source': 'test.csv', 'row': 14}), Document(page_content='球队: 啤酒人队\\n工资(百万): 97.65\\n胜场数: 83', metadata={'source': 'test.csv', 'row': 15}), Document(page_content='球队: 费城人队\\n工资(百万): 174.54\\n胜场数: 81', metadata={'source': 'test.csv', 'row': 16}), Document(page_content='球队: 钻石backs队\\n工资(百万): 74.28\\n胜场数: 81', metadata={'source': 'test.csv', 'row': 17}), Document(page_content='球队: 海盗队\\n工资(百万): 63.43\\n胜场数: 79', metadata={'source': 'test.csv', 'row': 18}), Document(page_content='球队: 神父队\\n工资(百万): 55.24\\n胜场数: 76', metadata={'source': 'test.csv', 'row': 19}), Document(page_content='球队: 水手队\\n工资(百万): 81.97\\n胜场数: 75', metadata={'source': 'test.csv', 'row': 20}), Document(page_content='球队: 大都会队\\n工资(百万): 93.35\\n胜场数: 74', metadata={'source': 'test.csv', 'row': 21}), Document(page_content='球队: 蓝鸟队\\n工资(百万): 75.48\\n胜场数: 73', metadata={'source': 'test.csv', 'row': 22}), Document(page_content='球队: 皇家队\\n工资(百万): 60.91\\n胜场数: 72', metadata={'source': 'test.csv', 'row': 23}), Document(page_content='球队: 马林鱼队\\n工资(百万): 118.07\\n胜场数: 69', metadata={'source': 'test.csv', 'row': 24}), Document(page_content='球队: 红袜队\\n工资(百万): 173.18\\n胜场数: 69', metadata={'source': 'test.csv', 'row': 25}), Document(page_content='球队: 印第安人队\\n工资(百万): 78.43\\n胜场数: 68', metadata={'source': 'test.csv', 'row': 26}), Document(page_content='球队: 双城队\\n工资(百万): 94.08\\n胜场数: 66', metadata={'source': 'test.csv', 'row': 27}), Document(page_content='球队: 落矶队\\n工资(百万): 78.06\\n胜场数: 64', metadata={'source': 'test.csv', 'row': 28}), Document(page_content='球队: 小熊队\\n工资(百万): 88.19\\n胜场数: 61', metadata={'source': 'test.csv', 'row': 29}), Document(page_content='球队: 太空人队\\n工资(百万): 60.65\\n胜场数: 55', metadata={'source': 'test.csv', 'row': 30})]\n"
     ]
    }
   ],
   "source": [
    "# 实例化CSVLoader，指定需要加载的CSV文件路径，同时指定csv参数，包括分隔符，引用字符和字段名\n",
    "loader = CSVLoader(file_path='test.csv', csv_args={\n",
    "    'delimiter': ',',\n",
    "    'quotechar': '\"',\n",
    "    'fieldnames': ['球队', '工资(百万)', '胜场数']\n",
    "})\n",
    "# 调用load方法加载数据\n",
    "data = loader.load()\n",
    "# 打印加载的数据\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aorUXzHLEULe"
   },
   "source": [
    "制定文档对应的字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzkcD7geEXd8",
    "outputId": "a2a04ace-3029-4446-b82e-1b4109cdc3ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='球队: 国民队\\n工资(百万): 81.34\\n胜场数: 98', metadata={'source': '国民队', 'row': 0}), Document(page_content='球队: 红人队\\n工资(百万): 82.20\\n胜场数: 97', metadata={'source': '红人队', 'row': 1}), Document(page_content='球队: 洋基队\\n工资(百万): 197.96\\n胜场数: 95', metadata={'source': '洋基队', 'row': 2}), Document(page_content='球队: 巨人队\\n工资(百万): 117.62\\n胜场数: 94', metadata={'source': '巨人队', 'row': 3}), Document(page_content='球队: 勇士队\\n工资(百万): 83.31\\n胜场数: 94', metadata={'source': '勇士队', 'row': 4}), Document(page_content='球队: 运动家队\\n工资(百万): 55.37\\n胜场数: 94', metadata={'source': '运动家队', 'row': 5}), Document(page_content='球队: 游骑兵队\\n工资(百万): 120.51\\n胜场数: 93', metadata={'source': '游骑兵队', 'row': 6}), Document(page_content='球队: 金莺队\\n工资(百万): 81.43\\n胜场数: 93', metadata={'source': '金莺队', 'row': 7}), Document(page_content='球队: 光芒队\\n工资(百万): 64.17\\n胜场数: 90', metadata={'source': '光芒队', 'row': 8}), Document(page_content='球队: 天使队\\n工资(百万): 154.49\\n胜场数: 89', metadata={'source': '天使队', 'row': 9}), Document(page_content='球队: 老虎队\\n工资(百万): 132.30\\n胜场数: 88', metadata={'source': '老虎队', 'row': 10}), Document(page_content='球队: 红雀队\\n工资(百万): 110.30\\n胜场数: 88', metadata={'source': '红雀队', 'row': 11}), Document(page_content='球队: 道奇队\\n工资(百万): 95.14\\n胜场数: 86', metadata={'source': '道奇队', 'row': 12}), Document(page_content='球队: 白袜队\\n工资(百万): 96.92\\n胜场数: 85', metadata={'source': '白袜队', 'row': 13}), Document(page_content='球队: 啤酒人队\\n工资(百万): 97.65\\n胜场数: 83', metadata={'source': '啤酒人队', 'row': 14}), Document(page_content='球队: 费城人队\\n工资(百万): 174.54\\n胜场数: 81', metadata={'source': '费城人队', 'row': 15}), Document(page_content='球队: 钻石backs队\\n工资(百万): 74.28\\n胜场数: 81', metadata={'source': '钻石backs队', 'row': 16}), Document(page_content='球队: 海盗队\\n工资(百万): 63.43\\n胜场数: 79', metadata={'source': '海盗队', 'row': 17}), Document(page_content='球队: 神父队\\n工资(百万): 55.24\\n胜场数: 76', metadata={'source': '神父队', 'row': 18}), Document(page_content='球队: 水手队\\n工资(百万): 81.97\\n胜场数: 75', metadata={'source': '水手队', 'row': 19}), Document(page_content='球队: 大都会队\\n工资(百万): 93.35\\n胜场数: 74', metadata={'source': '大都会队', 'row': 20}), Document(page_content='球队: 蓝鸟队\\n工资(百万): 75.48\\n胜场数: 73', metadata={'source': '蓝鸟队', 'row': 21}), Document(page_content='球队: 皇家队\\n工资(百万): 60.91\\n胜场数: 72', metadata={'source': '皇家队', 'row': 22}), Document(page_content='球队: 马林鱼队\\n工资(百万): 118.07\\n胜场数: 69', metadata={'source': '马林鱼队', 'row': 23}), Document(page_content='球队: 红袜队\\n工资(百万): 173.18\\n胜场数: 69', metadata={'source': '红袜队', 'row': 24}), Document(page_content='球队: 印第安人队\\n工资(百万): 78.43\\n胜场数: 68', metadata={'source': '印第安人队', 'row': 25}), Document(page_content='球队: 双城队\\n工资(百万): 94.08\\n胜场数: 66', metadata={'source': '双城队', 'row': 26}), Document(page_content='球队: 落矶队\\n工资(百万): 78.06\\n胜场数: 64', metadata={'source': '落矶队', 'row': 27}), Document(page_content='球队: 小熊队\\n工资(百万): 88.19\\n胜场数: 61', metadata={'source': '小熊队', 'row': 28}), Document(page_content='球队: 太空人队\\n工资(百万): 60.65\\n胜场数: 55', metadata={'source': '太空人队', 'row': 29})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "# 实例化CSVLoader，指定需要加载的CSV文件路径，同时指定source_column参数，将\"球队\"列作为每个文档的来源\n",
    "\n",
    "loader = CSVLoader(file_path='test.csv', source_column=\"球队\")\n",
    "# 调用load方法加载数据\n",
    "data1 = loader.load()\n",
    "# 打印加载的数据\n",
    "print(data1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0nMQXIpHA6c"
   },
   "source": [
    "#2加载同一目录下面的所有文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LSidav8IdLq"
   },
   "source": [
    "在这段代码中：\n",
    "\n",
    "1. `.` 表示当前目录，`..` 表示父目录。因此，`'../'` 就表示当前目录的上一级目录。 我这里把代码和文件都放一个目录了所以直接使用\".\" 表示当前目录。\n",
    "\n",
    "2. `glob=\"**/*.md\"` 是一个glob模式，用于匹配文件路径。在这个模式中：\n",
    "\n",
    "   - `**` 表示任意数量的目录。它可以匹配零个、一个或多个子目录。例如，如果你有一个文件路径`/a/b/c.txt`，那么`**`可以匹配`/a/`、`/a/b/` 或者空。\n",
    "\n",
    "   - `*.md` 表示任意的 `.md` 文件。`*` 是一个通配符，表示任意数量的任意字符，`.md` 表示以 `.md` 结尾的文件。因此，`*.md` 可以匹配任何以 `.md` 结尾的文件。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_Mgazf2HJro",
    "outputId": "3091c3fd-60dc-4f62-a201-b65a825987b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档内容：\n",
      "标题\n",
      "\n",
      "我是一个Markdown文件的示例。\n",
      "\n",
      "元数据：\n",
      "{'source': 'markdown.md'}\n",
      "\n",
      "文档内容：\n",
      "This directory includes a few sample datasets to get you started.\n",
      "\n",
      "california_housing_data*.csv is California housing data from the 1990 US\n",
      "    Census; more information is available at:\n",
      "    https://developers.google.com/machine-learning/crash-course/california-housing-data-description\n",
      "\n",
      "mnist_*.csv is a small sample of the\n",
      "    MNIST database, which is\n",
      "    described at: http://yann.lecun.com/exdb/mnist/\n",
      "\n",
      "anscombe.json contains a copy of\n",
      "    Anscombe's quartet; it\n",
      "    was originally described in\n",
      "Anscombe, F. J. (1973). 'Graphs in Statistical Analysis'. American\n",
      "Statistician. 27 (1): 17-21. JSTOR 2682899.\n",
      "and our copy was prepared by the\n",
      "vega_datasets library.\n",
      "\n",
      "元数据：\n",
      "{'source': 'sample_data/README.md'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader, PythonLoader\n",
    "\n",
    "# 加载指定目录下所有.md文件\n",
    "loader = DirectoryLoader('.', glob=\"**/*.md\")\n",
    "# 使用TextLoader加载文件\n",
    "#loader = DirectoryLoader('../', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "# 使用PythonLoader加载.py文件\n",
    "#py_loader = DirectoryLoader('../', glob=\"**/*.py\", loader_cls=PythonLoader)\n",
    "docs = loader.load()\n",
    "# 遍历加载的文档，并打印每个文档的内容\n",
    "for doc in docs:\n",
    "    print(f\"文档内容：\\n{doc.page_content}\\n\")\n",
    "    print(f\"元数据：\\n{doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tfDJ2lVKCt_"
   },
   "source": [
    "##2.1多线程加载文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7HQvt1LKErn"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# 使用多线程加载文件\n",
    "loader = DirectoryLoader('../', glob=\"**/*.md\", use_multithreading=True)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJnmtwvqJ5-C"
   },
   "source": [
    "##2.2加载文件进度条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edvTJKhkJxKm",
    "outputId": "d6550342-585e-4525-9f9d-25d011fb6520"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 138.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# 加载文件并显示进度条\n",
    "loader = DirectoryLoader('.', glob=\"**/*.md\", show_progress=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Es63meNKIZ1"
   },
   "source": [
    "##2.3选择不同的加载器加载文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPScQtsvKLol"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# 使用TextLoader加载文件\n",
    "loader = DirectoryLoader('.', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "#py_loader = DirectoryLoader('../', glob=\"**/*.py\", loader_cls=PythonLoader)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NydVaIELMuPt"
   },
   "source": [
    "#3.HTMl文件加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTc6S0HIMy-r",
    "outputId": "02a853f9-2b62-45c7-c40c-1f7f3037cb96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档内容：\n",
      "我的第一个标题\n",
      "\n",
      "我的第一个段落。\n",
      "\n",
      "元数据：\n",
      "{'source': 'fake-content.html'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "# 创建一个加载器来加载HTML文件\n",
    "loader = UnstructuredHTMLLoader(\"fake-content.html\")\n",
    "\n",
    "# 加载数据\n",
    "data = loader.load()\n",
    "for doc in data:\n",
    "    print(f\"文档内容：\\n{doc.page_content}\\n\")\n",
    "    print(f\"元数据：\\n{doc.metadata}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rh2wGcHWN0BD",
    "outputId": "6f114162-80c7-47ee-dfe3-b105c5d965f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档内容：\n",
      "\n",
      "\n",
      "我的第一个标题\n",
      "\n",
      "\n",
      "我的第一个标题\n",
      "我的第一个段落。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "元数据：\n",
      "{'source': 'fake-content.html', 'title': '我的第一个标题'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader\n",
    "\n",
    "# 创建一个使用BeautifulSoup4的加载器来加载HTML文件\n",
    "loader = BSHTMLLoader(\"fake-content.html\")\n",
    "\n",
    "# 加载数据\n",
    "data = loader.load()\n",
    "for doc in data:\n",
    "    print(f\"文档内容：\\n{doc.page_content}\\n\")\n",
    "    print(f\"元数据：\\n{doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3uvwUn6SlaI"
   },
   "source": [
    "#4.Json\n",
    "\n",
    "jq_schema用于指定我们要从JSON数据中提取的部分。\n",
    "\n",
    "在jq_schema='.content'中，.content表示我们要从JSON对象中提取名为content的键的值。\n",
    "\n",
    "对于jq_schema='.messages[]'，.messages[]表示我们要遍历名为messages的键的数组，并为数组中的每个对象提取信息。\n",
    "\n",
    "content_key参数用于指定从每个JSON对象中提取哪个键作为Document对象的page_content。在这个例子中，content_key=\"content\"表示我们正在从每个messages数组中的对象提取content键的值作为page_content。\n",
    "\n",
    "metadata_func函数则用于从每个JSON对象中提取元数据。在这个例子中，metadata_func函数从每个messages数组中的对象提取sender_name和timestamp_ms键的值，并将它们添加到Document对象的元数据中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5Iq-61bSnJj",
    "outputId": "8fef268a-f92f-407e-fe0e-2c9c48822f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档内容：\n",
      "消息1\n",
      "\n",
      "元数据：\n",
      "{'source': '/content/chat.json', 'seq_num': 1, 'sender_name': '用户1', 'timestamp_ms': 1675597571851}\n",
      "\n",
      "文档内容：\n",
      "消息2\n",
      "\n",
      "元数据：\n",
      "{'source': '/content/chat.json', 'seq_num': 2, 'sender_name': '用户2', 'timestamp_ms': 1675597435669}\n",
      "\n",
      "文档内容：\n",
      "消息3\n",
      "\n",
      "元数据：\n",
      "{'source': '/content/chat.json', 'seq_num': 3, 'sender_name': '用户1', 'timestamp_ms': 1675596277579}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "# 使用指定的jq模式从JSON文件加载数据\n",
    "loader = JSONLoader(\n",
    "    file_path='chat.json',\n",
    "    jq_schema='.messages[].content'\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "# 加载JSON行文件\n",
    "loader = JSONLoader(\n",
    "    file_path='chat.jsonl',\n",
    "    jq_schema='.content',\n",
    "    json_lines=True\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "# 提取元数据\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    # 添加自定义元数据\n",
    "    metadata[\"sender_name\"] = record.get(\"sender_name\")\n",
    "    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='chat.json',\n",
    "    jq_schema='.messages[]',\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func\n",
    ")\n",
    "data = loader.load()\n",
    "for doc in data:\n",
    "    print(f\"文档内容：\\n{doc.page_content}\\n\")\n",
    "    print(f\"元数据：\\n{doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNpIYlFbTQjW"
   },
   "source": [
    "#5.PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fz_XGcQNLV6",
    "outputId": "14b2b4e9-03da-4d93-990c-22310f16e859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.12.2-py3-none-any.whl (254 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.0/255.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-3.12.2\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYUfyouWNPG7",
    "outputId": "86ebb1b5-5cbe-4c2a-c47c-d618fc425a4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='我是一个pdf 文档 西游记的主人公是谁？ 师徒四人。', metadata={'source': 'file.pdf', 'page': 0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 安装pypdf\n",
    "# pip install pypdf\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# 使用PyPDFLoader加载PDF文件\n",
    "loader = PyPDFLoader(\"file.pdf\")\n",
    "# 加载文件之后将文件按照页进行切分\n",
    "pages = loader.load_and_split()\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gzqe2yB6OTjF"
   },
   "source": [
    "将上面的文档向量化之后保存到向量数据库中，然后进行查询:\n",
    "\n",
    "similarity_search 是一种查找最接近给定查询的文档的方法。在这个例子中，它被用来查找与查询 \"西游记的主人公是谁？\" 最相关的文档。k=1 表示我们只想返回一个最相关的文档。\n",
    "\n",
    "在执行 similarity_search 时，首先会将查询转换为一个向量（或嵌入），然后在文档集的嵌入中查找最接近查询嵌入的文档。这种接近度通常是通过计算嵌入之间的余弦相似度来衡量的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GB1UQ_WVOb7X",
    "outputId": "6081d3a4-4dbb-4bf6-a12e-6ef6e5d458ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 我是一个pdf 文档 西游记的主人公是谁？ 师徒四人。\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"西游记的主人公是谁？\", k=1)\n",
    "#docs 是一个文档的数据，里面每个doc 就是一页pdf\n",
    "#similarity_search 方法通过词向量的方式匹配与输入相似的词向量所在的页\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOLIK-bHQ98E",
    "outputId": "d59c84a4-180c-4589-c41b-c0cec4a02a9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='我是一个 pdf 文档 西游记的主人公是谁？ 师徒四人。', metadata={'source': 'file.pdf', 'coordinates': {'points': ((90.950552, 75.64051999999992), (90.950552, 117.40052000000003), (198.84399200000001, 117.40052000000003), (198.84399200000001, 75.64051999999992)), 'system': 'PixelSpace', 'layout_width': 595.2756, 'layout_height': 841.8898}, 'filename': 'file.pdf', 'filetype': 'application/pdf', 'page_number': 1, 'category': 'Title'})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# 使用UnstructuredPDFLoader加载PDF文件\n",
    "loader = UnstructuredPDFLoader(\"file.pdf\", mode=\"elements\")\n",
    "data = loader.load()\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7yw4vJF3oIv"
   },
   "source": [
    "#6.Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKPrYmgT3tUu",
    "outputId": "c17966ad-6d7c-4b8c-98a2-158b47cc2b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='孙悟空在莲花洞和红孩儿展开了一场激烈的争斗。\\n双方你来我往，招招凌厉。悟空化身诸多分身，红孩儿则化作烈火炙烤。\\n火焰蔓延，山洞内弥漫着熊熊火光，热浪滚滚而来。悟空大喝一声，举起金箍棒，猛烈地击打着红孩儿。\\n棒影纵横，势不可挡。红孩儿机敏地闪躲，使出了自己的招数，引动山洞内的岩石砸向悟空。悟空身手矫健，灵活地躲避着。\\n两人你来我往，打得难解难分。就在这时，观音菩萨突然现身，以神威庇护众人。\\n悟空和红孩儿都停下了动作，俯首顿足。\\n观音菩萨温和地说道：“你们何必争斗呢？都是本是同根生，相煎何太急？放下仇恨，回头是岸。”\\n听到观音菩萨的教诲，悟空和红孩儿心生悔意，彼此握手言和。他们决定共同修行，改过自新，为西游取经尽一份力。\\n从此，他们结成了不解的好兄弟，一同踏上了西天取经之路。' metadata={}\n"
     ]
    }
   ],
   "source": [
    "# 定义一个长文本。\n",
    "state_of_the_union = \"\"\"\n",
    "孙悟空在莲花洞和红孩儿展开了一场激烈的争斗。\n",
    "双方你来我往，招招凌厉。悟空化身诸多分身，红孩儿则化作烈火炙烤。\n",
    "火焰蔓延，山洞内弥漫着熊熊火光，热浪滚滚而来。悟空大喝一声，举起金箍棒，猛烈地击打着红孩儿。\n",
    "棒影纵横，势不可挡。红孩儿机敏地闪躲，使出了自己的招数，引动山洞内的岩石砸向悟空。悟空身手矫健，灵活地躲避着。\n",
    "两人你来我往，打得难解难分。就在这时，观音菩萨突然现身，以神威庇护众人。\n",
    "悟空和红孩儿都停下了动作，俯首顿足。\n",
    "观音菩萨温和地说道：“你们何必争斗呢？都是本是同根生，相煎何太急？放下仇恨，回头是岸。”\n",
    "听到观音菩萨的教诲，悟空和红孩儿心生悔意，彼此握手言和。他们决定共同修行，改过自新，为西游取经尽一份力。\n",
    "从此，他们结成了不解的好兄弟，一同踏上了西天取经之路。\n",
    "\"\"\"\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWf4xztyHDY_"
   },
   "source": [
    "#7.提取文档信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uR5cG09-J2EN",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "287d0eaa-81b6-49a9-b415-ce57ccab2be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting doctran\n",
      "  Downloading doctran-0.0.9-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: lxml<5.0.0,>=4.9.2 in /usr/local/lib/python3.10/dist-packages (from doctran) (4.9.3)\n",
      "Requirement already satisfied: openai<0.28.0,>=0.27.8 in /usr/local/lib/python3.10/dist-packages (from doctran) (0.27.8)\n",
      "Collecting presidio-analyzer<3.0.0,>=2.2.33 (from doctran)\n",
      "  Downloading presidio_analyzer-2.2.33-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting presidio-anonymizer<3.0.0,>=2.2.33 (from doctran)\n",
      "  Downloading presidio_anonymizer-2.2.33-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.10.9 in /usr/local/lib/python3.10/dist-packages (from doctran) (1.10.11)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.5.4 in /usr/local/lib/python3.10/dist-packages (from doctran) (3.5.4)\n",
      "Collecting tiktoken<0.4.0,>=0.3.3 (from doctran)\n",
      "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->doctran) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->doctran) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->doctran) (3.8.4)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer<3.0.0,>=2.2.33->doctran) (2022.10.31)\n",
      "Collecting tldextract (from presidio-analyzer<3.0.0,>=2.2.33->doctran)\n",
      "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer<3.0.0,>=2.2.33->doctran) (6.0)\n",
      "Collecting phonenumbers>=8.12 (from presidio-analyzer<3.0.0,>=2.2.33->doctran)\n",
      "  Downloading phonenumbers-8.13.16-py2.py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodome>=3.10.1 (from presidio-anonymizer<3.0.0,>=2.2.33->doctran)\n",
      "  Downloading pycryptodome-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.9->doctran) (4.7.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (1.22.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.4->doctran) (3.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->doctran) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->doctran) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->doctran) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->doctran) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.4->doctran) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.4->doctran) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.5.4->doctran) (8.1.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->doctran) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->doctran) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->doctran) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->doctran) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->doctran) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->doctran) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.5.4->doctran) (2.1.3)\n",
      "Collecting requests-file>=1.4 (from tldextract->presidio-analyzer<3.0.0,>=2.2.33->doctran)\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->presidio-analyzer<3.0.0,>=2.2.33->doctran) (3.12.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file>=1.4->tldextract->presidio-analyzer<3.0.0,>=2.2.33->doctran) (1.16.0)\n",
      "Installing collected packages: phonenumbers, pycryptodome, tiktoken, requests-file, presidio-anonymizer, tldextract, presidio-analyzer, doctran\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.4.0\n",
      "    Uninstalling tiktoken-0.4.0:\n",
      "      Successfully uninstalled tiktoken-0.4.0\n",
      "Successfully installed doctran-0.0.9 phonenumbers-8.13.16 presidio-analyzer-2.2.33 presidio-anonymizer-2.2.33 pycryptodome-3.18.0 requests-file-1.5.1 tiktoken-0.3.3 tldextract-3.4.4\n"
     ]
    }
   ],
   "source": [
    "pip install doctran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gku31rZRCQhY"
   },
   "source": [
    "调用如下代码， 我会得到这个错误OpenAI function call failed: The model: `gpt-4` does not exist\n",
    "意思是OpenAI 还没有给我开发 GPT -4 模型。 这个我在网上查了一下，虽然我是GPT Plus 会员，但是还没有进行过一次支付，需要在月底进行一次Develper 支付才能开放给我。 也就是使用OpenAI 调用Token 付费的功能，用信用卡支付之后。所以，我会持续观察。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XS21EADaHGVN",
    "outputId": "79c2926c-184b-4790-f1fa-3fd4ed15b588",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "日期：2023年7月1日\n",
      "\n",
      "主题：关于各种话题的更新和讨论\n",
      "\n",
      "亲爱的团队，\n",
      "\n",
      "希望这封邮件能找到你们的好。在这个文档中，我想给你们提供一些重要的更新，并讨论一些需要我们关注的各种话题。请把这里面的信息当作高度机密对待。\n",
      "\n",
      "安全和隐私措施\n",
      "作为我们持续保证客户数据安全和隐私的承诺的一部分，我们已在所有系统中实施了强有力的措施。我们要表扬来自IT部门的John Doe（邮箱：john.doe@example.com）对增强我们网络安全的勤勉工作。向前看，我们恳请大家严格遵守我们的数据保护政策和指南。另外，如果你们发现任何可能的安全风险或事件，请立即向我们的专门团队security@example.com报告。\n",
      "\n",
      "人力资源更新和员工福利\n",
      "最近，我们欢迎了几位在各自部门做出重大贡献的新团队成员。我想表扬Jane Smith（社保号：049-45-5928）在客户服务中的出色表现。Jane一直得到我们客户的积极反馈。此外，请记住我们的员工福利计划的开放报名期即将到来。如果你们有任何问题或需要帮助，请联系我们的人力资源代表Michael Johnson（电话：418-492-3850，邮箱：michael.johnson@example.com）。\n",
      "\n",
      "市场倡议和活动\n",
      "我们的市场团队一直在积极开发新策略，以增加品牌知名度和推动客户参与。我们要感谢Sarah Thompson（电话：415-555-1234）在管理我们的社交媒体平台方面的出色努力。Sarah在过去的一个月里成功地增加了我们20%的关注者。此外，请在你们的日历上标记即将于7月15日举行的产品发布活动。我们鼓励所有团队成员参加，并支持我们公司的这个激动人心的里程碑。\n",
      "\n",
      "研发项目\n",
      "在我们追求创新的过程中，我们的研发部门一直在各种项目上不知疲倦地工作。我想表扬David Rodriguez（邮箱：david.rodriguez@example.com）在项目领导角色中的杰出工作。David对我们开发尖端技术的贡献一直是不可或缺的。此外，我们想提醒大家在我们每月的研发头脑风暴会议中分享他们的想法和建议，会议定于7月10日。\n",
      "\n",
      "请把这个文档中的信息当作最高机密对待，并确保它不会被未经授权的个人分享。如果你们对讨论的话题有任何问题或疑虑，请随时直接联系我。\n",
      "\n",
      "感谢你们的关注，让我们继续携手努力，实现我们的目标。\n",
      "\n",
      "最好的祝福，\n",
      "\n",
      "Jason Fan\n",
      "联合创始人 & CEO\n",
      "Psychic\n",
      "jason@psychic.dev\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/transformers/transformers.py\u001b[0m in \u001b[0;36mexecuteOpenAICall\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatCompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfunction_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"function_call\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arguments\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The model: `gpt-4` does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/doctran.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformed_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtransformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mtransformed_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/transformers/transformers.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteOpenAICall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/transformers/transformers.py\u001b[0m in \u001b[0;36mexecuteOpenAICall\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OpenAI function call failed: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: OpenAI function call failed: The model: `gpt-4` does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-50025a8d1a83>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mproperty_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoctranPropertyExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m extracted_document = await property_extractor.atransform_documents(\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_transformers/doctran_text_extract.py\u001b[0m in \u001b[0;36matransform_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             doctran_doc = (\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mdoctran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/doctran.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtransformed_document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error executing transformation {transformation}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExtractProperty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DocumentTransformationBuilder'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error executing transformation (<Transformation.extract: 'DocumentExtractor'>, {'properties': [ExtractProperty(name='邮件类型', description='这封邮件的类型是什么。', type='string', items=None, enum=['更新', '待办事项', '客户反馈', '公告', '其他'], required=True), ExtractProperty(name='提及的人', description='这封邮件中提到的所有人的列表。', type='array', items={'name': '全名', 'description': '提到的人的全名。', 'type': 'string'}, enum=None, required=True), ExtractProperty(name='小白解释', description='像对五岁小孩解释这封邮件。', type='string', items=None, enum=None, required=True)]}): OpenAI function call failed: The model: `gpt-4` does not exist"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "from langchain.document_transformers import DoctranPropertyExtractor\n",
    "\n",
    "\n",
    "sample_text = \"\"\"\n",
    "\n",
    "日期：2023年7月1日\n",
    "\n",
    "主题：关于各种话题的更新和讨论\n",
    "\n",
    "亲爱的团队，\n",
    "\n",
    "希望这封邮件能找到你们的好。在这个文档中，我想给你们提供一些重要的更新，并讨论一些需要我们关注的各种话题。请把这里面的信息当作高度机密对待。\n",
    "\n",
    "安全和隐私措施\n",
    "作为我们持续保证客户数据安全和隐私的承诺的一部分，我们已在所有系统中实施了强有力的措施。我们要表扬来自IT部门的John Doe（邮箱：john.doe@example.com）对增强我们网络安全的勤勉工作。向前看，我们恳请大家严格遵守我们的数据保护政策和指南。另外，如果你们发现任何可能的安全风险或事件，请立即向我们的专门团队security@example.com报告。\n",
    "\n",
    "人力资源更新和员工福利\n",
    "最近，我们欢迎了几位在各自部门做出重大贡献的新团队成员。我想表扬Jane Smith（社保号：049-45-5928）在客户服务中的出色表现。Jane一直得到我们客户的积极反馈。此外，请记住我们的员工福利计划的开放报名期即将到来。如果你们有任何问题或需要帮助，请联系我们的人力资源代表Michael Johnson（电话：418-492-3850，邮箱：michael.johnson@example.com）。\n",
    "\n",
    "市场倡议和活动\n",
    "我们的市场团队一直在积极开发新策略，以增加品牌知名度和推动客户参与。我们要感谢Sarah Thompson（电话：415-555-1234）在管理我们的社交媒体平台方面的出色努力。Sarah在过去的一个月里成功地增加了我们20%的关注者。此外，请在你们的日历上标记即将于7月15日举行的产品发布活动。我们鼓励所有团队成员参加，并支持我们公司的这个激动人心的里程碑。\n",
    "\n",
    "研发项目\n",
    "在我们追求创新的过程中，我们的研发部门一直在各种项目上不知疲倦地工作。我想表扬David Rodriguez（邮箱：david.rodriguez@example.com）在项目领导角色中的杰出工作。David对我们开发尖端技术的贡献一直是不可或缺的。此外，我们想提醒大家在我们每月的研发头脑风暴会议中分享他们的想法和建议，会议定于7月10日。\n",
    "\n",
    "请把这个文档中的信息当作最高机密对待，并确保它不会被未经授权的个人分享。如果你们对讨论的话题有任何问题或疑虑，请随时直接联系我。\n",
    "\n",
    "感谢你们的关注，让我们继续携手努力，实现我们的目标。\n",
    "\n",
    "最好的祝福，\n",
    "\n",
    "Jason Fan\n",
    "联合创始人 & CEO\n",
    "Psychic\n",
    "jason@psychic.dev\n",
    "\"\"\"\n",
    "print(sample_text)\n",
    "documents = [Document(page_content=sample_text)]\n",
    "properties = [\n",
    "    {\n",
    "        \"name\": \"邮件类型\",\n",
    "        \"description\": \"这封邮件的类型是什么。\",\n",
    "        \"type\": \"string\",\n",
    "        \"enum\": [\"更新\", \"待办事项\", \"客户反馈\", \"公告\", \"其他\"],\n",
    "        \"required\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"提及的人\",\n",
    "        \"description\": \"这封邮件中提到的所有人的列表。\",\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "            \"name\": \"全名\",\n",
    "            \"description\": \"提到的人的全名。\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "        \"required\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"小白解释\",\n",
    "        \"description\": \"像对五岁小孩解释这封邮件。\",\n",
    "        \"type\": \"string\",\n",
    "        \"required\": True,\n",
    "    },\n",
    "]\n",
    "property_extractor = DoctranPropertyExtractor(properties=properties)\n",
    "extracted_document = await property_extractor.atransform_documents(\n",
    "    documents, properties=properties\n",
    ")\n",
    "print(json.dumps(extracted_document[0].metadata, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4WDyV5VGLJR"
   },
   "source": [
    "#8.文字转问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "qN0676WjGPbs",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "fdb0260c-9d08-41fc-fdea-75dc41af8f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "日期：2023年7月1日\n",
      "\n",
      "主题：关于各种话题的更新和讨论\n",
      "\n",
      "亲爱的团队，\n",
      "\n",
      "我希望这封邮件能找到你们的好。在这个文档中，我想给你们提供一些重要的更新，并讨论一些需要我们注意的各种话题。请把这里面的信息当作高度机密对待。\n",
      "\n",
      "安全和隐私措施\n",
      "作为我们持续确保客户数据的安全和隐私的承诺的一部分，我们在所有的系统中都实施了强大的措施。我们想要表扬IT部门的John Doe（电子邮件：john.doe@example.com）他在加强我们网络安全方面的勤奋工作。未来，我们希望每个人都严格遵守我们的数据保护政策和指导方针。另外，如果你发现任何可能的安全风险或事件，请立即向我们的专门团队security@example.com报告。\n",
      "\n",
      "人力资源更新和员工福利\n",
      "最近，我们欢迎了几位新的团队成员，他们在各自的部门做出了重大的贡献。我想要认可Jane Smith（社会安全号码：049-45-5928）在客户服务方面的杰出表现。Jane一直得到我们客户的积极反馈。此外，请记住，我们的员工福利计划的开放注册期即将到来。如果你有任何问题或需要帮助，请联系我们的人力资源代表Michael Johnson（电话：418-492-3850，电子邮件：michael.johnson@example.com）。\n",
      "\n",
      "营销倡议和活动\n",
      "我们的营销团队一直在积极开发新的策略，以提高品牌知名度并推动客户参与。我们想要感谢Sarah Thompson（电话：415-555-1234）在管理我们的社交媒体平台方面的特殊努力。Sarah在过去的一个月里成功地增加了我们的关注者基础20%。此外，请在你的日历上标记即将在7月15日举行的产品发布活动。我们鼓励所有团队成员参加并支持我们公司的这个激动人心的里程碑。\n",
      "\n",
      "研究和开发项目\n",
      "在我们追求创新的过程中，我们的研究和开发部门一直在各种项目上不知疲倦地工作。我想要承认David Rodriguez（电子邮件：david.rodriguez@example.com）在作为项目负责人的角色中的卓越工作。David对我们的尖端技术的开发做出了重要的贡献。此外，我们想要提醒每个人在我们每月的研发头脑风暴会议中分享他们的想法和建议，该会议定于7月10日进行。\n",
      "\n",
      "请对这个文档中的信息保密，确保不与未经授权的个人分享。如果你对讨论的话题有任何问题或关注，请随时直接联系我。\n",
      "\n",
      "感谢你的关注，让我们继续合作以实现我们的目标。\n",
      "\n",
      "最好的祝愿，\n",
      "\n",
      "Jason Fan\n",
      "联合创始人兼首席执行官\n",
      "Psychic\n",
      "jason@psychic.dev\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/transformers/transformers.py\u001b[0m in \u001b[0;36mexecuteOpenAICall\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatCompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfunction_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"function_call\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arguments\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The model: `gpt-4` does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/doctran.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformed_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtransformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mtransformed_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/transformers/transformers.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteOpenAICall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/transformers/transformers.py\u001b[0m in \u001b[0;36mexecuteOpenAICall\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OpenAI function call failed: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: OpenAI function call failed: The model: `gpt-4` does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-57beafad822b>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mqa_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoctranQATransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtransformed_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mqa_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matransform_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# 打印出转换后的文档的元数据，其中包含了生成的问题和答案\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_transformers/doctran_text_qa.py\u001b[0m in \u001b[0;36matransform_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             doctran_doc = (\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mdoctran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterrogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             )\n\u001b[1;32m     50\u001b[0m             questions_and_answers = doctran_doc.extracted_properties.get(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/doctran.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtransformed_document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error executing transformation {transformation}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExtractProperty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DocumentTransformationBuilder'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error executing transformation (<Transformation.interrogate: 'DocumentInterrogator'>, {}): OpenAI function call failed: The model: `gpt-4` does not exist"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "from langchain.document_transformers import DoctranQATransformer\n",
    "\n",
    "sample_text = \"\"\"\n",
    "\n",
    "日期：2023年7月1日\n",
    "\n",
    "主题：关于各种话题的更新和讨论\n",
    "\n",
    "亲爱的团队，\n",
    "\n",
    "我希望这封邮件能找到你们的好。在这个文档中，我想给你们提供一些重要的更新，并讨论一些需要我们注意的各种话题。请把这里面的信息当作高度机密对待。\n",
    "\n",
    "安全和隐私措施\n",
    "作为我们持续确保客户数据的安全和隐私的承诺的一部分，我们在所有的系统中都实施了强大的措施。我们想要表扬IT部门的John Doe（电子邮件：john.doe@example.com）他在加强我们网络安全方面的勤奋工作。未来，我们希望每个人都严格遵守我们的数据保护政策和指导方针。另外，如果你发现任何可能的安全风险或事件，请立即向我们的专门团队security@example.com报告。\n",
    "\n",
    "人力资源更新和员工福利\n",
    "最近，我们欢迎了几位新的团队成员，他们在各自的部门做出了重大的贡献。我想要认可Jane Smith（社会安全号码：049-45-5928）在客户服务方面的杰出表现。Jane一直得到我们客户的积极反馈。此外，请记住，我们的员工福利计划的开放注册期即将到来。如果你有任何问题或需要帮助，请联系我们的人力资源代表Michael Johnson（电话：418-492-3850，电子邮件：michael.johnson@example.com）。\n",
    "\n",
    "营销倡议和活动\n",
    "我们的营销团队一直在积极开发新的策略，以提高品牌知名度并推动客户参与。我们想要感谢Sarah Thompson（电话：415-555-1234）在管理我们的社交媒体平台方面的特殊努力。Sarah在过去的一个月里成功地增加了我们的关注者基础20%。此外，请在你的日历上标记即将在7月15日举行的产品发布活动。我们鼓励所有团队成员参加并支持我们公司的这个激动人心的里程碑。\n",
    "\n",
    "研究和开发项目\n",
    "在我们追求创新的过程中，我们的研究和开发部门一直在各种项目上不知疲倦地工作。我想要承认David Rodriguez（电子邮件：david.rodriguez@example.com）在作为项目负责人的角色中的卓越工作。David对我们的尖端技术的开发做出了重要的贡献。此外，我们想要提醒每个人在我们每月的研发头脑风暴会议中分享他们的想法和建议，该会议定于7月10日进行。\n",
    "\n",
    "请对这个文档中的信息保密，确保不与未经授权的个人分享。如果你对讨论的话题有任何问题或关注，请随时直接联系我。\n",
    "\n",
    "感谢你的关注，让我们继续合作以实现我们的目标。\n",
    "\n",
    "最好的祝愿，\n",
    "\n",
    "Jason Fan\n",
    "联合创始人兼首席执行官\n",
    "Psychic\n",
    "jason@psychic.dev\n",
    "\"\"\"\n",
    "print(sample_text)\n",
    "\n",
    "documents = [Document(page_content=sample_text)]\n",
    "qa_transformer = DoctranQATransformer()\n",
    "\n",
    "transformed_document = await qa_transformer.atransform_documents(documents)\n",
    "\n",
    "# 打印出转换后的文档的元数据，其中包含了生成的问题和答案\n",
    "print(json.dumps(transformed_document[0].metadata, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rJzCqM_JPMH"
   },
   "source": [
    "#9.文字翻译\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "collapsed": true,
    "id": "P4ghU0JZJRew",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1e972e7a-d4ae-4890-c1bd-5d4dc41fb58d"
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/transformers/transformers.py\u001b[0m in \u001b[0;36mexecuteOpenAICall\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatCompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfunction_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"function_call\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arguments\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The model: `gpt-4` does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/doctran.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformed_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtransformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mtransformed_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/transformers/transformers.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteOpenAICall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/transformers/transformers.py\u001b[0m in \u001b[0;36mexecuteOpenAICall\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OpenAI function call failed: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: OpenAI function call failed: The model: `gpt-4` does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9496a1ac0744>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# 翻译文档\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtranslated_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mqa_translator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matransform_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# 打印翻译后的文档\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_transformers/doctran_text_translate.py\u001b[0m in \u001b[0;36matransform_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         ]\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoctran_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mdoctran_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         return [\n\u001b[1;32m     57\u001b[0m             \u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformed_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doctran/doctran.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtransformed_document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error executing transformation {transformation}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExtractProperty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DocumentTransformationBuilder'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error executing transformation (<Transformation.translate: 'DocumentTranslator'>, {'language': 'spanish'}): OpenAI function call failed: The model: `gpt-4` does not exist"
     ]
    }
   ],
   "source": [
    "# 引入必要的库\n",
    "from langchain.schema import Document\n",
    "from langchain.document_transformers import DoctranTextTranslator\n",
    "\n",
    "# 输入文档内容\n",
    "sample_text = \"\"\"\n",
    "\n",
    "日期：2023年7月1日\n",
    "\n",
    "主题：关于各种话题的更新和讨论\n",
    "\n",
    "亲爱的团队，\n",
    "\n",
    "希望这封邮件找到你们时，一切安好。在这份文件中，我想向你们提供一些重要的更新，并讨论一些需要我们关注的话题。请将这里包含的信息视为高度机密。\n",
    "\n",
    "... （此处省略了文档的其余部分）\n",
    "\"\"\"\n",
    "\n",
    "# 将文本封装到Document对象中\n",
    "documents = [Document(page_content=sample_text)]\n",
    "\n",
    "# 初始化DoctranTextTranslator对象，设置目标语言为西班牙语\n",
    "qa_translator = DoctranTextTranslator(language=\"spanish\")\n",
    "\n",
    "# 翻译文档\n",
    "translated_document = await qa_translator.atransform_documents(documents)\n",
    "\n",
    "# 打印翻译后的文档\n",
    "print(translated_document[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZtyLQDGLpc0"
   },
   "source": [
    "#10.标记结构化数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elK2jukFLsXp"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_transformers.openai_functions import create_metadata_tagger\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"movie_title\": {\"type\": \"string\"},\n",
    "        \"critic\": {\"type\": \"string\"},\n",
    "        \"tone\": {\"type\": \"string\", \"enum\": [\"positive\", \"negative\"]},\n",
    "        \"rating\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"The number of stars the critic rated the movie\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"movie_title\", \"critic\", \"tone\"],\n",
    "}\n",
    "\n",
    "# Must be an OpenAI model that supports functions\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "document_transformer = create_metadata_tagger(metadata_schema=schema, llm=llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9Q7O5p0OTN9"
   },
   "source": [
    "\"reliable\" 是一个元数据键，用来指示文档的可靠性。它的值被设置为 False，意味着这份文档（也就是这个电影评论）可能不可靠。这可能是因为评论是匿名发布的，或者出于其他的一些原因。\n",
    "\n",
    "这个元数据可以在后续的处理、分析或决策中被使用。例如，你可能会选择忽略那些被标记为不可靠的评论，或者在分析时给予它们较低的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jV4SzJShLwpp",
    "outputId": "47ae78f2-680e-49a9-dea3-ca34185e26b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review of The Bee Movie\n",
      "By Roger Ebert\n",
      "\n",
      "This is the greatest movie ever made. 4 out of 5 stars.\n",
      "\n",
      "{\"movie_title\": \"The Bee Movie\", \"critic\": \"Roger Ebert\", \"tone\": \"positive\", \"rating\": 4}\n",
      "\n",
      "---------------\n",
      "\n",
      "Review of The Godfather\n",
      "By Anonymous\n",
      "\n",
      "This movie was super boring. 1 out of 5 stars.\n",
      "\n",
      "{\"movie_title\": \"The Godfather\", \"critic\": \"Anonymous\", \"tone\": \"negative\", \"rating\": 1, \"reliable\": false}\n"
     ]
    }
   ],
   "source": [
    "original_documents = [\n",
    "    Document(\n",
    "        page_content=\"Review of The Bee Movie\\nBy Roger Ebert\\n\\nThis is the greatest movie ever made. 4 out of 5 stars.\"\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Review of The Godfather\\nBy Anonymous\\n\\nThis movie was super boring. 1 out of 5 stars.\",\n",
    "        metadata={\"reliable\": False},\n",
    "    ),\n",
    "]\n",
    "enhanced_documents = document_transformer.transform_documents(original_documents)\n",
    "import json\n",
    "print(\n",
    "    *[d.page_content + \"\\n\\n\" + json.dumps(d.metadata) for d in enhanced_documents],\n",
    "    sep=\"\\n\\n---------------\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77idQ_IGcuIH"
   },
   "source": [
    "#11.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIVixhyjcwqq",
    "outputId": "9edf6f90-4868-43c9-f240-875e616cf74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0008689711103215814,\n",
       " -0.017755543813109398,\n",
       " 0.02561519294977188,\n",
       " 0.0006347055896185338,\n",
       " -0.01870233379304409]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "[\n",
    "\"我在这里!\",\n",
    "\"你好啊!\",\n",
    "\"你叫什么名字?\",\n",
    "\"大家都叫我小明\",\n",
    "\"小明你好!\"\n",
    "]\n",
    ")\n",
    "#向量数组的长度， 数据的每个item存放一句话\n",
    "print(len(embeddings))\n",
    "#向量的维度是 1536\n",
    "print( len(embeddings[3]))\n",
    "embedded_query = embeddings_model.embed_query(\"对话中提到了谁?\")\n",
    "embedded_query\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDQL_S6ghkXj"
   },
   "source": [
    "#12.Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxZZ55bmilgq"
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC6h0ztcnM8j"
   },
   "source": [
    "##12.1一般用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLP7MLL3hoiE",
    "outputId": "c14d7ed2-f5b5-4ec2-b7d9-479024cb5004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明说：“今天天气真好啊”\n",
      "小红说：“冰淇淋真好吃”\n",
      "小刚在打篮球\n",
      "小非在踢足球\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "#加载文档\n",
    "loader = TextLoader(\"test.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "#将文档切割成小块\n",
    "text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "#使用句子转换器对文本进行Embedding\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#将向量保存到Chroma中\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "#查询，返回最相似的文档块（chunk）\n",
    "query = \"小明在干什么？\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsZ_jlN3nScw"
   },
   "source": [
    "##12.2向量数据持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dM-BW-glnIS",
    "outputId": "db77494a-301f-43b0-860f-b444def8e7b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明说：“今天天气真好啊”\n",
      "小红说：“冰淇淋真好吃”\n",
      "小刚在打篮球\n",
      "小非在踢足球\n"
     ]
    }
   ],
   "source": [
    "#将 chroma 数据保存到磁盘上\n",
    "db_to_disk = Chroma.from_documents(docs, embedding_function, persist_directory=\"chroma_db\")\n",
    "db_to_disk.persist()\n",
    "docs = db_to_disk.similarity_search(query)\n",
    "\n",
    "# load from disk\n",
    "db_to_disk = Chroma(persist_directory=\"chroma_db\", embedding_function=embedding_function)\n",
    "docs = db_to_disk.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCSVj467nX0j"
   },
   "source": [
    "##12.3定义metadata 以便查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvFcmmQKmYrs",
    "outputId": "bc003949-2009-43f1-a697-e217c083ed8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'test.txt'}\n",
      "{'ids': ['1'], 'embeddings': None, 'metadatas': [{'source': 'specified-source'}], 'documents': ['小明说：“今天天气真好啊”\\n小红说：“冰淇淋真好吃”\\n小刚在打篮球\\n小非在踢足球']}\n"
     ]
    }
   ],
   "source": [
    "# create simple ids\n",
    "ids = [str(i) for i in range(1, len(docs) + 1)]\n",
    "\n",
    "# 插入文本块ID，针对每个文本块 配置对应的ID\n",
    "example_db = Chroma.from_documents(docs, embedding_function, ids=ids)\n",
    "docs = example_db.similarity_search(query)\n",
    "print(docs[0].metadata)\n",
    "\n",
    "# 更新文档的metadata 加入source 的字段输入定义好的信息，为了后面查询做准备\n",
    "docs[0].metadata = {\"source\": \"specified-source\"}\n",
    "example_db.update_document(ids[0], docs[0])\n",
    "print(example_db._collection.get(ids=[ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXUCWCiGmkbL",
    "outputId": "5dabb155-fdc3-4505-c31d-8e934c21ef02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'source': 'specified-source'}],\n",
       " 'documents': ['小明说：“今天天气真好啊”\\n小红说：“冰淇淋真好吃”\\n小刚在打篮球\\n小非在踢足球']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter collection for updated source\n",
    "example_db.get(where={\"source\": \"specified-source\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6cltGM5neV2"
   },
   "source": [
    "#13 FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x72S_UVOnkjk",
    "outputId": "f198e360-c7df-4263-cf23-a52cbb54eef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明说：“今天天气真好啊”\n",
      "小红说：“冰淇淋真好吃”\n",
      "小刚在打篮球\n",
      "小非在踢足球\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"test.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "query = \"小明在干什么？\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CvFiRgauSyj"
   },
   "source": [
    "##13.1 查询结果通过score返回相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxd70wgIo1Jb",
    "outputId": "404c3a0c-b91e-42eb-8e07-98597673b0d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='小明说：“今天天气真好啊”\\n小红说：“冰淇淋真好吃”\\n小刚在打篮球\\n小非在踢足球', metadata={'source': 'test.txt'}),\n",
       " 0.3869996)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_scores = db.similarity_search_with_score(query)\n",
    "\n",
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxP0kPIgvSdI"
   },
   "source": [
    "##13.2 合并库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVT52gIvo-v7",
    "outputId": "70661b1e-c647-409a-ce9f-d7763d3408de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a1d3057c-cb85-40a8-a216-2ba23cc9034f': Document(page_content='你好', metadata={}),\n",
       " '30a93606-4330-4c06-9d10-2825c073c910': Document(page_content='世界', metadata={})}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1 = FAISS.from_texts([\"你好\"], embeddings)\n",
    "db2 = FAISS.from_texts([\"世界\"], embeddings)\n",
    "\n",
    "db1.merge_from(db2)\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr1ziOzDubaw"
   },
   "source": [
    "##13.3将Page 作为metadata进行查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjbJBeJxq-KO",
    "outputId": "3dce937b-cd28-4822-f873-f7c1929368d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: 你好, Metadata: {'page': 1}, Score: 6.0087098498033415e-15\n",
      "Content: 你好, Metadata: {'page': 2}, Score: 6.0087098498033415e-15\n",
      "Content: 你好, Metadata: {'page': 3}, Score: 6.0087098498033415e-15\n",
      "Content: 你好, Metadata: {'page': 4}, Score: 6.0087098498033415e-15\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "list_of_documents = [\n",
    "    Document(page_content=\"你好\", metadata=dict(page=1)),\n",
    "    Document(page_content=\"小明\", metadata=dict(page=1)),\n",
    "    Document(page_content=\"你好\", metadata=dict(page=2)),\n",
    "    Document(page_content=\"小红\", metadata=dict(page=2)),\n",
    "    Document(page_content=\"你好\", metadata=dict(page=3)),\n",
    "    Document(page_content=\"小熊\", metadata=dict(page=3)),\n",
    "    Document(page_content=\"你好\", metadata=dict(page=4)),\n",
    "    Document(page_content=\"小鸟\", metadata=dict(page=4)),\n",
    "]\n",
    "db = FAISS.from_documents(list_of_documents, embeddings)\n",
    "results_with_scores = db.similarity_search_with_score(\"你好\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIoiAdwzrInc",
    "outputId": "3b23b2d1-4819-46ae-83cf-8a29f357cf51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: 你好, Metadata: {'page': 1}, Score: 6.0087098498033415e-15\n",
      "Content: 小明, Metadata: {'page': 1}, Score: 0.32664310932159424\n"
     ]
    }
   ],
   "source": [
    "results_with_scores = db.similarity_search_with_score(\"你好\", filter=dict(page=1))\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpWwbB-dsFv-"
   },
   "source": [
    "#14.MultiQueryRetriever 多查询索引器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3foQPd6bhaq"
   },
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhAnzfvnnBK5"
   },
   "source": [
    "##14.1 通过MultiQueryRetriever生成多维度查询输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBkDjHZrsKvS"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 加载文档\n",
    "loader = WebBaseLoader(\"https://chatgptzhanghao.com/\")\n",
    "data = loader.load()\n",
    "\n",
    "# 切割文档\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# 存储到chroma的向量数据库中\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hG6AZ-W6auB2",
    "outputId": "96f65ab5-c038-4e6a-ade3-55d122e86334"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. GPT是指什么？', '2. GPT的定义是什么？', '3. GPT是什么概念？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='想要购买\\xa0 ChatGPT 已注册好成品号的朋友，可以直接 点此链接到某宝购买ChatGPT成品号，马上提升工作效率！永久免费使用的官方正版账号。 \\n\\n\\n\\nChatGPT 是什么 \\n\\n\\n\\nChatGPT 是由 OpenAI 开发的一种大型语言模型，可以用于回答各种问题、生成文本、进行对话等自然语言处理任务。简单来说，Chat GPT就是最新一代通用型超强 AI，拥有庞大的知识库，能够回答各种各样的问题。有什么问题，问ChatGPT就行了！ChatGPT 由 OpenAI 在2022年11月30号发布，发布5天后全球用户数就超过了100w。 \\n\\n\\n\\n \\n\\n\\n\\nChatGPT 官网\\n \\n\\n\\n\\nChatGPT的官方网址是\\xa0https://chat.openai.com\\xa0请认准其官方域名 openai.com，不是这个域名的ChatGPT服务都是镜像站或者假冒伪劣网站，请注意辨别。（国内山寨版本很多） \\n\\n\\n\\n如何注册 ChatGPT 使用流程' metadata={'description': '本文提供了适合小白简单易学的 ChatGPT 最新使用教程，一步步手把手，超详细亲测有效。通过本文你将学会如何注册 ChatGPT 账号，如何使用 ChatGPT，适合中国地区的用户。买ChatGPT账号就到ChatGPT账号网。', 'language': 'en-US', 'source': 'https://chatgptzhanghao.com/', 'title': 'ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网'}\n",
      "page_content='你可以尝试用各种方式向 ChatGPT 提出各种各样的问题或者指令，通过这一步你将更能体会到 ChatGPT 的强大之处。来吧，朋友，让我们真正学会 Chat GPT 怎么用。比如：叫 ChatGPT 写一首诗叫 ChatGPT 写代码叫 ChatGPT进行翻译叫 ChatGPT 编故事 \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\xa0好啦，到这里为止，我们已经一步步教会了你怎麽注冊 ChatGPT账号，怎么通过 ChatGPT 手机号验证，怎么用 ChatGPT。\\xa0祝大家玩的愉快！ \\n\\n\\n\\n如果看到这里还是觉得 ChatGPT 账号注册过于麻烦的朋友，可以直接到某宝购买哦，\\xa0点此购买ChatGPT现成账号\\xa0打开看到 “皮特的号” 就是ChatGPT账号。这更适合不懂技术或不想折腾但是又想马上尽快体验到 ChatGPT 的朋友，而且账号可靠，售后保证，有什么不懂随时可以咨询，用起来也很省心。 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n更多阅读' metadata={'description': '本文提供了适合小白简单易学的 ChatGPT 最新使用教程，一步步手把手，超详细亲测有效。通过本文你将学会如何注册 ChatGPT 账号，如何使用 ChatGPT，适合中国地区的用户。买ChatGPT账号就到ChatGPT账号网。', 'language': 'en-US', 'source': 'https://chatgptzhanghao.com/', 'title': 'ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网'}\n",
      "page_content='ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n\\n\\n\\n\\n\\n\\n\\n\\nChatGPT 账号网 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChatGPT 怎么用最新详细教程-新手小白一看就会 \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t本文最后更新时间：2023-07-11\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n ChatGPT 最近火爆异常，相信大家都听说过他的大名。因为不仅功能十分强大，关键账号还是永久免费使用。（注意：官方的号就是永久免费使用的，任何需要充值才能提问，或者提问次数有限制的都是盗版山寨割韭菜的）。但是ChatGPT不支持国内网络和国内用户，很多新手小白朋友还是不清楚 ChatGPT 是什么，怎么才能拥有账号，本文将手把手告诉大家，ChatGPT\\xa0是什么和怎么用怎么注册账号。' metadata={'description': '本文提供了适合小白简单易学的 ChatGPT 最新使用教程，一步步手把手，超详细亲测有效。通过本文你将学会如何注册 ChatGPT 账号，如何使用 ChatGPT，适合中国地区的用户。买ChatGPT账号就到ChatGPT账号网。', 'language': 'en-US', 'source': 'https://chatgptzhanghao.com/', 'title': 'ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网'}\n",
      "page_content='ChatGPT可以用来干什么？秘籍宝典使用手册ChatGPT 登录不了怎么办？ChatGPT Plus是什么ChatGPT:sorry you have been blocked\\xa0 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChatGPT账号、PLUS账号 购买请扫码添加下方微信咨询 \\n\\n\\n\\n \\n\\n\\n\\n买 ChatGPT 账号就到 ChatGPT 账号网' metadata={'description': '本文提供了适合小白简单易学的 ChatGPT 最新使用教程，一步步手把手，超详细亲测有效。通过本文你将学会如何注册 ChatGPT 账号，如何使用 ChatGPT，适合中国地区的用户。买ChatGPT账号就到ChatGPT账号网。', 'language': 'en-US', 'source': 'https://chatgptzhanghao.com/', 'title': 'ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网'}\n",
      "page_content='这么强大的工具我们都想体验一下，那么\\xa0ChatGPT\\xa0怎么用呢？接下来将给你逐步详细介绍。使用 ChatGPT 主要有4步：注册 ChatGPT 账号注册需要一个外国手机号码：通过短信接码平台\\xa0sms-activate.org\\xa0完成 ChatGPT 手机号验证登录 ChatGPT 账号，对话框输入，开始使用 Chat GPT ！\\xa0输入任意问题，探索 ChatGPT 的强大功能好了，我们开始逐步图文介绍。 \\n\\n\\n\\n由于 OpenAI 官方限制了中国大陆地区用户使用，所以中国手机号码无法验证，需要外国手机号，这导致注册 Chat GPT 的过程会略微复杂。对于怕麻烦的朋友来说最简单直接的办法就是到淘宝买一个已经注册验证好的号，自动发货，即买即用，已经搞定外国手机号验证，不用准备各种复杂的环境（适合新手小白）。非常推荐！点此到某宝购买ChatGPT成品号下面是具体过程。 \\n\\n\\n\\n1.注册 ChatGPT 账号' metadata={'description': '本文提供了适合小白简单易学的 ChatGPT 最新使用教程，一步步手把手，超详细亲测有效。通过本文你将学会如何注册 ChatGPT 账号，如何使用 ChatGPT，适合中国地区的用户。买ChatGPT账号就到ChatGPT账号网。', 'language': 'en-US', 'source': 'https://chatgptzhanghao.com/', 'title': 'ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "question = \"什么是GPT？\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "#创建多维度提问查询\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "#通过多维度查询输入，获取文档内容\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)\n",
    "#打印搜索的文档内容\n",
    "for doc in unique_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGIPlJ_anOaH"
   },
   "source": [
    "##14.2 通过llm生成多维度查询输入，使用MultiQueryRetriever执行多维度查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UOLsXt6cjBD"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"你是一个AI语言模型助手。\n",
    "    你的任务是为给定的用户问题生成五个不同的版本，以便从向量数据库中检索相关文档。\n",
    "    通过生成用户问题的多个视角，你的目标是帮助用户克服基于距离的相似性搜索的一些限制。\n",
    "    请以新行分隔提供这些替代问题。原始问题：{question}\"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
    "# Other inputs\n",
    "question = \"什么是GPT？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9CeTj9di-Co",
    "outputId": "197afa22-2b81-4e00-b236-51514a84f004"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. GPT是什么意思？', '2. GPT代表什么？', '3. GPT是哪个领域的缩写？', '4. GPT是什么类型的模型？', '5. GPT的全称是什么？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='想要购买\\xa0 ChatGPT 已注册好成品号的朋友，可以直接 点此链接到某宝购买ChatGPT成品号，马上提升工作效率！永久免费使用的官方正版账号。 \\n\\n\\n\\nChatGPT 是什么 \\n\\n\\n\\nChatGPT 是由 OpenAI 开发的一种大型语言模型，可以用于回答各种问题、生成文本、进行对话等自然语言处理任务。简单来说，Chat GPT就是最新一代通用型超强 AI，拥有庞大的知识库，能够回答各种各样的问题。有什么问题，问ChatGPT就行了！ChatGPT 由 OpenAI 在2022年11月30号发布，发布5天后全球用户数就超过了100w。 \\n\\n\\n\\n \\n\\n\\n\\nChatGPT 官网\\n \\n\\n\\n\\nChatGPT的官方网址是\\xa0https://chat.openai.com\\xa0请认准其官方域名 openai.com，不是这个域名的ChatGPT服务都是镜像站或者假冒伪劣网站，请注意辨别。（国内山寨版本很多） \\n\\n\\n\\n如何注册 ChatGPT 使用流程' metadata={'description': '本文提供了适合小白简单易学的 ChatGPT 最新使用教程，一步步手把手，超详细亲测有效。通过本文你将学会如何注册 ChatGPT 账号，如何使用 ChatGPT，适合中国地区的用户。买ChatGPT账号就到ChatGPT账号网。', 'language': 'en-US', 'source': 'https://chatgptzhanghao.com/', 'title': 'ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网'}\n",
      "page_content='你可以尝试用各种方式向 ChatGPT 提出各种各样的问题或者指令，通过这一步你将更能体会到 ChatGPT 的强大之处。来吧，朋友，让我们真正学会 Chat GPT 怎么用。比如：叫 ChatGPT 写一首诗叫 ChatGPT 写代码叫 ChatGPT进行翻译叫 ChatGPT 编故事 \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\xa0好啦，到这里为止，我们已经一步步教会了你怎麽注冊 ChatGPT账号，怎么通过 ChatGPT 手机号验证，怎么用 ChatGPT。\\xa0祝大家玩的愉快！ \\n\\n\\n\\n如果看到这里还是觉得 ChatGPT 账号注册过于麻烦的朋友，可以直接到某宝购买哦，\\xa0点此购买ChatGPT现成账号\\xa0打开看到 “皮特的号” 就是ChatGPT账号。这更适合不懂技术或不想折腾但是又想马上尽快体验到 ChatGPT 的朋友，而且账号可靠，售后保证，有什么不懂随时可以咨询，用起来也很省心。 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n更多阅读' metadata={'description': '本文提供了适合小白简单易学的 ChatGPT 最新使用教程，一步步手把手，超详细亲测有效。通过本文你将学会如何注册 ChatGPT 账号，如何使用 ChatGPT，适合中国地区的用户。买ChatGPT账号就到ChatGPT账号网。', 'language': 'en-US', 'source': 'https://chatgptzhanghao.com/', 'title': 'ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网'}\n",
      "page_content='ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n\\n\\n\\n\\n\\n\\n\\n\\nChatGPT 账号网 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChatGPT 怎么用最新详细教程-新手小白一看就会 \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t本文最后更新时间：2023-07-11\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n ChatGPT 最近火爆异常，相信大家都听说过他的大名。因为不仅功能十分强大，关键账号还是永久免费使用。（注意：官方的号就是永久免费使用的，任何需要充值才能提问，或者提问次数有限制的都是盗版山寨割韭菜的）。但是ChatGPT不支持国内网络和国内用户，很多新手小白朋友还是不清楚 ChatGPT 是什么，怎么才能拥有账号，本文将手把手告诉大家，ChatGPT\\xa0是什么和怎么用怎么注册账号。' metadata={'description': '本文提供了适合小白简单易学的 ChatGPT 最新使用教程，一步步手把手，超详细亲测有效。通过本文你将学会如何注册 ChatGPT 账号，如何使用 ChatGPT，适合中国地区的用户。买ChatGPT账号就到ChatGPT账号网。', 'language': 'en-US', 'source': 'https://chatgptzhanghao.com/', 'title': 'ChatGPT 怎么用最新详细教程-新手小白一看就会-ChatGPT 账号网'}\n"
     ]
    }
   ],
   "source": [
    "# 执行多维度查询\n",
    "retriever = MultiQueryRetriever(\n",
    "    #\"lines\" 是输出解析的属性值，意思是按照行解析输出信息\n",
    "    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")\n",
    "\n",
    "# 根据查询生成结果\n",
    "unique_docs = retriever.get_relevant_documents(\n",
    "    query=\"什么是GPT？\"\n",
    ")\n",
    "len(unique_docs)\n",
    "#打印搜索的文档内容\n",
    "for doc in unique_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKBu7WlPKEZp"
   },
   "source": [
    "#15.上下文压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MWdwCYO1KJCJ"
   },
   "outputs": [],
   "source": [
    "#打印文档内容函数\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uPnCadjK5vZ",
    "outputId": "06ab45fe-ed00-4e64-8e17-d5ff64a91d76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "很久很久以前，有一片幽深的森林，森林里住着各种各样的动物。在这里，生活着一只憨态可掬的小老虎，名叫泰格，以及一只纯洁无瑕的小白兔，名叫莉莉。尽管他们的种族、生活习性不同，他们却是最好的朋友。\n",
      "\n",
      "一天，泰格和莉莉正在森林中欢快地玩耍。突然，他们看到一颗巨大的金色果实高高挂在一棵巨大的树上。他们都被这颗金色的果实吸引，但它摘取这个果实需要通力合作。因此，他们决定一起尝试摘取果实。泰格用他强壮的身体爬上树干，而莉莉则坐在他的背上，用她的敏捷和轻盈摘下了果实。这是他们共同努力的结果，他们的友情也因此而更加坚固。\n",
      "\n",
      "然而，美好的时光总是短暂的。有一天，一个邪恶的猎人闯进了森林，他决定捉拿泰格，因为他觉得泰格的毛皮非常珍贵。猎人在泰格常去的水源处设下了陷阱，而无知的泰格就这样落入了猎人的陷阱。\n",
      "\n",
      "莉莉看到泰格陷入危险，害怕又担心。尽管她是一只小小的白兔，没有强大的力量，但她决定尽自己的努力去救泰格。她找到了森林中的智者，一只老乌龟，向他寻求帮助。老乌龟告诉莉莉，只有找到神秘的宝石，才能解除陷阱。\n",
      "\n",
      "小白兔莉莉决定冒险去寻找这块神秘的宝石。她独自穿过森林，跨过河流，攀上山丘，经历了无数的困难与危险。最终，莉莉在一片密布魔法的花海中找到了这块神秘的宝石。\n",
      "\n",
      "持有宝石的莉莉回到了陷阱所在地，她用尽全身的力气，对着陷阱呼唤出宝石的力量。在一道耀眼的光芒后，陷阱被解除了，泰格得救了。\n",
      "\n",
      "看到莉莉的勇气和决心，所有的森林动物都深受感动。他们不再只是彼此的食物，而是真正的朋友。从那天起，森林里的动物们开始和睦共处，他们相互帮助，彼此尊重，一同守护他们的家园。\n",
      "\n",
      "莉莉和泰格的故事在森林中流传，他们的友谊和勇气成为了所有动物的楷模。他们的故事告诉我们，真正的朋友会在你需要的时候站出来帮助你，无论他是强大的老虎，还是柔弱的兔子。友谊的力量超越了种族和身份，是我们在生活中最宝贵的财富。\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "documents = TextLoader('story.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "#创建索引器针对嵌入之后的文档进行查询\n",
    "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
    "#一个童话故事， 小白兔（莉莉）要去救，小老虎（泰格），但是能力有限，就去找老乌龟想办法。\n",
    "docs = retriever.get_relevant_documents(\"老乌龟做了什么？\")\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPBieXsoLp-P",
    "outputId": "916db2f9-8e16-4436-e68e-fe170ee9f74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "老乌龟告诉莉莉，只有找到神秘的宝石，才能解除陷阱。\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "#通过OpenAI提供的llm对文档进行压缩，说白了就是通过llm 对文件块生成了摘要\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "#当索引器丢给上下文压缩器进行压缩，为后面查询做准备。\n",
    "#这个索引器之前是基于没有压缩文档建立的，如果要对压缩的文档进行查询就需要放到这里进行压缩。\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "#通过压缩之后的索引器进行文档的查询\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\"老乌龟做了什事情？\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhH3ItwTVRXh"
   },
   "source": [
    "#16.self filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIOUJCy1YvA0"
   },
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhPY5qVIZFea",
    "outputId": "3cba1880-b3fc-4879-ad50-200d24f96637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages (1.1.7)\n"
     ]
    }
   ],
   "source": [
    "pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yk-jmGb1XYxy"
   },
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"一群科学家带回了恐龙，然后混乱随之而来\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\", \"director\": \"Unknown\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"莱昂纳多·迪卡普里奥迷失在梦中的梦中的梦中...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"克里斯托弗·诺兰\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"一名心理学家迷失在一系列的梦中，梦中的梦，梦中的梦，然后《盗梦空间》使用了这个想法\",\n",
    "        metadata={\"year\": 2006, \"director\": \"今敏\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"一群女人非常健康，一些男人对她们充满了向往\",\n",
    "        metadata={\"year\": 2019, \"director\": \"格雷塔·葛韦格\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"玩具变得活灵活现，他们玩得很开心\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\", \"director\": \"Unknown\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"三个男人走进区域，三个男人从区域中走出\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"rating\": 9.9,\n",
    "            \"director\": \"安德烈·塔可夫斯基\",\n",
    "            \"genre\": \"science fiction\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63cuYxiIZr5q",
    "outputId": "ecd4416e-bf59-469c-fba2-424bd665c724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lark==1.1.5\n",
      "  Downloading lark-1.1.5-py3-none-any.whl (107 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m102.4/108.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "  Attempting uninstall: lark\n",
      "    Found existing installation: lark 1.1.4\n",
      "    Uninstalling lark-1.1.4:\n",
      "      Successfully uninstalled lark-1.1.4\n",
      "Successfully installed lark-1.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install lark==1.1.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3E8XTFeIYlPP"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "import lark\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"电影的类型\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"电影发布的年份\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"导演的名字\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"电影的评分（1-10）\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"电影摘要\"\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vectorstore, document_content_description, metadata_field_info, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NDx0ViVIcctR",
    "outputId": "3ea63b29-0820-4e39-c2b6-b7870825b0af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='恐龙' filter=None limit=None\n"
     ]
    }
   ],
   "source": [
    "# This example only specifies a relevant query\n",
    "docs = retriever.get_relevant_documents(\"哪些电影提到了恐龙？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68lcL_xHcs_A",
    "outputId": "3549ac1e-bce1-41ac-9d27-e3fdf86e3359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='梦' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='梦')]) limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example only specifies a filter\n",
    "retriever.get_relevant_documents(\"我要看评分超过8.5 和梦有关的电影\")\n",
    "#pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk7ozkmukh0a"
   },
   "source": [
    "#17.time weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m7IWNATNklZR"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Define your embedding model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "# Initialize the vectorstore as empty\n",
    "embedding_size = 1536\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\n",
    "retriever = TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, decay_rate=.0000000000000000000000001, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "629JvhlDk3_Y",
    "outputId": "31ebd0c1-c351-4e3b-e165-72085a8ecdd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='hello world', metadata={'last_accessed_at': datetime.datetime(2023, 7, 21, 12, 53, 2, 266030), 'created_at': datetime.datetime(2023, 7, 21, 12, 53, 1, 999849), 'buffer_idx': 14})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "retriever.add_documents([Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday})])\n",
    "retriever.add_documents([Document(page_content=\"hello food\")])\n",
    "\n",
    "# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough\n",
    "retriever.get_relevant_documents(\"hello world\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
